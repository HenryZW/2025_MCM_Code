{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3aff8be-ba05-4963-a6e4-ba56e68ea3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, AdamW, RMSprop\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0d79c1-cbdb-44e5-9275-115683a9cd30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial df_raw shape: (249329, 9)\n",
      "Name encoding saved and Name column removed successfully.\n",
      "Feature hashing completed.\n",
      "After row-level transforms, df_filtered shape: (249329, 6332)\n",
      "Grouped by (Year, NOC) shape: (3222, 6332)\n",
      "After NOC encoding, df_agg shape: (3222, 6565)\n",
      "df_agg_scaled shape: (3222, 6565)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0) Load & Basic Cleaning\n",
    "# ------------------------------------------------------------------\n",
    "data_path = \"MCM_2025\\\\2025_MCM-ICM_Problems\\\\2025_Problem_C_Data\\\\summerOly_athletes.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Remove rows with digits or '.' in 'Team'\n",
    "df_no_digits = data[~data['Team'].str.contains(r'\\d|\\.', na=False)]\n",
    "\n",
    "# Remove rows with Roman numerals in 'Team'\n",
    "roman_pattern = r'(?:\\s|-)M{0,4}(?:CM|CD|D?C{0,3})(?:XC|XL|L?X{0,3})(?:IX|IV|V?I{0,3})(?:\\s|$)'\n",
    "df_no_roman = df_no_digits[~df_no_digits['Team'].str.contains(roman_pattern, na=False, flags=re.IGNORECASE)]\n",
    "\n",
    "df_raw = df_no_roman.copy()\n",
    "\n",
    "# Convert categorical columns to strings and fill missing values\n",
    "for col in [\"Name\", \"Sex\", \"Team\", \"NOC\", \"City\", \"Sport\", \"Event\", \"Medal\"]:\n",
    "    df_raw[col] = df_raw[col].astype(str).fillna(\"Unknown\")\n",
    "\n",
    "# Ensure 'Year' is numeric and drop missing values\n",
    "df_raw[\"Year\"] = pd.to_numeric(df_raw[\"Year\"], errors=\"coerce\")\n",
    "df_raw.dropna(subset=[\"Year\"], inplace=True)\n",
    "df_raw[\"Year\"] = df_raw[\"Year\"].astype(int)\n",
    "\n",
    "print(\"Initial df_raw shape:\", df_raw.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Encoding Transformations with Storage\n",
    "# ------------------------------------------------------------------\n",
    "df_filtered = df_raw.copy()\n",
    "\n",
    "# (1a) Label-encode & scale \"Name\"\n",
    "label_encoder_name = LabelEncoder()\n",
    "df_filtered[\"Name_Label\"] = label_encoder_name.fit_transform(df_filtered[\"Name\"])\n",
    "\n",
    "# Save label encoder for later use\n",
    "with open(\"name_label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder_name, f)\n",
    "\n",
    "# Initialize and fit MinMaxScaler\n",
    "scaler_name = MinMaxScaler(feature_range=(0, 1))\n",
    "df_filtered[\"Name_Label\"] = scaler_name.fit_transform(df_filtered[[\"Name_Label\"]])\n",
    "\n",
    "# Save the scaler for future use\n",
    "with open(\"name_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_name, f)\n",
    "\n",
    "# Create the mapping correctly by using the encoder's classes_\n",
    "name_mapping_df = pd.DataFrame({\n",
    "    \"Name\": label_encoder_name.classes_,\n",
    "    \"Encoded_Value\": range(len(label_encoder_name.classes_))\n",
    "})\n",
    "\n",
    "# Save the correct mapping\n",
    "name_mapping_df.to_csv(\"name_encoding.csv\", index=False)\n",
    "\n",
    "# Drop the original Name column after encoding\n",
    "df_filtered.drop(columns=[\"Name\"], inplace=True)\n",
    "\n",
    "print(\"Name encoding saved and Name column removed successfully.\")\n",
    "\n",
    "# (1b) FeatureHash \"Team\", \"Sport\", \"Event\"\n",
    "n_features = 2100  # Adjustable number of hashed features for better performance\n",
    "hashers = {\n",
    "    \"Team\": FeatureHasher(n_features=n_features, input_type='string'),\n",
    "    \"Sport\": FeatureHasher(n_features=n_features, input_type='string'),\n",
    "    \"Event\": FeatureHasher(n_features=n_features, input_type='string')\n",
    "}\n",
    "\n",
    "for col, hasher_obj in hashers.items():\n",
    "    hashed = hasher_obj.transform(df_filtered[col].apply(lambda x: [x]))\n",
    "    hashed_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "        hashed, columns=[f\"{col}_hashed_{i}\" for i in range(n_features)]\n",
    "    )\n",
    "    df_filtered.drop(columns=[col], inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    hashed_df.reset_index(drop=True, inplace=True)\n",
    "    df_filtered = pd.concat([df_filtered, hashed_df], axis=1)\n",
    "\n",
    "# Save the hashers for later use\n",
    "with open(\"hashers.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hashers, f)\n",
    "\n",
    "print(\"Feature hashing completed.\")\n",
    "\n",
    "# (1c) One-hot encode \"Sex\", \"City\", \"Medal\"\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohe_array = ohe.fit_transform(df_filtered[[\"Sex\", \"City\", \"Medal\"]])\n",
    "ohe_cols = ohe.get_feature_names_out([\"Sex\", \"City\", \"Medal\"])\n",
    "ohe_df = pd.DataFrame(ohe_array.toarray(), columns=ohe_cols)\n",
    "\n",
    "df_filtered.drop(columns=[\"Sex\", \"City\", \"Medal\"], inplace=True)\n",
    "df_filtered = pd.concat([df_filtered, ohe_df], axis=1)\n",
    "\n",
    "# Save one-hot encoder\n",
    "with open(\"onehot_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe, f)\n",
    "\n",
    "print(\"After row-level transforms, df_filtered shape:\", df_filtered.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Group by (Year, NOC)\n",
    "# ------------------------------------------------------------------\n",
    "df_filtered[\"Year\"] = df_filtered[\"Year\"].astype(int)\n",
    "\n",
    "grouped_cols = df_filtered.drop(\n",
    "    columns=[\"Medal_Gold\", \"Medal_Silver\", \"Medal_Bronze\", \"Year\"],\n",
    "    errors=\"ignore\"\n",
    ").columns.difference([\"NOC\"])\n",
    "\n",
    "df_agg = df_filtered.groupby([\"Year\", \"NOC\"], as_index=False).agg({c: \"sum\" for c in grouped_cols})\n",
    "df_agg = df_agg.merge(\n",
    "    df_filtered.groupby([\"Year\", \"NOC\"], as_index=False).agg({\n",
    "        \"Medal_Gold\": \"sum\", \"Medal_Silver\": \"sum\", \"Medal_Bronze\": \"sum\"\n",
    "    }),\n",
    "    on=[\"Year\", \"NOC\"], how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Grouped by (Year, NOC) shape:\", df_agg.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Encode NOC at the aggregated level\n",
    "# ------------------------------------------------------------------\n",
    "ohe_noc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "noc_arr = ohe_noc.fit_transform(df_agg[[\"NOC\"]])\n",
    "noc_cols = [f\"NOC_{cat}\" for cat in ohe_noc.categories_[0]]\n",
    "noc_df = pd.DataFrame(noc_arr.toarray(), columns=noc_cols)\n",
    "\n",
    "df_agg.reset_index(drop=True, inplace=True)\n",
    "noc_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_agg = pd.concat([df_agg.drop(columns=[\"NOC\"]), noc_df], axis=1)\n",
    "\n",
    "# Save NOC encoder\n",
    "with open(\"noc_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe_noc, f)\n",
    "\n",
    "print(\"After NOC encoding, df_agg shape:\", df_agg.shape)\n",
    "\n",
    "# Convert numeric\n",
    "num_cols = df_agg.select_dtypes(include=[\"number\"]).columns\n",
    "df_agg[num_cols] = df_agg[num_cols].astype(\"float32\")\n",
    "df_agg[\"Year\"] = df_agg[\"Year\"].astype(\"int32\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Scale Features / Targets\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = df_agg.drop(columns=[\"Year\", \"Medal_Gold\", \"Medal_Silver\", \"Medal_Bronze\"]).columns\n",
    "target_cols = [\"Medal_Gold\", \"Medal_Silver\", \"Medal_Bronze\"]\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "scaler_targets = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_features.fit_transform(df_agg[feature_cols])\n",
    "y_scaled = scaler_targets.fit_transform(df_agg[target_cols])\n",
    "\n",
    "df_agg[feature_cols] = X_scaled\n",
    "df_agg[target_cols] = y_scaled\n",
    "\n",
    "# Save scalers\n",
    "with open(\"scaler_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_features, f)\n",
    "with open(\"scaler_targets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_targets, f)\n",
    "\n",
    "print(\"df_agg_scaled shape:\", df_agg.shape)\n",
    "\n",
    "df_agg.to_csv(\"processed_athlete_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f64c5b3-b5fb-4c65-9db6-7288fbdc07eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2603, 6561) (2603, 3)\n",
      "Test  shape: (619, 6561) (619, 3)\n",
      "\n",
      "Training on 653 samples, validating on 650 samples.\n",
      "Epoch 1/300\n",
      "21/21 - 3s - 164ms/step - loss: 20.9390 - mae: 0.4884 - val_loss: 18.6146 - val_mae: 0.7868 - learning_rate: 0.0025\n",
      "Epoch 2/300\n",
      "21/21 - 1s - 71ms/step - loss: 12.7381 - mae: 0.4075 - val_loss: 10.2509 - val_mae: 0.3279 - learning_rate: 0.0025\n",
      "Epoch 3/300\n",
      "21/21 - 1s - 68ms/step - loss: 8.1458 - mae: 0.3492 - val_loss: 6.3709 - val_mae: 0.3147 - learning_rate: 0.0025\n",
      "Epoch 4/300\n",
      "21/21 - 1s - 68ms/step - loss: 5.2550 - mae: 0.3363 - val_loss: 4.6883 - val_mae: 0.3870 - learning_rate: 0.0024\n",
      "Epoch 5/300\n",
      "21/21 - 1s - 68ms/step - loss: 3.7880 - mae: 0.3503 - val_loss: 3.9534 - val_mae: 0.3776 - learning_rate: 0.0024\n",
      "Epoch 6/300\n",
      "21/21 - 1s - 67ms/step - loss: 3.0918 - mae: 0.3315 - val_loss: 3.3928 - val_mae: 0.3239 - learning_rate: 0.0024\n",
      "Epoch 7/300\n",
      "21/21 - 1s - 67ms/step - loss: 2.6688 - mae: 0.3333 - val_loss: 3.0776 - val_mae: 0.3999 - learning_rate: 0.0024\n",
      "Epoch 8/300\n",
      "21/21 - 1s - 67ms/step - loss: 2.3904 - mae: 0.3341 - val_loss: 2.8903 - val_mae: 0.3446 - learning_rate: 0.0023\n",
      "Epoch 9/300\n",
      "21/21 - 1s - 66ms/step - loss: 2.2496 - mae: 0.3264 - val_loss: 2.7043 - val_mae: 0.3473 - learning_rate: 0.0023\n",
      "Epoch 10/300\n",
      "21/21 - 1s - 66ms/step - loss: 2.0290 - mae: 0.3348 - val_loss: 2.3486 - val_mae: 0.3327 - learning_rate: 0.0023\n",
      "Epoch 11/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.9094 - mae: 0.3359 - val_loss: 2.4164 - val_mae: 0.3809 - learning_rate: 0.0023\n",
      "Epoch 12/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.9131 - mae: 0.3351 - val_loss: 2.2725 - val_mae: 0.3193 - learning_rate: 0.0022\n",
      "Epoch 13/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.8376 - mae: 0.3274 - val_loss: 2.1845 - val_mae: 0.3329 - learning_rate: 0.0022\n",
      "Epoch 14/300\n",
      "21/21 - 1s - 70ms/step - loss: 1.7699 - mae: 0.3319 - val_loss: 2.0856 - val_mae: 0.3413 - learning_rate: 0.0022\n",
      "Epoch 15/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.5582 - mae: 0.3407 - val_loss: 1.9297 - val_mae: 0.3437 - learning_rate: 0.0022\n",
      "Epoch 16/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.4563 - mae: 0.3224 - val_loss: 1.9552 - val_mae: 0.3266 - learning_rate: 0.0022\n",
      "Epoch 17/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.4454 - mae: 0.3142 - val_loss: 1.8492 - val_mae: 0.3725 - learning_rate: 0.0021\n",
      "Epoch 18/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.5778 - mae: 0.3307 - val_loss: 1.9556 - val_mae: 0.3028 - learning_rate: 0.0021\n",
      "Epoch 19/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.4742 - mae: 0.3135 - val_loss: 1.9524 - val_mae: 0.3503 - learning_rate: 0.0021\n",
      "Epoch 20/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.5017 - mae: 0.3104 - val_loss: 2.1309 - val_mae: 0.3536 - learning_rate: 0.0021\n",
      "Epoch 21/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.5608 - mae: 0.3232 - val_loss: 1.9155 - val_mae: 0.3034 - learning_rate: 0.0020\n",
      "Epoch 22/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.5138 - mae: 0.3166 - val_loss: 2.1853 - val_mae: 0.3719 - learning_rate: 0.0020\n",
      "Epoch 23/300\n",
      "21/21 - 1s - 69ms/step - loss: 1.7577 - mae: 0.3190 - val_loss: 2.2098 - val_mae: 0.2606 - learning_rate: 0.0020\n",
      "Epoch 24/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.8922 - mae: 0.3216 - val_loss: 2.0355 - val_mae: 0.2603 - learning_rate: 0.0020\n",
      "Epoch 25/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.8859 - mae: 0.3593 - val_loss: 2.0357 - val_mae: 0.2428 - learning_rate: 0.0020\n",
      "Epoch 26/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.6038 - mae: 0.3325 - val_loss: 1.9165 - val_mae: 0.3356 - learning_rate: 0.0019\n",
      "Epoch 27/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.2959 - mae: 0.3205 - val_loss: 1.7281 - val_mae: 0.3440 - learning_rate: 0.0019\n",
      "Epoch 28/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.2467 - mae: 0.3170 - val_loss: 1.8618 - val_mae: 0.3825 - learning_rate: 0.0019\n",
      "Epoch 29/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.4971 - mae: 0.3325 - val_loss: 1.8882 - val_mae: 0.3924 - learning_rate: 0.0019\n",
      "Epoch 30/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.5595 - mae: 0.3250 - val_loss: 1.8243 - val_mae: 0.3083 - learning_rate: 0.0019\n",
      "Epoch 31/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.3802 - mae: 0.3016 - val_loss: 1.7280 - val_mae: 0.2949 - learning_rate: 0.0018\n",
      "Epoch 32/300\n",
      "21/21 - 1s - 65ms/step - loss: 1.5951 - mae: 0.3184 - val_loss: 1.9839 - val_mae: 0.2857 - learning_rate: 0.0018\n",
      "Epoch 33/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.6987 - mae: 0.3136 - val_loss: 1.7146 - val_mae: 0.2747 - learning_rate: 0.0018\n",
      "Epoch 34/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.4360 - mae: 0.3082 - val_loss: 1.9443 - val_mae: 0.3507 - learning_rate: 0.0018\n",
      "Epoch 35/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.4283 - mae: 0.3257 - val_loss: 2.0324 - val_mae: 0.3121 - learning_rate: 0.0018\n",
      "Epoch 36/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.6260 - mae: 0.3290 - val_loss: 2.0255 - val_mae: 0.3456 - learning_rate: 0.0018\n",
      "Epoch 37/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.2602 - mae: 0.3039 - val_loss: 1.7599 - val_mae: 0.4352 - learning_rate: 0.0017\n",
      "Epoch 38/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.0174 - mae: 0.3084 - val_loss: 1.7464 - val_mae: 0.4066 - learning_rate: 0.0017\n",
      "Epoch 39/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.1465 - mae: 0.3119 - val_loss: 1.5804 - val_mae: 0.3421 - learning_rate: 0.0017\n",
      "Epoch 40/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.3407 - mae: 0.3243 - val_loss: 2.0181 - val_mae: 0.4012 - learning_rate: 0.0017\n",
      "Epoch 41/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.4583 - mae: 0.3117 - val_loss: 1.9871 - val_mae: 0.3107 - learning_rate: 0.0017\n",
      "Epoch 42/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.3505 - mae: 0.2952 - val_loss: 1.6396 - val_mae: 0.3090 - learning_rate: 0.0017\n",
      "Epoch 43/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.3134 - mae: 0.3057 - val_loss: 1.8310 - val_mae: 0.3165 - learning_rate: 0.0016\n",
      "Epoch 44/300\n",
      "21/21 - 1s - 69ms/step - loss: 1.3955 - mae: 0.3282 - val_loss: 1.7787 - val_mae: 0.2995 - learning_rate: 0.0016\n",
      "Epoch 45/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.4944 - mae: 0.3235 - val_loss: 1.7493 - val_mae: 0.2991 - learning_rate: 0.0016\n",
      "Epoch 46/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.6416 - mae: 0.3093 - val_loss: 1.7749 - val_mae: 0.2662 - learning_rate: 0.0016\n",
      "Epoch 47/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.4570 - mae: 0.2997 - val_loss: 2.0599 - val_mae: 0.3374 - learning_rate: 0.0016\n",
      "Epoch 48/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.4858 - mae: 0.3146 - val_loss: 1.9631 - val_mae: 0.2934 - learning_rate: 0.0016\n",
      "Epoch 49/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.5622 - mae: 0.3028 - val_loss: 1.9856 - val_mae: 0.3525 - learning_rate: 0.0015\n",
      "Epoch 50/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.3552 - mae: 0.3037 - val_loss: 2.1386 - val_mae: 0.4051 - learning_rate: 0.0015\n",
      "Epoch 51/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.4635 - mae: 0.3138 - val_loss: 1.9475 - val_mae: 0.3169 - learning_rate: 0.0015\n",
      "Epoch 52/300\n",
      "21/21 - 1s - 69ms/step - loss: 1.5914 - mae: 0.3228 - val_loss: 1.9038 - val_mae: 0.2849 - learning_rate: 0.0015\n",
      "Epoch 53/300\n",
      "21/21 - 1s - 67ms/step - loss: 1.5163 - mae: 0.3118 - val_loss: 1.7981 - val_mae: 0.3008 - learning_rate: 0.0015\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0007337960414588451.\n",
      "21/21 - 1s - 66ms/step - loss: 1.2803 - mae: 0.3046 - val_loss: 1.7443 - val_mae: 0.3908 - learning_rate: 7.3380e-04\n",
      "Epoch 55/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.0527 - mae: 0.3066 - val_loss: 1.7569 - val_mae: 0.3960 - learning_rate: 7.2646e-04\n",
      "Epoch 56/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.9425 - mae: 0.2893 - val_loss: 1.7192 - val_mae: 0.3747 - learning_rate: 7.1919e-04\n",
      "Epoch 57/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8797 - mae: 0.2737 - val_loss: 1.6402 - val_mae: 0.3956 - learning_rate: 7.1200e-04\n",
      "Epoch 58/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.7923 - mae: 0.2703 - val_loss: 1.6240 - val_mae: 0.3745 - learning_rate: 7.0488e-04\n",
      "Epoch 59/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.8739 - mae: 0.3005 - val_loss: 1.5532 - val_mae: 0.3966 - learning_rate: 6.9783e-04\n",
      "Epoch 60/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8084 - mae: 0.2822 - val_loss: 1.5571 - val_mae: 0.4121 - learning_rate: 6.9085e-04\n",
      "Epoch 61/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.8072 - mae: 0.2707 - val_loss: 1.4143 - val_mae: 0.3246 - learning_rate: 6.8395e-04\n",
      "Epoch 62/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8033 - mae: 0.2676 - val_loss: 1.4819 - val_mae: 0.3732 - learning_rate: 6.7711e-04\n",
      "Epoch 63/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.7546 - mae: 0.2620 - val_loss: 1.3969 - val_mae: 0.3548 - learning_rate: 6.7034e-04\n",
      "Epoch 64/300\n",
      "21/21 - 1s - 71ms/step - loss: 0.7166 - mae: 0.2649 - val_loss: 1.4160 - val_mae: 0.3622 - learning_rate: 6.6363e-04\n",
      "Epoch 65/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6713 - mae: 0.2665 - val_loss: 1.4387 - val_mae: 0.3821 - learning_rate: 6.5700e-04\n",
      "Epoch 66/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.7235 - mae: 0.2766 - val_loss: 1.5236 - val_mae: 0.3625 - learning_rate: 6.5043e-04\n",
      "Epoch 67/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8813 - mae: 0.2927 - val_loss: 1.2601 - val_mae: 0.2850 - learning_rate: 6.4392e-04\n",
      "Epoch 68/300\n",
      "21/21 - 1s - 66ms/step - loss: 1.0195 - mae: 0.2829 - val_loss: 1.3301 - val_mae: 0.2792 - learning_rate: 6.3748e-04\n",
      "Epoch 69/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.9677 - mae: 0.2711 - val_loss: 1.4486 - val_mae: 0.2917 - learning_rate: 6.3111e-04\n",
      "Epoch 70/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8620 - mae: 0.2584 - val_loss: 1.5026 - val_mae: 0.3431 - learning_rate: 6.2480e-04\n",
      "Epoch 71/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.7351 - mae: 0.2575 - val_loss: 1.5064 - val_mae: 0.3898 - learning_rate: 6.1855e-04\n",
      "Epoch 72/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6624 - mae: 0.2555 - val_loss: 1.3857 - val_mae: 0.3493 - learning_rate: 6.1236e-04\n",
      "Epoch 73/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.8007 - mae: 0.2810 - val_loss: 1.3368 - val_mae: 0.3308 - learning_rate: 6.0624e-04\n",
      "Epoch 74/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.7794 - mae: 0.2724 - val_loss: 1.3616 - val_mae: 0.3154 - learning_rate: 6.0018e-04\n",
      "Epoch 75/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.7673 - mae: 0.2709 - val_loss: 1.2471 - val_mae: 0.3113 - learning_rate: 5.9418e-04\n",
      "Epoch 76/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.7293 - mae: 0.2587 - val_loss: 1.4597 - val_mae: 0.3328 - learning_rate: 5.8823e-04\n",
      "Epoch 77/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6767 - mae: 0.2590 - val_loss: 1.5677 - val_mae: 0.3746 - learning_rate: 5.8235e-04\n",
      "Epoch 78/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6370 - mae: 0.2459 - val_loss: 1.4451 - val_mae: 0.3510 - learning_rate: 5.7653e-04\n",
      "Epoch 79/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6450 - mae: 0.2586 - val_loss: 1.3145 - val_mae: 0.3218 - learning_rate: 5.7076e-04\n",
      "Epoch 80/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.7270 - mae: 0.2724 - val_loss: 1.2877 - val_mae: 0.3084 - learning_rate: 5.6505e-04\n",
      "Epoch 81/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8034 - mae: 0.2676 - val_loss: 1.3800 - val_mae: 0.2871 - learning_rate: 5.5940e-04\n",
      "Epoch 82/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8419 - mae: 0.2622 - val_loss: 1.3814 - val_mae: 0.2956 - learning_rate: 5.5381e-04\n",
      "Epoch 83/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.9297 - mae: 0.2676 - val_loss: 1.5069 - val_mae: 0.2865 - learning_rate: 5.4827e-04\n",
      "Epoch 84/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.8873 - mae: 0.2554 - val_loss: 1.4862 - val_mae: 0.2927 - learning_rate: 5.4279e-04\n",
      "Epoch 85/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.7984 - mae: 0.2435 - val_loss: 1.5270 - val_mae: 0.3189 - learning_rate: 5.3736e-04\n",
      "Epoch 86/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.7303 - mae: 0.2358 - val_loss: 1.4063 - val_mae: 0.3149 - learning_rate: 5.3199e-04\n",
      "Epoch 87/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6712 - mae: 0.2329 - val_loss: 1.4955 - val_mae: 0.3429 - learning_rate: 5.2667e-04\n",
      "Epoch 88/300\n",
      "21/21 - 1s - 70ms/step - loss: 0.6622 - mae: 0.2503 - val_loss: 1.4355 - val_mae: 0.3613 - learning_rate: 5.2140e-04\n",
      "Epoch 89/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6522 - mae: 0.2457 - val_loss: 1.4595 - val_mae: 0.3671 - learning_rate: 5.1619e-04\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.00025551262660883367.\n",
      "21/21 - 1s - 66ms/step - loss: 0.6643 - mae: 0.2573 - val_loss: 1.4769 - val_mae: 0.3606 - learning_rate: 2.5551e-04\n",
      "Epoch 91/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.6519 - mae: 0.2445 - val_loss: 1.4953 - val_mae: 0.3688 - learning_rate: 2.5296e-04\n",
      "Epoch 92/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.5574 - mae: 0.2126 - val_loss: 1.4174 - val_mae: 0.3557 - learning_rate: 2.5043e-04\n",
      "Epoch 93/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.5030 - mae: 0.2029 - val_loss: 1.3983 - val_mae: 0.3608 - learning_rate: 2.4792e-04\n",
      "Epoch 94/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.4421 - mae: 0.1913 - val_loss: 1.4215 - val_mae: 0.3850 - learning_rate: 2.4544e-04\n",
      "Epoch 95/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.4220 - mae: 0.1964 - val_loss: 1.3637 - val_mae: 0.3792 - learning_rate: 2.4299e-04\n",
      "Epoch 96/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3985 - mae: 0.2007 - val_loss: 1.3086 - val_mae: 0.3710 - learning_rate: 2.4056e-04\n",
      "Epoch 97/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3786 - mae: 0.1960 - val_loss: 1.3324 - val_mae: 0.3793 - learning_rate: 2.3815e-04\n",
      "Epoch 98/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3781 - mae: 0.2072 - val_loss: 1.3179 - val_mae: 0.3846 - learning_rate: 2.3577e-04\n",
      "Epoch 99/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.3579 - mae: 0.2085 - val_loss: 1.2219 - val_mae: 0.3505 - learning_rate: 2.3342e-04\n",
      "Epoch 100/300\n",
      "21/21 - 1s - 70ms/step - loss: 0.3556 - mae: 0.2002 - val_loss: 1.2117 - val_mae: 0.3327 - learning_rate: 2.3108e-04\n",
      "Epoch 101/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3407 - mae: 0.2024 - val_loss: 1.3081 - val_mae: 0.3692 - learning_rate: 2.2877e-04\n",
      "Epoch 102/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3303 - mae: 0.1980 - val_loss: 1.2611 - val_mae: 0.3360 - learning_rate: 2.2648e-04\n",
      "Epoch 103/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3296 - mae: 0.1913 - val_loss: 1.1846 - val_mae: 0.3215 - learning_rate: 2.2422e-04\n",
      "Epoch 104/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3571 - mae: 0.2127 - val_loss: 1.2434 - val_mae: 0.3531 - learning_rate: 2.2198e-04\n",
      "Epoch 105/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3378 - mae: 0.2080 - val_loss: 1.1285 - val_mae: 0.3319 - learning_rate: 2.1976e-04\n",
      "Epoch 106/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3057 - mae: 0.1863 - val_loss: 1.0934 - val_mae: 0.2949 - learning_rate: 2.1756e-04\n",
      "Epoch 107/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.2998 - mae: 0.1844 - val_loss: 1.1766 - val_mae: 0.3238 - learning_rate: 2.1538e-04\n",
      "Epoch 108/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.2933 - mae: 0.1925 - val_loss: 1.2277 - val_mae: 0.3382 - learning_rate: 2.1323e-04\n",
      "Epoch 109/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.2736 - mae: 0.1820 - val_loss: 1.1551 - val_mae: 0.3158 - learning_rate: 2.1110e-04\n",
      "Epoch 110/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.3076 - mae: 0.1989 - val_loss: 1.0416 - val_mae: 0.2951 - learning_rate: 2.0899e-04\n",
      "Epoch 111/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3517 - mae: 0.2126 - val_loss: 1.0901 - val_mae: 0.2937 - learning_rate: 2.0690e-04\n",
      "Epoch 112/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3522 - mae: 0.2090 - val_loss: 1.0629 - val_mae: 0.3039 - learning_rate: 2.0483e-04\n",
      "Epoch 113/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3687 - mae: 0.2045 - val_loss: 1.0787 - val_mae: 0.2904 - learning_rate: 2.0278e-04\n",
      "Epoch 114/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.3473 - mae: 0.1980 - val_loss: 1.0697 - val_mae: 0.2664 - learning_rate: 2.0075e-04\n",
      "Epoch 115/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.3076 - mae: 0.1794 - val_loss: 1.1473 - val_mae: 0.3014 - learning_rate: 1.9874e-04\n",
      "Epoch 116/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2947 - mae: 0.1755 - val_loss: 1.1368 - val_mae: 0.3099 - learning_rate: 1.9676e-04\n",
      "Epoch 117/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2968 - mae: 0.1816 - val_loss: 1.1493 - val_mae: 0.3081 - learning_rate: 1.9479e-04\n",
      "Epoch 118/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2939 - mae: 0.1817 - val_loss: 1.0657 - val_mae: 0.3136 - learning_rate: 1.9284e-04\n",
      "Epoch 119/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2741 - mae: 0.1792 - val_loss: 1.1908 - val_mae: 0.3197 - learning_rate: 1.9091e-04\n",
      "Epoch 120/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2710 - mae: 0.1788 - val_loss: 1.2252 - val_mae: 0.3345 - learning_rate: 1.8900e-04\n",
      "Epoch 121/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2676 - mae: 0.1816 - val_loss: 1.1286 - val_mae: 0.3089 - learning_rate: 1.8711e-04\n",
      "Epoch 122/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.2583 - mae: 0.1771 - val_loss: 1.0747 - val_mae: 0.2883 - learning_rate: 1.8524e-04\n",
      "Epoch 123/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2460 - mae: 0.1715 - val_loss: 1.1419 - val_mae: 0.3108 - learning_rate: 1.8339e-04\n",
      "Epoch 124/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2477 - mae: 0.1756 - val_loss: 1.1987 - val_mae: 0.3457 - learning_rate: 1.8156e-04\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 8.98698708624579e-05.\n",
      "21/21 - 1s - 66ms/step - loss: 0.2371 - mae: 0.1733 - val_loss: 1.0501 - val_mae: 0.2929 - learning_rate: 8.9870e-05\n",
      "Epoch 126/300\n",
      "21/21 - 1s - 65ms/step - loss: 0.2401 - mae: 0.1718 - val_loss: 1.1466 - val_mae: 0.3190 - learning_rate: 8.8971e-05\n",
      "Epoch 127/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2233 - mae: 0.1582 - val_loss: 1.1631 - val_mae: 0.3225 - learning_rate: 8.8081e-05\n",
      "Epoch 128/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2225 - mae: 0.1538 - val_loss: 1.1121 - val_mae: 0.3049 - learning_rate: 8.7201e-05\n",
      "Epoch 129/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.2107 - mae: 0.1481 - val_loss: 1.1127 - val_mae: 0.3096 - learning_rate: 8.6329e-05\n",
      "Epoch 130/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1933 - mae: 0.1447 - val_loss: 1.1194 - val_mae: 0.3134 - learning_rate: 8.5465e-05\n",
      "Epoch 131/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1864 - mae: 0.1444 - val_loss: 1.1231 - val_mae: 0.3174 - learning_rate: 8.4611e-05\n",
      "Epoch 132/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1870 - mae: 0.1425 - val_loss: 1.1045 - val_mae: 0.3153 - learning_rate: 8.3765e-05\n",
      "Epoch 133/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1753 - mae: 0.1408 - val_loss: 1.1242 - val_mae: 0.3207 - learning_rate: 8.2927e-05\n",
      "Epoch 134/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1759 - mae: 0.1434 - val_loss: 1.0976 - val_mae: 0.3156 - learning_rate: 8.2098e-05\n",
      "Epoch 135/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1672 - mae: 0.1359 - val_loss: 1.0602 - val_mae: 0.3064 - learning_rate: 8.1277e-05\n",
      "Epoch 136/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1746 - mae: 0.1448 - val_loss: 1.0751 - val_mae: 0.3066 - learning_rate: 8.0464e-05\n",
      "Epoch 137/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1718 - mae: 0.1433 - val_loss: 1.1000 - val_mae: 0.3163 - learning_rate: 7.9659e-05\n",
      "Epoch 138/300\n",
      "21/21 - 1s - 69ms/step - loss: 0.1644 - mae: 0.1443 - val_loss: 1.0462 - val_mae: 0.2971 - learning_rate: 7.8863e-05\n",
      "Epoch 139/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1747 - mae: 0.1507 - val_loss: 1.1068 - val_mae: 0.3189 - learning_rate: 7.8074e-05\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 3.864666723529808e-05.\n",
      "21/21 - 1s - 68ms/step - loss: 0.1599 - mae: 0.1440 - val_loss: 1.0602 - val_mae: 0.3049 - learning_rate: 3.8647e-05\n",
      "Epoch 141/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1540 - mae: 0.1344 - val_loss: 1.0813 - val_mae: 0.3114 - learning_rate: 3.8260e-05\n",
      "Epoch 142/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1530 - mae: 0.1342 - val_loss: 1.0538 - val_mae: 0.3033 - learning_rate: 3.7878e-05\n",
      "Epoch 143/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1410 - mae: 0.1257 - val_loss: 1.0509 - val_mae: 0.3052 - learning_rate: 3.7499e-05\n",
      "Epoch 144/300\n",
      "21/21 - 1s - 70ms/step - loss: 0.1385 - mae: 0.1226 - val_loss: 1.0591 - val_mae: 0.3052 - learning_rate: 3.7124e-05\n",
      "Epoch 145/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1420 - mae: 0.1261 - val_loss: 1.0546 - val_mae: 0.3038 - learning_rate: 3.6753e-05\n",
      "Epoch 146/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1419 - mae: 0.1293 - val_loss: 1.0338 - val_mae: 0.2941 - learning_rate: 3.6385e-05\n",
      "Epoch 147/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1422 - mae: 0.1294 - val_loss: 1.0431 - val_mae: 0.3005 - learning_rate: 3.6021e-05\n",
      "Epoch 148/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1379 - mae: 0.1257 - val_loss: 1.0498 - val_mae: 0.3003 - learning_rate: 3.5661e-05\n",
      "Epoch 149/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1391 - mae: 0.1287 - val_loss: 1.0381 - val_mae: 0.3003 - learning_rate: 3.5304e-05\n",
      "Epoch 150/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1329 - mae: 0.1251 - val_loss: 1.0405 - val_mae: 0.2996 - learning_rate: 3.4951e-05\n",
      "Epoch 151/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1378 - mae: 0.1290 - val_loss: 1.0426 - val_mae: 0.2999 - learning_rate: 3.4602e-05\n",
      "Epoch 152/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1312 - mae: 0.1244 - val_loss: 1.0370 - val_mae: 0.2994 - learning_rate: 3.4256e-05\n",
      "Epoch 153/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.1272 - mae: 0.1246 - val_loss: 1.0219 - val_mae: 0.2944 - learning_rate: 3.3913e-05\n",
      "Epoch 154/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1343 - mae: 0.1223 - val_loss: 1.0259 - val_mae: 0.2957 - learning_rate: 3.3574e-05\n",
      "Epoch 155/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1255 - mae: 0.1217 - val_loss: 1.0268 - val_mae: 0.2985 - learning_rate: 3.3238e-05\n",
      "Epoch 156/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1196 - mae: 0.1183 - val_loss: 1.0230 - val_mae: 0.2969 - learning_rate: 3.2906e-05\n",
      "Epoch 157/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1229 - mae: 0.1195 - val_loss: 1.0206 - val_mae: 0.2953 - learning_rate: 3.2577e-05\n",
      "Epoch 158/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1194 - mae: 0.1197 - val_loss: 0.9890 - val_mae: 0.2885 - learning_rate: 3.2251e-05\n",
      "Epoch 159/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1271 - mae: 0.1238 - val_loss: 1.0309 - val_mae: 0.2977 - learning_rate: 3.1929e-05\n",
      "Epoch 160/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1223 - mae: 0.1236 - val_loss: 0.9940 - val_mae: 0.2872 - learning_rate: 3.1609e-05\n",
      "Epoch 161/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1151 - mae: 0.1182 - val_loss: 1.0040 - val_mae: 0.2937 - learning_rate: 3.1293e-05\n",
      "Epoch 162/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1258 - mae: 0.1259 - val_loss: 1.0253 - val_mae: 0.2982 - learning_rate: 3.0980e-05\n",
      "Epoch 163/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1132 - mae: 0.1207 - val_loss: 1.0064 - val_mae: 0.2924 - learning_rate: 3.0671e-05\n",
      "Epoch 164/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1200 - mae: 0.1271 - val_loss: 1.0233 - val_mae: 0.2967 - learning_rate: 3.0364e-05\n",
      "Epoch 165/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1246 - mae: 0.1254 - val_loss: 0.9994 - val_mae: 0.2920 - learning_rate: 3.0060e-05\n",
      "Epoch 166/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1211 - mae: 0.1224 - val_loss: 0.9781 - val_mae: 0.2869 - learning_rate: 2.9760e-05\n",
      "Epoch 167/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1108 - mae: 0.1189 - val_loss: 0.9866 - val_mae: 0.2864 - learning_rate: 2.9462e-05\n",
      "Epoch 168/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1099 - mae: 0.1173 - val_loss: 1.0076 - val_mae: 0.2931 - learning_rate: 2.9167e-05\n",
      "Epoch 169/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1175 - mae: 0.1227 - val_loss: 1.0090 - val_mae: 0.2925 - learning_rate: 2.8876e-05\n",
      "Epoch 170/300\n",
      "21/21 - 1s - 70ms/step - loss: 0.1226 - mae: 0.1278 - val_loss: 0.9822 - val_mae: 0.2874 - learning_rate: 2.8587e-05\n",
      "Epoch 171/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.1090 - mae: 0.1198 - val_loss: 0.9809 - val_mae: 0.2853 - learning_rate: 2.8301e-05\n",
      "Epoch 172/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1091 - mae: 0.1194 - val_loss: 1.0179 - val_mae: 0.2962 - learning_rate: 2.8018e-05\n",
      "Epoch 173/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1072 - mae: 0.1184 - val_loss: 0.9996 - val_mae: 0.2846 - learning_rate: 2.7738e-05\n",
      "Epoch 174/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1038 - mae: 0.1160 - val_loss: 1.0122 - val_mae: 0.2915 - learning_rate: 2.7461e-05\n",
      "Epoch 175/300\n",
      "21/21 - 1s - 69ms/step - loss: 0.1119 - mae: 0.1220 - val_loss: 0.9929 - val_mae: 0.2867 - learning_rate: 2.7186e-05\n",
      "Epoch 176/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1092 - mae: 0.1189 - val_loss: 0.9889 - val_mae: 0.2855 - learning_rate: 2.6914e-05\n",
      "Epoch 177/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1269 - mae: 0.1279 - val_loss: 0.9769 - val_mae: 0.2852 - learning_rate: 2.6645e-05\n",
      "Epoch 178/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1146 - mae: 0.1246 - val_loss: 0.9912 - val_mae: 0.2933 - learning_rate: 2.6378e-05\n",
      "Epoch 179/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1126 - mae: 0.1223 - val_loss: 0.9902 - val_mae: 0.2865 - learning_rate: 2.6115e-05\n",
      "Epoch 180/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1062 - mae: 0.1170 - val_loss: 0.9703 - val_mae: 0.2786 - learning_rate: 2.5854e-05\n",
      "Epoch 181/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1047 - mae: 0.1173 - val_loss: 0.9949 - val_mae: 0.2902 - learning_rate: 2.5595e-05\n",
      "Epoch 182/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1023 - mae: 0.1167 - val_loss: 0.9871 - val_mae: 0.2887 - learning_rate: 2.5339e-05\n",
      "Epoch 183/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1037 - mae: 0.1168 - val_loss: 1.0003 - val_mae: 0.2921 - learning_rate: 2.5086e-05\n",
      "Epoch 184/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1058 - mae: 0.1188 - val_loss: 0.9824 - val_mae: 0.2904 - learning_rate: 2.4835e-05\n",
      "Epoch 185/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1059 - mae: 0.1208 - val_loss: 1.0122 - val_mae: 0.2952 - learning_rate: 2.4586e-05\n",
      "Epoch 186/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1066 - mae: 0.1209 - val_loss: 0.9507 - val_mae: 0.2815 - learning_rate: 2.4341e-05\n",
      "Epoch 187/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1170 - mae: 0.1251 - val_loss: 0.9813 - val_mae: 0.2910 - learning_rate: 2.4097e-05\n",
      "Epoch 188/300\n",
      "21/21 - 1s - 70ms/step - loss: 0.1028 - mae: 0.1183 - val_loss: 0.9630 - val_mae: 0.2813 - learning_rate: 2.3856e-05\n",
      "Epoch 189/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1052 - mae: 0.1186 - val_loss: 0.9457 - val_mae: 0.2789 - learning_rate: 2.3618e-05\n",
      "Epoch 190/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1021 - mae: 0.1193 - val_loss: 0.9723 - val_mae: 0.2863 - learning_rate: 2.3381e-05\n",
      "Epoch 191/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0989 - mae: 0.1169 - val_loss: 0.9496 - val_mae: 0.2801 - learning_rate: 2.3148e-05\n",
      "Epoch 192/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1001 - mae: 0.1157 - val_loss: 0.9509 - val_mae: 0.2808 - learning_rate: 2.2916e-05\n",
      "Epoch 193/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1097 - mae: 0.1233 - val_loss: 0.9616 - val_mae: 0.2825 - learning_rate: 2.2687e-05\n",
      "Epoch 194/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0961 - mae: 0.1133 - val_loss: 0.9879 - val_mae: 0.2897 - learning_rate: 2.2460e-05\n",
      "Epoch 195/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1039 - mae: 0.1203 - val_loss: 0.9729 - val_mae: 0.2893 - learning_rate: 2.2236e-05\n",
      "Epoch 196/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1020 - mae: 0.1153 - val_loss: 0.9475 - val_mae: 0.2800 - learning_rate: 2.2013e-05\n",
      "Epoch 197/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1088 - mae: 0.1211 - val_loss: 0.9796 - val_mae: 0.2817 - learning_rate: 2.1793e-05\n",
      "Epoch 198/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.1028 - mae: 0.1171 - val_loss: 0.9263 - val_mae: 0.2723 - learning_rate: 2.1575e-05\n",
      "Epoch 199/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1021 - mae: 0.1177 - val_loss: 0.9358 - val_mae: 0.2781 - learning_rate: 2.1359e-05\n",
      "Epoch 200/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0997 - mae: 0.1181 - val_loss: 0.9462 - val_mae: 0.2791 - learning_rate: 2.1146e-05\n",
      "Epoch 201/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0958 - mae: 0.1138 - val_loss: 0.9565 - val_mae: 0.2811 - learning_rate: 2.0934e-05\n",
      "Epoch 202/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0936 - mae: 0.1141 - val_loss: 1.0082 - val_mae: 0.2954 - learning_rate: 2.0725e-05\n",
      "Epoch 203/300\n",
      "21/21 - 2s - 73ms/step - loss: 0.0936 - mae: 0.1119 - val_loss: 0.9680 - val_mae: 0.2822 - learning_rate: 2.0518e-05\n",
      "Epoch 204/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0928 - mae: 0.1127 - val_loss: 0.9542 - val_mae: 0.2808 - learning_rate: 2.0313e-05\n",
      "Epoch 205/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0909 - mae: 0.1116 - val_loss: 0.9734 - val_mae: 0.2856 - learning_rate: 2.0109e-05\n",
      "Epoch 206/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0915 - mae: 0.1110 - val_loss: 0.9797 - val_mae: 0.2858 - learning_rate: 1.9908e-05\n",
      "Epoch 207/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.1026 - mae: 0.1207 - val_loss: 0.9716 - val_mae: 0.2873 - learning_rate: 1.9709e-05\n",
      "Epoch 208/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0960 - mae: 0.1186 - val_loss: 0.9864 - val_mae: 0.2890 - learning_rate: 1.9512e-05\n",
      "Epoch 209/300\n",
      "21/21 - 1s - 65ms/step - loss: 0.0925 - mae: 0.1154 - val_loss: 0.9626 - val_mae: 0.2818 - learning_rate: 1.9317e-05\n",
      "Epoch 210/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0891 - mae: 0.1083 - val_loss: 0.9748 - val_mae: 0.2859 - learning_rate: 1.9124e-05\n",
      "Epoch 211/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0930 - mae: 0.1127 - val_loss: 0.9575 - val_mae: 0.2803 - learning_rate: 1.8933e-05\n",
      "Epoch 212/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0914 - mae: 0.1106 - val_loss: 0.9425 - val_mae: 0.2799 - learning_rate: 1.8743e-05\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 213: ReduceLROnPlateau reducing learning rate to 9.277933713747188e-06.\n",
      "21/21 - 1s - 67ms/step - loss: 0.0900 - mae: 0.1105 - val_loss: 0.9678 - val_mae: 0.2827 - learning_rate: 9.2779e-06\n",
      "Epoch 214/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0867 - mae: 0.1116 - val_loss: 0.9679 - val_mae: 0.2837 - learning_rate: 9.1852e-06\n",
      "Epoch 215/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.0898 - mae: 0.1119 - val_loss: 0.9774 - val_mae: 0.2855 - learning_rate: 9.0933e-06\n",
      "Epoch 216/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0837 - mae: 0.1061 - val_loss: 0.9695 - val_mae: 0.2841 - learning_rate: 9.0024e-06\n",
      "Epoch 217/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0873 - mae: 0.1087 - val_loss: 0.9478 - val_mae: 0.2789 - learning_rate: 8.9123e-06\n",
      "Epoch 218/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0936 - mae: 0.1141 - val_loss: 0.9423 - val_mae: 0.2778 - learning_rate: 8.8232e-06\n",
      "Epoch 219/300\n",
      "21/21 - 1s - 65ms/step - loss: 0.0889 - mae: 0.1090 - val_loss: 0.9464 - val_mae: 0.2789 - learning_rate: 8.7350e-06\n",
      "Epoch 220/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0883 - mae: 0.1117 - val_loss: 0.9551 - val_mae: 0.2809 - learning_rate: 8.6476e-06\n",
      "Epoch 221/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0802 - mae: 0.1043 - val_loss: 0.9474 - val_mae: 0.2786 - learning_rate: 8.5612e-06\n",
      "Epoch 222/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0848 - mae: 0.1058 - val_loss: 0.9522 - val_mae: 0.2804 - learning_rate: 8.4756e-06\n",
      "Epoch 223/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0899 - mae: 0.1088 - val_loss: 0.9690 - val_mae: 0.2854 - learning_rate: 8.3908e-06\n",
      "Epoch 224/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0984 - mae: 0.1146 - val_loss: 0.9608 - val_mae: 0.2829 - learning_rate: 8.3069e-06\n",
      "Epoch 225/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0860 - mae: 0.1075 - val_loss: 0.9524 - val_mae: 0.2794 - learning_rate: 8.2238e-06\n",
      "Epoch 226/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0841 - mae: 0.1043 - val_loss: 0.9447 - val_mae: 0.2793 - learning_rate: 8.1416e-06\n",
      "Epoch 227/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0842 - mae: 0.1058 - val_loss: 0.9479 - val_mae: 0.2804 - learning_rate: 8.0602e-06\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 3.989782271673903e-06.\n",
      "21/21 - 1s - 66ms/step - loss: 0.0845 - mae: 0.1068 - val_loss: 0.9591 - val_mae: 0.2825 - learning_rate: 3.9898e-06\n",
      "Epoch 229/300\n",
      "21/21 - 1s - 69ms/step - loss: 0.0863 - mae: 0.1074 - val_loss: 0.9578 - val_mae: 0.2814 - learning_rate: 3.9499e-06\n",
      "Epoch 230/300\n",
      "21/21 - 1s - 69ms/step - loss: 0.0841 - mae: 0.1055 - val_loss: 0.9525 - val_mae: 0.2806 - learning_rate: 3.9104e-06\n",
      "Epoch 231/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0808 - mae: 0.1040 - val_loss: 0.9467 - val_mae: 0.2800 - learning_rate: 3.8713e-06\n",
      "Epoch 232/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0849 - mae: 0.1061 - val_loss: 0.9498 - val_mae: 0.2800 - learning_rate: 3.8326e-06\n",
      "Epoch 233/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0838 - mae: 0.1040 - val_loss: 0.9510 - val_mae: 0.2795 - learning_rate: 3.7942e-06\n",
      "Epoch 234/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0817 - mae: 0.1026 - val_loss: 0.9515 - val_mae: 0.2794 - learning_rate: 3.7563e-06\n",
      "Epoch 235/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0797 - mae: 0.1018 - val_loss: 0.9542 - val_mae: 0.2800 - learning_rate: 3.7187e-06\n",
      "Epoch 236/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0864 - mae: 0.1074 - val_loss: 0.9494 - val_mae: 0.2795 - learning_rate: 3.6816e-06\n",
      "Epoch 237/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0830 - mae: 0.1048 - val_loss: 0.9473 - val_mae: 0.2784 - learning_rate: 3.6447e-06\n",
      "Epoch 238/300\n",
      "21/21 - 1s - 66ms/step - loss: 0.0902 - mae: 0.1082 - val_loss: 0.9459 - val_mae: 0.2779 - learning_rate: 3.6083e-06\n",
      "Epoch 239/300\n",
      "21/21 - 1s - 69ms/step - loss: 0.0790 - mae: 0.1028 - val_loss: 0.9472 - val_mae: 0.2791 - learning_rate: 3.5722e-06\n",
      "Epoch 240/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0792 - mae: 0.1024 - val_loss: 0.9438 - val_mae: 0.2786 - learning_rate: 3.5365e-06\n",
      "Epoch 241/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.0840 - mae: 0.1050 - val_loss: 0.9387 - val_mae: 0.2762 - learning_rate: 3.5011e-06\n",
      "Epoch 242/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0876 - mae: 0.1069 - val_loss: 0.9407 - val_mae: 0.2771 - learning_rate: 3.4661e-06\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 243: ReduceLROnPlateau reducing learning rate to 1.715722987682966e-06.\n",
      "21/21 - 1s - 66ms/step - loss: 0.0804 - mae: 0.1023 - val_loss: 0.9386 - val_mae: 0.2780 - learning_rate: 1.7157e-06\n",
      "Epoch 244/300\n",
      "21/21 - 1s - 70ms/step - loss: 0.0781 - mae: 0.1013 - val_loss: 0.9389 - val_mae: 0.2783 - learning_rate: 1.6986e-06\n",
      "Epoch 245/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0775 - mae: 0.1003 - val_loss: 0.9393 - val_mae: 0.2782 - learning_rate: 1.6816e-06\n",
      "Epoch 246/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.0844 - mae: 0.1067 - val_loss: 0.9405 - val_mae: 0.2777 - learning_rate: 1.6648e-06\n",
      "Epoch 247/300\n",
      "21/21 - 1s - 68ms/step - loss: 0.0812 - mae: 0.1030 - val_loss: 0.9405 - val_mae: 0.2772 - learning_rate: 1.6481e-06\n",
      "Epoch 248/300\n",
      "21/21 - 1s - 67ms/step - loss: 0.0848 - mae: 0.1020 - val_loss: 0.9388 - val_mae: 0.2762 - learning_rate: 1.6316e-06\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Fold Validation MAE: 0.2723\n",
      "\n",
      "Training on 1303 samples, validating on 650 samples.\n",
      "Epoch 1/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.4891 - mae: 0.2825 - val_loss: 0.5876 - val_mae: 0.2473 - learning_rate: 8.1582e-06\n",
      "Epoch 2/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.3137 - mae: 0.2309 - val_loss: 0.5420 - val_mae: 0.2399 - learning_rate: 8.0766e-06\n",
      "Epoch 3/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.2555 - mae: 0.2095 - val_loss: 0.5293 - val_mae: 0.2326 - learning_rate: 7.9958e-06\n",
      "Epoch 4/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.2062 - mae: 0.1938 - val_loss: 0.5665 - val_mae: 0.2408 - learning_rate: 7.9159e-06\n",
      "Epoch 5/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.2015 - mae: 0.1852 - val_loss: 0.5777 - val_mae: 0.2448 - learning_rate: 7.8367e-06\n",
      "Epoch 6/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.2010 - mae: 0.1821 - val_loss: 0.4806 - val_mae: 0.2177 - learning_rate: 7.7583e-06\n",
      "Epoch 7/300\n",
      "41/41 - 3s - 66ms/step - loss: 0.1724 - mae: 0.1775 - val_loss: 0.4743 - val_mae: 0.2143 - learning_rate: 7.6808e-06\n",
      "Epoch 8/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1699 - mae: 0.1720 - val_loss: 0.5293 - val_mae: 0.2298 - learning_rate: 7.6040e-06\n",
      "Epoch 9/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1543 - mae: 0.1665 - val_loss: 0.5045 - val_mae: 0.2241 - learning_rate: 7.5279e-06\n",
      "Epoch 10/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1567 - mae: 0.1642 - val_loss: 0.5074 - val_mae: 0.2248 - learning_rate: 7.4526e-06\n",
      "Epoch 11/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1571 - mae: 0.1596 - val_loss: 0.5141 - val_mae: 0.2221 - learning_rate: 7.3781e-06\n",
      "Epoch 12/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1464 - mae: 0.1568 - val_loss: 0.4907 - val_mae: 0.2196 - learning_rate: 7.3043e-06\n",
      "Epoch 13/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1616 - mae: 0.1601 - val_loss: 0.4968 - val_mae: 0.2198 - learning_rate: 7.2313e-06\n",
      "Epoch 14/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1334 - mae: 0.1530 - val_loss: 0.5201 - val_mae: 0.2234 - learning_rate: 7.1590e-06\n",
      "Epoch 15/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1500 - mae: 0.1554 - val_loss: 0.5004 - val_mae: 0.2189 - learning_rate: 7.0874e-06\n",
      "Epoch 16/300\n",
      "41/41 - 3s - 61ms/step - loss: 0.1416 - mae: 0.1502 - val_loss: 0.4890 - val_mae: 0.2177 - learning_rate: 7.0165e-06\n",
      "Epoch 17/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1453 - mae: 0.1516 - val_loss: 0.4976 - val_mae: 0.2216 - learning_rate: 6.9463e-06\n",
      "Epoch 18/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1397 - mae: 0.1507 - val_loss: 0.4973 - val_mae: 0.2169 - learning_rate: 6.8769e-06\n",
      "Epoch 19/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1387 - mae: 0.1486 - val_loss: 0.4843 - val_mae: 0.2139 - learning_rate: 6.8081e-06\n",
      "Epoch 20/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1432 - mae: 0.1482 - val_loss: 0.5104 - val_mae: 0.2222 - learning_rate: 6.7400e-06\n",
      "Epoch 21/300\n",
      "41/41 - 3s - 61ms/step - loss: 0.1196 - mae: 0.1417 - val_loss: 0.4816 - val_mae: 0.2129 - learning_rate: 6.6726e-06\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 3.3029514270310756e-06.\n",
      "41/41 - 3s - 63ms/step - loss: 0.1319 - mae: 0.1434 - val_loss: 0.4992 - val_mae: 0.2172 - learning_rate: 3.3030e-06\n",
      "Epoch 23/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1271 - mae: 0.1413 - val_loss: 0.4763 - val_mae: 0.2119 - learning_rate: 3.2699e-06\n",
      "Epoch 24/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1279 - mae: 0.1400 - val_loss: 0.4957 - val_mae: 0.2153 - learning_rate: 3.2372e-06\n",
      "Epoch 25/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1148 - mae: 0.1365 - val_loss: 0.4792 - val_mae: 0.2136 - learning_rate: 3.2049e-06\n",
      "Epoch 26/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1153 - mae: 0.1361 - val_loss: 0.4916 - val_mae: 0.2170 - learning_rate: 3.1728e-06\n",
      "Epoch 27/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1308 - mae: 0.1407 - val_loss: 0.4661 - val_mae: 0.2100 - learning_rate: 3.1411e-06\n",
      "Epoch 28/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1191 - mae: 0.1337 - val_loss: 0.4900 - val_mae: 0.2159 - learning_rate: 3.1097e-06\n",
      "Epoch 29/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1272 - mae: 0.1381 - val_loss: 0.4974 - val_mae: 0.2182 - learning_rate: 3.0786e-06\n",
      "Epoch 30/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1083 - mae: 0.1307 - val_loss: 0.4837 - val_mae: 0.2150 - learning_rate: 3.0478e-06\n",
      "Epoch 31/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1178 - mae: 0.1358 - val_loss: 0.4780 - val_mae: 0.2131 - learning_rate: 3.0173e-06\n",
      "Epoch 32/300\n",
      "41/41 - 3s - 61ms/step - loss: 0.1108 - mae: 0.1328 - val_loss: 0.4887 - val_mae: 0.2152 - learning_rate: 2.9871e-06\n",
      "Epoch 33/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1159 - mae: 0.1319 - val_loss: 0.4926 - val_mae: 0.2156 - learning_rate: 2.9573e-06\n",
      "Epoch 34/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1210 - mae: 0.1340 - val_loss: 0.4980 - val_mae: 0.2182 - learning_rate: 2.9277e-06\n",
      "Epoch 35/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1190 - mae: 0.1358 - val_loss: 0.4910 - val_mae: 0.2175 - learning_rate: 2.8984e-06\n",
      "Epoch 36/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1251 - mae: 0.1362 - val_loss: 0.4977 - val_mae: 0.2171 - learning_rate: 2.8694e-06\n",
      "Epoch 37/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1102 - mae: 0.1300 - val_loss: 0.4690 - val_mae: 0.2112 - learning_rate: 2.8407e-06\n",
      "Epoch 38/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1214 - mae: 0.1356 - val_loss: 0.4886 - val_mae: 0.2171 - learning_rate: 2.8123e-06\n",
      "Epoch 39/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1215 - mae: 0.1360 - val_loss: 0.4770 - val_mae: 0.2137 - learning_rate: 2.7842e-06\n",
      "Epoch 40/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1149 - mae: 0.1323 - val_loss: 0.4761 - val_mae: 0.2132 - learning_rate: 2.7564e-06\n",
      "Epoch 41/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1112 - mae: 0.1318 - val_loss: 0.4860 - val_mae: 0.2164 - learning_rate: 2.7288e-06\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.3507534504242358e-06.\n",
      "41/41 - 3s - 62ms/step - loss: 0.1194 - mae: 0.1302 - val_loss: 0.4872 - val_mae: 0.2144 - learning_rate: 1.3508e-06\n",
      "Epoch 43/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1138 - mae: 0.1302 - val_loss: 0.4954 - val_mae: 0.2167 - learning_rate: 1.3372e-06\n",
      "Epoch 44/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1121 - mae: 0.1276 - val_loss: 0.4932 - val_mae: 0.2166 - learning_rate: 1.3239e-06\n",
      "Epoch 45/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1124 - mae: 0.1290 - val_loss: 0.4947 - val_mae: 0.2168 - learning_rate: 1.3106e-06\n",
      "Epoch 46/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1169 - mae: 0.1294 - val_loss: 0.4842 - val_mae: 0.2155 - learning_rate: 1.2975e-06\n",
      "Epoch 47/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1148 - mae: 0.1307 - val_loss: 0.4816 - val_mae: 0.2156 - learning_rate: 1.2846e-06\n",
      "Epoch 48/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1128 - mae: 0.1274 - val_loss: 0.4769 - val_mae: 0.2146 - learning_rate: 1.2717e-06\n",
      "Epoch 49/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1111 - mae: 0.1306 - val_loss: 0.4836 - val_mae: 0.2170 - learning_rate: 1.2590e-06\n",
      "Epoch 50/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1072 - mae: 0.1280 - val_loss: 0.4868 - val_mae: 0.2170 - learning_rate: 1.2464e-06\n",
      "Epoch 51/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1209 - mae: 0.1304 - val_loss: 0.4894 - val_mae: 0.2174 - learning_rate: 1.2339e-06\n",
      "Epoch 52/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1109 - mae: 0.1268 - val_loss: 0.4846 - val_mae: 0.2171 - learning_rate: 1.2216e-06\n",
      "Epoch 53/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1193 - mae: 0.1288 - val_loss: 0.4911 - val_mae: 0.2180 - learning_rate: 1.2094e-06\n",
      "Epoch 54/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1109 - mae: 0.1284 - val_loss: 0.4861 - val_mae: 0.2165 - learning_rate: 1.1973e-06\n",
      "Epoch 55/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1070 - mae: 0.1279 - val_loss: 0.4889 - val_mae: 0.2164 - learning_rate: 1.1853e-06\n",
      "Epoch 56/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1214 - mae: 0.1299 - val_loss: 0.4793 - val_mae: 0.2148 - learning_rate: 1.1735e-06\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 5.80863343202509e-07.\n",
      "41/41 - 3s - 61ms/step - loss: 0.1207 - mae: 0.1325 - val_loss: 0.4851 - val_mae: 0.2165 - learning_rate: 5.8086e-07\n",
      "Epoch 58/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1051 - mae: 0.1252 - val_loss: 0.4841 - val_mae: 0.2161 - learning_rate: 5.7505e-07\n",
      "Epoch 59/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.1070 - mae: 0.1285 - val_loss: 0.4838 - val_mae: 0.2155 - learning_rate: 5.6930e-07\n",
      "Epoch 60/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1046 - mae: 0.1225 - val_loss: 0.4816 - val_mae: 0.2152 - learning_rate: 5.6361e-07\n",
      "Epoch 61/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1040 - mae: 0.1247 - val_loss: 0.4805 - val_mae: 0.2150 - learning_rate: 5.5797e-07\n",
      "Epoch 62/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1052 - mae: 0.1275 - val_loss: 0.4822 - val_mae: 0.2155 - learning_rate: 5.5240e-07\n",
      "Epoch 63/300\n",
      "41/41 - 3s - 65ms/step - loss: 0.1097 - mae: 0.1262 - val_loss: 0.4858 - val_mae: 0.2164 - learning_rate: 5.4687e-07\n",
      "Epoch 64/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1050 - mae: 0.1276 - val_loss: 0.4902 - val_mae: 0.2171 - learning_rate: 5.4140e-07\n",
      "Epoch 65/300\n",
      "41/41 - 3s - 62ms/step - loss: 0.1062 - mae: 0.1244 - val_loss: 0.4879 - val_mae: 0.2166 - learning_rate: 5.3599e-07\n",
      "Epoch 66/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1005 - mae: 0.1231 - val_loss: 0.4844 - val_mae: 0.2159 - learning_rate: 5.3063e-07\n",
      "Epoch 67/300\n",
      "41/41 - 3s - 65ms/step - loss: 0.1138 - mae: 0.1274 - val_loss: 0.4806 - val_mae: 0.2151 - learning_rate: 5.2532e-07\n",
      "Epoch 68/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.1031 - mae: 0.1227 - val_loss: 0.4812 - val_mae: 0.2154 - learning_rate: 5.2007e-07\n",
      "Epoch 69/300\n",
      "41/41 - 3s - 66ms/step - loss: 0.1026 - mae: 0.1229 - val_loss: 0.4808 - val_mae: 0.2155 - learning_rate: 5.1487e-07\n",
      "Epoch 70/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.1149 - mae: 0.1250 - val_loss: 0.4816 - val_mae: 0.2155 - learning_rate: 5.0972e-07\n",
      "Epoch 71/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.1078 - mae: 0.1269 - val_loss: 0.4857 - val_mae: 0.2159 - learning_rate: 5.0462e-07\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 2.49788115525007e-07.\n",
      "41/41 - 3s - 66ms/step - loss: 0.1134 - mae: 0.1267 - val_loss: 0.4830 - val_mae: 0.2153 - learning_rate: 2.4979e-07\n",
      "Epoch 73/300\n",
      "41/41 - 3s - 64ms/step - loss: 0.1050 - mae: 0.1224 - val_loss: 0.4826 - val_mae: 0.2154 - learning_rate: 2.4729e-07\n",
      "Epoch 74/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1141 - mae: 0.1260 - val_loss: 0.4845 - val_mae: 0.2158 - learning_rate: 2.4482e-07\n",
      "Epoch 75/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1068 - mae: 0.1261 - val_loss: 0.4847 - val_mae: 0.2156 - learning_rate: 2.4237e-07\n",
      "Epoch 76/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1085 - mae: 0.1266 - val_loss: 0.4883 - val_mae: 0.2163 - learning_rate: 2.3995e-07\n",
      "Epoch 77/300\n",
      "41/41 - 3s - 63ms/step - loss: 0.1065 - mae: 0.1241 - val_loss: 0.4856 - val_mae: 0.2163 - learning_rate: 2.3755e-07\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Fold Validation MAE: 0.2100\n",
      "\n",
      "Training on 1953 samples, validating on 650 samples.\n",
      "Epoch 1/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.3116 - mae: 0.2378 - val_loss: 0.4562 - val_mae: 0.1993 - learning_rate: 1.1877e-06\n",
      "Epoch 2/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.2913 - mae: 0.2264 - val_loss: 0.4059 - val_mae: 0.1910 - learning_rate: 1.1759e-06\n",
      "Epoch 3/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.2669 - mae: 0.2174 - val_loss: 0.3941 - val_mae: 0.1881 - learning_rate: 1.1641e-06\n",
      "Epoch 4/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.2437 - mae: 0.2083 - val_loss: 0.3632 - val_mae: 0.1830 - learning_rate: 1.1525e-06\n",
      "Epoch 5/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.2458 - mae: 0.2069 - val_loss: 0.3629 - val_mae: 0.1837 - learning_rate: 1.1409e-06\n",
      "Epoch 6/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.2509 - mae: 0.2053 - val_loss: 0.3689 - val_mae: 0.1851 - learning_rate: 1.1295e-06\n",
      "Epoch 7/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.2304 - mae: 0.2029 - val_loss: 0.3550 - val_mae: 0.1816 - learning_rate: 1.1182e-06\n",
      "Epoch 8/300\n",
      "62/62 - 4s - 60ms/step - loss: 0.2238 - mae: 0.1990 - val_loss: 0.3558 - val_mae: 0.1820 - learning_rate: 1.1070e-06\n",
      "Epoch 9/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.2232 - mae: 0.1995 - val_loss: 0.3496 - val_mae: 0.1806 - learning_rate: 1.0960e-06\n",
      "Epoch 10/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.2141 - mae: 0.1961 - val_loss: 0.3414 - val_mae: 0.1785 - learning_rate: 1.0850e-06\n",
      "Epoch 11/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.2053 - mae: 0.1953 - val_loss: 0.3408 - val_mae: 0.1788 - learning_rate: 1.0742e-06\n",
      "Epoch 12/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.2063 - mae: 0.1927 - val_loss: 0.3369 - val_mae: 0.1783 - learning_rate: 1.0634e-06\n",
      "Epoch 13/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1918 - mae: 0.1887 - val_loss: 0.3479 - val_mae: 0.1802 - learning_rate: 1.0528e-06\n",
      "Epoch 14/300\n",
      "62/62 - 4s - 64ms/step - loss: 0.1903 - mae: 0.1864 - val_loss: 0.3389 - val_mae: 0.1783 - learning_rate: 1.0423e-06\n",
      "Epoch 15/300\n",
      "62/62 - 4s - 67ms/step - loss: 0.1987 - mae: 0.1892 - val_loss: 0.3347 - val_mae: 0.1782 - learning_rate: 1.0318e-06\n",
      "Epoch 16/300\n",
      "62/62 - 4s - 64ms/step - loss: 0.2016 - mae: 0.1872 - val_loss: 0.3273 - val_mae: 0.1773 - learning_rate: 1.0215e-06\n",
      "Epoch 17/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1927 - mae: 0.1816 - val_loss: 0.3322 - val_mae: 0.1778 - learning_rate: 1.0113e-06\n",
      "Epoch 18/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1947 - mae: 0.1860 - val_loss: 0.3400 - val_mae: 0.1797 - learning_rate: 1.0012e-06\n",
      "Epoch 19/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1908 - mae: 0.1833 - val_loss: 0.3365 - val_mae: 0.1788 - learning_rate: 9.9118e-07\n",
      "Epoch 20/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1826 - mae: 0.1800 - val_loss: 0.3376 - val_mae: 0.1794 - learning_rate: 9.8127e-07\n",
      "Epoch 21/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1857 - mae: 0.1792 - val_loss: 0.3282 - val_mae: 0.1776 - learning_rate: 9.7145e-07\n",
      "Epoch 22/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1790 - mae: 0.1792 - val_loss: 0.3239 - val_mae: 0.1767 - learning_rate: 9.6174e-07\n",
      "Epoch 23/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1878 - mae: 0.1792 - val_loss: 0.3137 - val_mae: 0.1746 - learning_rate: 9.5212e-07\n",
      "Epoch 24/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1743 - mae: 0.1778 - val_loss: 0.3153 - val_mae: 0.1751 - learning_rate: 9.4260e-07\n",
      "Epoch 25/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1707 - mae: 0.1760 - val_loss: 0.3269 - val_mae: 0.1777 - learning_rate: 9.3317e-07\n",
      "Epoch 26/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1807 - mae: 0.1773 - val_loss: 0.3259 - val_mae: 0.1776 - learning_rate: 9.2384e-07\n",
      "Epoch 27/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1765 - mae: 0.1764 - val_loss: 0.3352 - val_mae: 0.1795 - learning_rate: 9.1460e-07\n",
      "Epoch 28/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1729 - mae: 0.1750 - val_loss: 0.3265 - val_mae: 0.1778 - learning_rate: 9.0546e-07\n",
      "Epoch 29/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1817 - mae: 0.1783 - val_loss: 0.3249 - val_mae: 0.1777 - learning_rate: 8.9640e-07\n",
      "Epoch 30/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1751 - mae: 0.1750 - val_loss: 0.3243 - val_mae: 0.1772 - learning_rate: 8.8744e-07\n",
      "Epoch 31/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1741 - mae: 0.1751 - val_loss: 0.3335 - val_mae: 0.1788 - learning_rate: 8.7856e-07\n",
      "Epoch 32/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1597 - mae: 0.1691 - val_loss: 0.3220 - val_mae: 0.1766 - learning_rate: 8.6978e-07\n",
      "Epoch 33/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1705 - mae: 0.1722 - val_loss: 0.3316 - val_mae: 0.1786 - learning_rate: 8.6108e-07\n",
      "Epoch 34/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1777 - mae: 0.1742 - val_loss: 0.3263 - val_mae: 0.1780 - learning_rate: 8.5247e-07\n",
      "Epoch 35/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1677 - mae: 0.1716 - val_loss: 0.3211 - val_mae: 0.1767 - learning_rate: 8.4395e-07\n",
      "Epoch 36/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1632 - mae: 0.1704 - val_loss: 0.3096 - val_mae: 0.1745 - learning_rate: 8.3551e-07\n",
      "Epoch 37/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1682 - mae: 0.1703 - val_loss: 0.3059 - val_mae: 0.1734 - learning_rate: 8.2715e-07\n",
      "Epoch 38/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1664 - mae: 0.1699 - val_loss: 0.3117 - val_mae: 0.1746 - learning_rate: 8.1888e-07\n",
      "Epoch 39/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1600 - mae: 0.1683 - val_loss: 0.3227 - val_mae: 0.1772 - learning_rate: 8.1069e-07\n",
      "Epoch 40/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1836 - mae: 0.1707 - val_loss: 0.3181 - val_mae: 0.1764 - learning_rate: 8.0258e-07\n",
      "Epoch 41/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1588 - mae: 0.1674 - val_loss: 0.3166 - val_mae: 0.1765 - learning_rate: 7.9456e-07\n",
      "Epoch 42/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1681 - mae: 0.1717 - val_loss: 0.3339 - val_mae: 0.1796 - learning_rate: 7.8661e-07\n",
      "Epoch 43/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1538 - mae: 0.1667 - val_loss: 0.3262 - val_mae: 0.1784 - learning_rate: 7.7875e-07\n",
      "Epoch 44/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1643 - mae: 0.1681 - val_loss: 0.3163 - val_mae: 0.1768 - learning_rate: 7.7096e-07\n",
      "Epoch 45/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1565 - mae: 0.1644 - val_loss: 0.3138 - val_mae: 0.1760 - learning_rate: 7.6325e-07\n",
      "Epoch 46/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1520 - mae: 0.1626 - val_loss: 0.3097 - val_mae: 0.1750 - learning_rate: 7.5562e-07\n",
      "Epoch 47/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1664 - mae: 0.1718 - val_loss: 0.3083 - val_mae: 0.1748 - learning_rate: 7.4806e-07\n",
      "Epoch 48/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1605 - mae: 0.1658 - val_loss: 0.3128 - val_mae: 0.1756 - learning_rate: 7.4058e-07\n",
      "Epoch 49/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1575 - mae: 0.1664 - val_loss: 0.3156 - val_mae: 0.1765 - learning_rate: 7.3317e-07\n",
      "Epoch 50/300\n",
      "62/62 - 4s - 60ms/step - loss: 0.1525 - mae: 0.1641 - val_loss: 0.3178 - val_mae: 0.1771 - learning_rate: 7.2584e-07\n",
      "Epoch 51/300\n",
      "62/62 - 4s - 60ms/step - loss: 0.1550 - mae: 0.1631 - val_loss: 0.3132 - val_mae: 0.1761 - learning_rate: 7.1858e-07\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 3.556989724984305e-07.\n",
      "62/62 - 4s - 62ms/step - loss: 0.1497 - mae: 0.1612 - val_loss: 0.3144 - val_mae: 0.1764 - learning_rate: 3.5570e-07\n",
      "Epoch 53/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1508 - mae: 0.1607 - val_loss: 0.3099 - val_mae: 0.1755 - learning_rate: 3.5214e-07\n",
      "Epoch 54/300\n",
      "62/62 - 4s - 60ms/step - loss: 0.1501 - mae: 0.1624 - val_loss: 0.3240 - val_mae: 0.1786 - learning_rate: 3.4862e-07\n",
      "Epoch 55/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1532 - mae: 0.1640 - val_loss: 0.3179 - val_mae: 0.1774 - learning_rate: 3.4513e-07\n",
      "Epoch 56/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1459 - mae: 0.1615 - val_loss: 0.3131 - val_mae: 0.1765 - learning_rate: 3.4168e-07\n",
      "Epoch 57/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1617 - mae: 0.1654 - val_loss: 0.3178 - val_mae: 0.1778 - learning_rate: 3.3827e-07\n",
      "Epoch 58/300\n",
      "62/62 - 4s - 60ms/step - loss: 0.1538 - mae: 0.1633 - val_loss: 0.3176 - val_mae: 0.1778 - learning_rate: 3.3488e-07\n",
      "Epoch 59/300\n",
      "62/62 - 4s - 60ms/step - loss: 0.1477 - mae: 0.1619 - val_loss: 0.3204 - val_mae: 0.1782 - learning_rate: 3.3153e-07\n",
      "Epoch 60/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1458 - mae: 0.1614 - val_loss: 0.3132 - val_mae: 0.1767 - learning_rate: 3.2822e-07\n",
      "Epoch 61/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1499 - mae: 0.1629 - val_loss: 0.3164 - val_mae: 0.1773 - learning_rate: 3.2494e-07\n",
      "Epoch 62/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1452 - mae: 0.1610 - val_loss: 0.3139 - val_mae: 0.1768 - learning_rate: 3.2169e-07\n",
      "Epoch 63/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1532 - mae: 0.1632 - val_loss: 0.3166 - val_mae: 0.1772 - learning_rate: 3.1847e-07\n",
      "Epoch 64/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1533 - mae: 0.1630 - val_loss: 0.3232 - val_mae: 0.1787 - learning_rate: 3.1529e-07\n",
      "Epoch 65/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1501 - mae: 0.1620 - val_loss: 0.3198 - val_mae: 0.1781 - learning_rate: 3.1213e-07\n",
      "Epoch 66/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1540 - mae: 0.1638 - val_loss: 0.3125 - val_mae: 0.1767 - learning_rate: 3.0901e-07\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 1.5296095057237835e-07.\n",
      "62/62 - 4s - 61ms/step - loss: 0.1516 - mae: 0.1590 - val_loss: 0.3132 - val_mae: 0.1771 - learning_rate: 1.5296e-07\n",
      "Epoch 68/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1445 - mae: 0.1617 - val_loss: 0.3121 - val_mae: 0.1769 - learning_rate: 1.5143e-07\n",
      "Epoch 69/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1455 - mae: 0.1588 - val_loss: 0.3127 - val_mae: 0.1769 - learning_rate: 1.4992e-07\n",
      "Epoch 70/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1557 - mae: 0.1610 - val_loss: 0.3142 - val_mae: 0.1773 - learning_rate: 1.4842e-07\n",
      "Epoch 71/300\n",
      "62/62 - 4s - 60ms/step - loss: 0.1471 - mae: 0.1593 - val_loss: 0.3168 - val_mae: 0.1777 - learning_rate: 1.4693e-07\n",
      "Epoch 72/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1477 - mae: 0.1612 - val_loss: 0.3175 - val_mae: 0.1778 - learning_rate: 1.4546e-07\n",
      "Epoch 73/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1472 - mae: 0.1610 - val_loss: 0.3201 - val_mae: 0.1783 - learning_rate: 1.4401e-07\n",
      "Epoch 74/300\n",
      "62/62 - 4s - 65ms/step - loss: 0.1439 - mae: 0.1597 - val_loss: 0.3191 - val_mae: 0.1780 - learning_rate: 1.4257e-07\n",
      "Epoch 75/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1521 - mae: 0.1627 - val_loss: 0.3168 - val_mae: 0.1777 - learning_rate: 1.4114e-07\n",
      "Epoch 76/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1542 - mae: 0.1607 - val_loss: 0.3207 - val_mae: 0.1786 - learning_rate: 1.3973e-07\n",
      "Epoch 77/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1531 - mae: 0.1609 - val_loss: 0.3217 - val_mae: 0.1786 - learning_rate: 1.3834e-07\n",
      "Epoch 78/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1407 - mae: 0.1587 - val_loss: 0.3226 - val_mae: 0.1788 - learning_rate: 1.3695e-07\n",
      "Epoch 79/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1418 - mae: 0.1572 - val_loss: 0.3208 - val_mae: 0.1785 - learning_rate: 1.3558e-07\n",
      "Epoch 80/300\n",
      "62/62 - 4s - 62ms/step - loss: 0.1527 - mae: 0.1594 - val_loss: 0.3216 - val_mae: 0.1788 - learning_rate: 1.3423e-07\n",
      "Epoch 81/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1504 - mae: 0.1585 - val_loss: 0.3217 - val_mae: 0.1787 - learning_rate: 1.3288e-07\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 6.577767663884515e-08.\n",
      "62/62 - 4s - 62ms/step - loss: 0.1433 - mae: 0.1578 - val_loss: 0.3242 - val_mae: 0.1792 - learning_rate: 6.5778e-08\n",
      "Epoch 83/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1392 - mae: 0.1581 - val_loss: 0.3231 - val_mae: 0.1790 - learning_rate: 6.5120e-08\n",
      "Epoch 84/300\n",
      "62/62 - 4s - 61ms/step - loss: 0.1530 - mae: 0.1602 - val_loss: 0.3217 - val_mae: 0.1788 - learning_rate: 6.4469e-08\n",
      "Epoch 85/300\n",
      "62/62 - 4s - 66ms/step - loss: 0.1488 - mae: 0.1579 - val_loss: 0.3190 - val_mae: 0.1781 - learning_rate: 6.3824e-08\n",
      "Epoch 86/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1417 - mae: 0.1590 - val_loss: 0.3188 - val_mae: 0.1781 - learning_rate: 6.3186e-08\n",
      "Epoch 87/300\n",
      "62/62 - 4s - 63ms/step - loss: 0.1535 - mae: 0.1590 - val_loss: 0.3192 - val_mae: 0.1781 - learning_rate: 6.2554e-08\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Fold Validation MAE: 0.1734\n",
      "\n",
      "Average CV MAE: 0.2186\n",
      "\n",
      "Training on the full train dataset and testing on unseen data...\n",
      "Epoch 1/300\n",
      "82/82 - 5s - 62ms/step - loss: 0.2701 - mae: 0.2178 - val_loss: 0.6251 - val_mae: 0.2519 - learning_rate: 3.1277e-07\n",
      "Epoch 2/300\n",
      "82/82 - 5s - 62ms/step - loss: 0.2782 - mae: 0.2164 - val_loss: 0.6072 - val_mae: 0.2495 - learning_rate: 3.0964e-07\n",
      "Epoch 3/300\n",
      "82/82 - 5s - 65ms/step - loss: 0.2612 - mae: 0.2120 - val_loss: 0.5946 - val_mae: 0.2480 - learning_rate: 3.0655e-07\n",
      "Epoch 4/300\n",
      "82/82 - 5s - 66ms/step - loss: 0.2595 - mae: 0.2114 - val_loss: 0.5877 - val_mae: 0.2470 - learning_rate: 3.0348e-07\n",
      "Epoch 5/300\n",
      "82/82 - 5s - 63ms/step - loss: 0.2509 - mae: 0.2103 - val_loss: 0.5812 - val_mae: 0.2460 - learning_rate: 3.0045e-07\n",
      "Epoch 6/300\n",
      "82/82 - 5s - 63ms/step - loss: 0.2427 - mae: 0.2062 - val_loss: 0.5779 - val_mae: 0.2455 - learning_rate: 2.9744e-07\n",
      "Epoch 7/300\n",
      "82/82 - 5s - 63ms/step - loss: 0.2430 - mae: 0.2080 - val_loss: 0.5730 - val_mae: 0.2449 - learning_rate: 2.9447e-07\n",
      "Epoch 8/300\n",
      "82/82 - 5s - 64ms/step - loss: 0.2367 - mae: 0.2054 - val_loss: 0.5687 - val_mae: 0.2443 - learning_rate: 2.9152e-07\n",
      "Epoch 9/300\n",
      "82/82 - 5s - 64ms/step - loss: 0.2318 - mae: 0.2019 - val_loss: 0.5670 - val_mae: 0.2440 - learning_rate: 2.8861e-07\n",
      "Epoch 10/300\n",
      "82/82 - 5s - 64ms/step - loss: 0.2256 - mae: 0.1993 - val_loss: 0.5657 - val_mae: 0.2439 - learning_rate: 2.8572e-07\n",
      "Epoch 11/300\n",
      "82/82 - 5s - 62ms/step - loss: 0.2351 - mae: 0.2000 - val_loss: 0.5664 - val_mae: 0.2442 - learning_rate: 2.8286e-07\n",
      "Epoch 12/300\n",
      "82/82 - 5s - 62ms/step - loss: 0.2248 - mae: 0.1994 - val_loss: 0.5636 - val_mae: 0.2436 - learning_rate: 2.8003e-07\n",
      "Epoch 13/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2255 - mae: 0.1984 - val_loss: 0.5625 - val_mae: 0.2431 - learning_rate: 2.7723e-07\n",
      "Epoch 14/300\n",
      "82/82 - 5s - 62ms/step - loss: 0.2301 - mae: 0.1997 - val_loss: 0.5603 - val_mae: 0.2429 - learning_rate: 2.7446e-07\n",
      "Epoch 15/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2241 - mae: 0.1978 - val_loss: 0.5617 - val_mae: 0.2432 - learning_rate: 2.7172e-07\n",
      "Epoch 16/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2242 - mae: 0.1982 - val_loss: 0.5606 - val_mae: 0.2430 - learning_rate: 2.6900e-07\n",
      "Epoch 17/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2221 - mae: 0.1957 - val_loss: 0.5602 - val_mae: 0.2431 - learning_rate: 2.6631e-07\n",
      "Epoch 18/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2244 - mae: 0.1976 - val_loss: 0.5601 - val_mae: 0.2430 - learning_rate: 2.6365e-07\n",
      "Epoch 19/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2232 - mae: 0.1970 - val_loss: 0.5624 - val_mae: 0.2435 - learning_rate: 2.6101e-07\n",
      "Epoch 20/300\n",
      "82/82 - 5s - 62ms/step - loss: 0.2126 - mae: 0.1949 - val_loss: 0.5631 - val_mae: 0.2437 - learning_rate: 2.5840e-07\n",
      "Epoch 21/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2114 - mae: 0.1923 - val_loss: 0.5591 - val_mae: 0.2430 - learning_rate: 2.5582e-07\n",
      "Epoch 22/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2072 - mae: 0.1925 - val_loss: 0.5595 - val_mae: 0.2430 - learning_rate: 2.5326e-07\n",
      "Epoch 23/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2188 - mae: 0.1930 - val_loss: 0.5581 - val_mae: 0.2428 - learning_rate: 2.5073e-07\n",
      "Epoch 24/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2045 - mae: 0.1919 - val_loss: 0.5587 - val_mae: 0.2433 - learning_rate: 2.4822e-07\n",
      "Epoch 25/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2139 - mae: 0.1943 - val_loss: 0.5578 - val_mae: 0.2430 - learning_rate: 2.4574e-07\n",
      "Epoch 26/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2151 - mae: 0.1935 - val_loss: 0.5557 - val_mae: 0.2424 - learning_rate: 2.4328e-07\n",
      "Epoch 27/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2138 - mae: 0.1949 - val_loss: 0.5566 - val_mae: 0.2427 - learning_rate: 2.4085e-07\n",
      "Epoch 28/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2112 - mae: 0.1930 - val_loss: 0.5581 - val_mae: 0.2428 - learning_rate: 2.3844e-07\n",
      "Epoch 29/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2042 - mae: 0.1906 - val_loss: 0.5616 - val_mae: 0.2433 - learning_rate: 2.3605e-07\n",
      "Epoch 30/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2155 - mae: 0.1911 - val_loss: 0.5602 - val_mae: 0.2431 - learning_rate: 2.3369e-07\n",
      "Epoch 31/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2066 - mae: 0.1898 - val_loss: 0.5619 - val_mae: 0.2435 - learning_rate: 2.3136e-07\n",
      "Epoch 32/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2110 - mae: 0.1905 - val_loss: 0.5633 - val_mae: 0.2438 - learning_rate: 2.2904e-07\n",
      "Epoch 33/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2087 - mae: 0.1892 - val_loss: 0.5637 - val_mae: 0.2438 - learning_rate: 2.2675e-07\n",
      "Epoch 34/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2038 - mae: 0.1871 - val_loss: 0.5641 - val_mae: 0.2438 - learning_rate: 2.2448e-07\n",
      "Epoch 35/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2014 - mae: 0.1879 - val_loss: 0.5640 - val_mae: 0.2439 - learning_rate: 2.2224e-07\n",
      "Epoch 36/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1967 - mae: 0.1876 - val_loss: 0.5635 - val_mae: 0.2438 - learning_rate: 2.2002e-07\n",
      "Epoch 37/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2049 - mae: 0.1876 - val_loss: 0.5660 - val_mae: 0.2443 - learning_rate: 2.1782e-07\n",
      "Epoch 38/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2041 - mae: 0.1896 - val_loss: 0.5703 - val_mae: 0.2453 - learning_rate: 2.1564e-07\n",
      "Epoch 39/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2046 - mae: 0.1870 - val_loss: 0.5692 - val_mae: 0.2450 - learning_rate: 2.1348e-07\n",
      "Epoch 40/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2004 - mae: 0.1880 - val_loss: 0.5704 - val_mae: 0.2453 - learning_rate: 2.1135e-07\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.0461697996788644e-07.\n",
      "82/82 - 5s - 61ms/step - loss: 0.1986 - mae: 0.1874 - val_loss: 0.5691 - val_mae: 0.2450 - learning_rate: 1.0462e-07\n",
      "Epoch 42/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2030 - mae: 0.1867 - val_loss: 0.5680 - val_mae: 0.2450 - learning_rate: 1.0357e-07\n",
      "Epoch 43/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.1898 - mae: 0.1836 - val_loss: 0.5687 - val_mae: 0.2450 - learning_rate: 1.0254e-07\n",
      "Epoch 44/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.1918 - mae: 0.1831 - val_loss: 0.5668 - val_mae: 0.2448 - learning_rate: 1.0151e-07\n",
      "Epoch 45/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1987 - mae: 0.1860 - val_loss: 0.5674 - val_mae: 0.2448 - learning_rate: 1.0049e-07\n",
      "Epoch 46/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1932 - mae: 0.1833 - val_loss: 0.5685 - val_mae: 0.2451 - learning_rate: 9.9490e-08\n",
      "Epoch 47/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.2029 - mae: 0.1854 - val_loss: 0.5672 - val_mae: 0.2448 - learning_rate: 9.8495e-08\n",
      "Epoch 48/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1926 - mae: 0.1856 - val_loss: 0.5674 - val_mae: 0.2447 - learning_rate: 9.7510e-08\n",
      "Epoch 49/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1992 - mae: 0.1836 - val_loss: 0.5679 - val_mae: 0.2448 - learning_rate: 9.6535e-08\n",
      "Epoch 50/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1939 - mae: 0.1843 - val_loss: 0.5688 - val_mae: 0.2449 - learning_rate: 9.5569e-08\n",
      "Epoch 51/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1930 - mae: 0.1847 - val_loss: 0.5697 - val_mae: 0.2451 - learning_rate: 9.4614e-08\n",
      "Epoch 52/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1972 - mae: 0.1838 - val_loss: 0.5697 - val_mae: 0.2452 - learning_rate: 9.3668e-08\n",
      "Epoch 53/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1966 - mae: 0.1828 - val_loss: 0.5693 - val_mae: 0.2451 - learning_rate: 9.2731e-08\n",
      "Epoch 54/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2022 - mae: 0.1871 - val_loss: 0.5676 - val_mae: 0.2449 - learning_rate: 9.1804e-08\n",
      "Epoch 55/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.1956 - mae: 0.1862 - val_loss: 0.5701 - val_mae: 0.2453 - learning_rate: 9.0886e-08\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 4.498835792787759e-08.\n",
      "82/82 - 5s - 61ms/step - loss: 0.2021 - mae: 0.1871 - val_loss: 0.5709 - val_mae: 0.2455 - learning_rate: 4.4988e-08\n",
      "Epoch 57/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.1915 - mae: 0.1842 - val_loss: 0.5707 - val_mae: 0.2453 - learning_rate: 4.4538e-08\n",
      "Epoch 58/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1952 - mae: 0.1848 - val_loss: 0.5721 - val_mae: 0.2456 - learning_rate: 4.4093e-08\n",
      "Epoch 59/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.2010 - mae: 0.1851 - val_loss: 0.5716 - val_mae: 0.2457 - learning_rate: 4.3652e-08\n",
      "Epoch 60/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1967 - mae: 0.1858 - val_loss: 0.5714 - val_mae: 0.2456 - learning_rate: 4.3216e-08\n",
      "Epoch 61/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1959 - mae: 0.1846 - val_loss: 0.5706 - val_mae: 0.2455 - learning_rate: 4.2783e-08\n",
      "Epoch 62/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1971 - mae: 0.1832 - val_loss: 0.5721 - val_mae: 0.2458 - learning_rate: 4.2356e-08\n",
      "Epoch 63/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.1991 - mae: 0.1837 - val_loss: 0.5731 - val_mae: 0.2460 - learning_rate: 4.1932e-08\n",
      "Epoch 64/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1872 - mae: 0.1822 - val_loss: 0.5728 - val_mae: 0.2458 - learning_rate: 4.1513e-08\n",
      "Epoch 65/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1873 - mae: 0.1823 - val_loss: 0.5722 - val_mae: 0.2459 - learning_rate: 4.1098e-08\n",
      "Epoch 66/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1936 - mae: 0.1837 - val_loss: 0.5729 - val_mae: 0.2461 - learning_rate: 4.0687e-08\n",
      "Epoch 67/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1903 - mae: 0.1835 - val_loss: 0.5733 - val_mae: 0.2461 - learning_rate: 4.0280e-08\n",
      "Epoch 68/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1934 - mae: 0.1856 - val_loss: 0.5745 - val_mae: 0.2463 - learning_rate: 3.9877e-08\n",
      "Epoch 69/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1913 - mae: 0.1837 - val_loss: 0.5736 - val_mae: 0.2461 - learning_rate: 3.9478e-08\n",
      "Epoch 70/300\n",
      "82/82 - 5s - 61ms/step - loss: 0.1937 - mae: 0.1828 - val_loss: 0.5744 - val_mae: 0.2462 - learning_rate: 3.9083e-08\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.9346304114264967e-08.\n",
      "82/82 - 5s - 59ms/step - loss: 0.1969 - mae: 0.1831 - val_loss: 0.5726 - val_mae: 0.2460 - learning_rate: 1.9346e-08\n",
      "Epoch 72/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1918 - mae: 0.1824 - val_loss: 0.5739 - val_mae: 0.2461 - learning_rate: 1.9153e-08\n",
      "Epoch 73/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1967 - mae: 0.1839 - val_loss: 0.5740 - val_mae: 0.2462 - learning_rate: 1.8961e-08\n",
      "Epoch 74/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1843 - mae: 0.1804 - val_loss: 0.5740 - val_mae: 0.2461 - learning_rate: 1.8772e-08\n",
      "Epoch 75/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1891 - mae: 0.1817 - val_loss: 0.5750 - val_mae: 0.2463 - learning_rate: 1.8584e-08\n",
      "Epoch 76/300\n",
      "82/82 - 5s - 60ms/step - loss: 0.1901 - mae: 0.1832 - val_loss: 0.5745 - val_mae: 0.2461 - learning_rate: 1.8398e-08\n",
      "Final Test Loss: 0.5557429790496826 MAE: 0.2424231320619583\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 4) Train-Test Split (chronological on Year)\n",
    "# ------------------------------------------------------------------\n",
    "split_year = 2016\n",
    "train_df = df_agg[df_agg[\"Year\"] < split_year].copy()\n",
    "test_df  = df_agg[df_agg[\"Year\"] >= split_year].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"Year\"] + target_cols)\n",
    "y_train = train_df[target_cols]\n",
    "\n",
    "X_test  = test_df.drop(columns=[\"Year\"] + target_cols)\n",
    "y_test  = test_df[target_cols]\n",
    "\n",
    "model_input_columns = X_train.columns.tolist()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test  shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Build & Train a FFNN with Callbacks & Cross-Validation\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Multi-threading\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"20\"\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(20)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(20)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(input_dim,)),\n",
    "\n",
    "        Dense(2048, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(512, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(256, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(128, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(64, activation='swish'),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Dense(64, activation='swish'),\n",
    "        Dense(3, activation='linear')\n",
    "    ])\n",
    "    optimizer = AdamW(learning_rate=0.0005, weight_decay=1e-5)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def learning_rate_scheduler(epoch, lr):\n",
    "    if epoch == 0:\n",
    "        return lr * 5\n",
    "    return lr * 0.99\n",
    "\n",
    "# Initializing\n",
    "nn_model = build_nn_model(X_train.shape[1])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, verbose=1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Time Series Cross-Validation\n",
    "# ------------------------------------------------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "cv_results = []\n",
    "\n",
    "for train_index, val_index in tscv.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    print(f\"\\nTraining on {len(train_index)} samples, validating on {len(val_index)} samples.\")\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(learning_rate_scheduler)\n",
    "    history = nn_model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        epochs=300,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        callbacks=[early_stop, reduce_lr, lr_scheduler],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    val_predictions = nn_model.predict(X_val_fold)\n",
    "    val_mae = mean_absolute_error(y_val_fold, val_predictions)\n",
    "    cv_results.append(val_mae)\n",
    "    print(f\"Fold Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage CV MAE: {np.mean(cv_results):.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Final Training & Testing\n",
    "# ------------------------------------------------------------------\n",
    "if not X_test.empty:\n",
    "    print(\"\\nTraining on the full train dataset and testing on unseen data...\")\n",
    "    history = nn_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=300,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop, reduce_lr, lr_scheduler],\n",
    "        verbose=2\n",
    "    )\n",
    "    final_loss, final_mae = nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Final Test Loss:\", final_loss, \"MAE:\", final_mae)\n",
    "else:\n",
    "    print(\"No test set to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6a4fdb-ca86-4185-8b79-46503070f79f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_extrapolated_noc_year(\n",
    "    noc_code, \n",
    "    year_value,\n",
    "    df_agg,              \n",
    "    scaler_features,     \n",
    "    scaler_targets,      \n",
    "    nn_model,            \n",
    "    feature_cols,        \n",
    "    encoder_files,       \n",
    "    additional_features=None,  \n",
    "    target_cols=[\"Medal_Gold\",\"Medal_Silver\",\"Medal_Bronze\"],\n",
    "    max_history_medals=10,\n",
    "    max_close_points=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict medals for a given NOC and year using historical data and optional additional features.\n",
    "    \"\"\"\n",
    "\n",
    "    noc_col = f\"NOC_{noc_code}\"\n",
    "    if noc_col not in df_agg.columns:\n",
    "        print(f\"[Extrapolation] Column {noc_col} not found in df_agg. No data for NOC={noc_code}.\")\n",
    "        return None\n",
    "\n",
    "    df_noc_all = df_agg[df_agg[noc_col] == 1].copy()\n",
    "    df_noc_all_sorted = df_noc_all.sort_values(\"Year\")\n",
    "\n",
    "    df_noc_all_sorted[\"year_diff\"] = (df_noc_all_sorted[\"Year\"] - year_value).abs()\n",
    "    df_noc_closest = df_noc_all_sorted.nsmallest(max_close_points, \"year_diff\").copy()\n",
    "\n",
    "    empty_row = pd.DataFrame(columns=feature_cols, index=[0]).fillna(0.0)\n",
    "    empty_row[\"Year\"] = float(year_value)\n",
    "\n",
    "    for c in df_agg.columns:\n",
    "        if c.startswith(\"NOC_\"):\n",
    "            empty_row[c] = 1.0 if c == noc_col else 0.0\n",
    "\n",
    "    def encode_additional_features(new_data, encoder_files, feature_cols):\n",
    "        with open(encoder_files[\"hashers\"], \"rb\") as f:\n",
    "            hashers = pickle.load(f)\n",
    "\n",
    "        encoded_features = pd.DataFrame(columns=feature_cols).fillna(0)\n",
    "        for feature, value in new_data.items():\n",
    "            if feature in hashers:\n",
    "                hashed_feature = hashers[feature].transform([[value]])\n",
    "                hashed_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "                    hashed_feature, columns=[f\"{feature}_hashed_{i}\" for i in range(2100)]\n",
    "                )\n",
    "                for col in hashed_df.columns:\n",
    "                    if col in encoded_features.columns:\n",
    "                        encoded_features[col] = hashed_df[col]\n",
    "\n",
    "        return encoded_features\n",
    "\n",
    "    if additional_features:\n",
    "        encoded_new_features = encode_additional_features(additional_features, encoder_files, feature_cols)\n",
    "        empty_row.update(encoded_new_features)\n",
    "\n",
    "    empty_row = empty_row.reindex(columns=feature_cols, fill_value=0)\n",
    "\n",
    "    # Moving average for extrapolation\n",
    "    def moving_average_baseline(x_vals, y_vals, year_value, window=3):\n",
    "        valid_indices = x_vals < year_value\n",
    "        if np.sum(valid_indices) < window:\n",
    "            window = np.sum(valid_indices)\n",
    "        if window < 1:\n",
    "            return np.mean(y_vals)  \n",
    "        sorted_indices = np.argsort(x_vals[valid_indices])\n",
    "        moving_avg = np.mean(y_vals[valid_indices][sorted_indices][-window:])\n",
    "        return moving_avg\n",
    "\n",
    "    for col in feature_cols:\n",
    "        if col.startswith(\"NOC_\") or col.startswith(\"Medal_\") or col == \"Year\":\n",
    "            continue\n",
    "        if col not in df_noc_closest.columns:\n",
    "            empty_row[col] = 0.0\n",
    "            continue\n",
    "\n",
    "        valid_rows = df_noc_closest[~df_noc_closest[col].isna()].copy()\n",
    "        if valid_rows.empty:\n",
    "            empty_row[col] = 0.0\n",
    "            continue\n",
    "\n",
    "        x_vals = valid_rows[\"Year\"].to_numpy().astype(float)\n",
    "        y_vals = valid_rows[col].to_numpy().astype(float)\n",
    "\n",
    "        pred_val = moving_average_baseline(x_vals, y_vals, year_value)\n",
    "        empty_row[col] = pred_val\n",
    "\n",
    "    # Ensure the feature order matches training data\n",
    "    X_unscaled = empty_row[feature_cols].copy()\n",
    "    X_scaled = scaler_features.transform(X_unscaled)\n",
    "\n",
    "    preds_scaled = nn_model.predict(X_scaled)\n",
    "    preds_real = scaler_targets.inverse_transform(preds_scaled)\n",
    "    gold, silver, bronze = preds_real[0]\n",
    "\n",
    "    print(f\"\\n[Extrapolated] Predicted medals for NOC={noc_code}, Year={year_value}:\")\n",
    "    print(f\"  Gold={gold:.1f}, Silver={silver:.1f}, Bronze={bronze:.1f}\")\n",
    "\n",
    "    return (gold, silver, bronze)\n",
    "\n",
    "encoder_files = {\n",
    "    \"hashers\": \"hashers.pkl\"\n",
    "}\n",
    "\n",
    "predicted_medals = predict_extrapolated_noc_year(\n",
    "    noc_code=\"CHN\",\n",
    "    year_value=2024,\n",
    "    df_agg=df_agg,\n",
    "    scaler_features=scaler_features,\n",
    "    scaler_targets=scaler_targets,\n",
    "    nn_model=nn_model,\n",
    "    feature_cols=feature_cols,\n",
    "    encoder_files=encoder_files,\n",
    "    additional_features={\"Sport\": \"Basketball\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a0c6d8e-ba0b-4250-b434-afeea0e43e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting for NOC=CHN, Year=2012\n",
      "[Debug] No existing data for NOC=CHN in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2012:\n",
      "  Gold=-0.6, Silver=0.4, Bronze=0.8\n",
      "Predicted medals: Gold=-0.6, Silver=0.4, Bronze=0.8\n",
      "No actual data for NOC=CHN, Year=2012\n",
      "\n",
      "Predicting for NOC=USA, Year=2012\n",
      "[Debug] No existing data for NOC=USA in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=USA, Year=2012:\n",
      "  Gold=6.4, Silver=0.6, Bronze=0.4\n",
      "Predicted medals: Gold=6.4, Silver=0.6, Bronze=0.4\n",
      "No actual data for NOC=USA, Year=2012\n",
      "\n",
      "Predicting for NOC=GBR, Year=2012\n",
      "[Debug] No existing data for NOC=GBR in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=GBR, Year=2012:\n",
      "  Gold=-0.6, Silver=0.4, Bronze=1.4\n",
      "Predicted medals: Gold=-0.6, Silver=0.4, Bronze=1.4\n",
      "No actual data for NOC=GBR, Year=2012\n",
      "\n",
      "Predicting for NOC=RUS, Year=2012\n",
      "[Debug] No existing data for NOC=RUS in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=RUS, Year=2012:\n",
      "  Gold=-1.0, Silver=0.0, Bronze=0.3\n",
      "Predicted medals: Gold=-1.0, Silver=0.0, Bronze=0.3\n",
      "No actual data for NOC=RUS, Year=2012\n",
      "\n",
      "Predicting for NOC=GER, Year=2012\n",
      "[Debug] No existing data for NOC=GER in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=GER, Year=2012:\n",
      "  Gold=0.4, Silver=2.5, Bronze=1.7\n",
      "Predicted medals: Gold=0.4, Silver=2.5, Bronze=1.7\n",
      "No actual data for NOC=GER, Year=2012\n",
      "\n",
      "Predicting for NOC=CHN, Year=2016\n",
      "[Debug] No existing data for NOC=CHN in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2016:\n",
      "  Gold=-0.6, Silver=0.4, Bronze=0.8\n",
      "Predicted medals: Gold=-0.6, Silver=0.4, Bronze=0.8\n",
      "No actual data for NOC=CHN, Year=2016\n",
      "\n",
      "Predicting for NOC=USA, Year=2016\n",
      "[Debug] No existing data for NOC=USA in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=USA, Year=2016:\n",
      "  Gold=6.4, Silver=0.6, Bronze=0.4\n",
      "Predicted medals: Gold=6.4, Silver=0.6, Bronze=0.4\n",
      "No actual data for NOC=USA, Year=2016\n",
      "\n",
      "Predicting for NOC=GBR, Year=2016\n",
      "[Debug] No existing data for NOC=GBR in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=GBR, Year=2016:\n",
      "  Gold=-0.6, Silver=0.4, Bronze=1.4\n",
      "Predicted medals: Gold=-0.6, Silver=0.4, Bronze=1.4\n",
      "No actual data for NOC=GBR, Year=2016\n",
      "\n",
      "Predicting for NOC=RUS, Year=2016\n",
      "[Debug] No existing data for NOC=RUS in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=RUS, Year=2016:\n",
      "  Gold=-1.0, Silver=0.0, Bronze=0.3\n",
      "Predicted medals: Gold=-1.0, Silver=0.0, Bronze=0.3\n",
      "No actual data for NOC=RUS, Year=2016\n",
      "\n",
      "Predicting for NOC=GER, Year=2016\n",
      "[Debug] No existing data for NOC=GER in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=GER, Year=2016:\n",
      "  Gold=0.4, Silver=2.5, Bronze=1.7\n",
      "Predicted medals: Gold=0.4, Silver=2.5, Bronze=1.7\n",
      "No actual data for NOC=GER, Year=2016\n",
      "\n",
      "Predicting for NOC=CHN, Year=2020\n",
      "[Debug] No existing data for NOC=CHN in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2020:\n",
      "  Gold=-0.6, Silver=0.4, Bronze=0.8\n",
      "Predicted medals: Gold=-0.6, Silver=0.4, Bronze=0.8\n",
      "No actual data for NOC=CHN, Year=2020\n",
      "\n",
      "Predicting for NOC=USA, Year=2020\n",
      "[Debug] No existing data for NOC=USA in df_agg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_33040\\885964429.py:53: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m    139\u001b[0m noc_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGBR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGER\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 140\u001b[0m comparison_results \u001b[38;5;241m=\u001b[39m predict_and_compare(\n\u001b[0;32m    141\u001b[0m     noc_list\u001b[38;5;241m=\u001b[39mnoc_list,\n\u001b[0;32m    142\u001b[0m     df_agg\u001b[38;5;241m=\u001b[39mdf_agg,\n\u001b[0;32m    143\u001b[0m     scaler_features\u001b[38;5;241m=\u001b[39mscaler_features,\n\u001b[0;32m    144\u001b[0m     scaler_targets\u001b[38;5;241m=\u001b[39mscaler_targets,\n\u001b[0;32m    145\u001b[0m     nn_model\u001b[38;5;241m=\u001b[39mnn_model,\n\u001b[0;32m    146\u001b[0m     feature_cols\u001b[38;5;241m=\u001b[39mfeature_cols,\n\u001b[0;32m    147\u001b[0m     show_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    149\u001b[0m selected_nocs \u001b[38;5;241m=\u001b[39m noc_list\n\u001b[0;32m    150\u001b[0m selected_years \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2012\u001b[39m, \u001b[38;5;241m2016\u001b[39m, \u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m2024\u001b[39m]\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36mpredict_and_compare\u001b[1;34m(noc_list, df_agg, scaler_features, scaler_targets, nn_model, feature_cols, check_years, show_history)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get predictions for NOC and year\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     pred_result \u001b[38;5;241m=\u001b[39m predict_extrapolated_noc_year(\n\u001b[0;32m     31\u001b[0m         noc_code\u001b[38;5;241m=\u001b[39mnoc,\n\u001b[0;32m     32\u001b[0m         year_value\u001b[38;5;241m=\u001b[39myear,\n\u001b[0;32m     33\u001b[0m         df_agg\u001b[38;5;241m=\u001b[39mdf_agg,\n\u001b[0;32m     34\u001b[0m         scaler_features\u001b[38;5;241m=\u001b[39mscaler_features,\n\u001b[0;32m     35\u001b[0m         scaler_targets\u001b[38;5;241m=\u001b[39mscaler_targets,\n\u001b[0;32m     36\u001b[0m         nn_model\u001b[38;5;241m=\u001b[39mnn_model,\n\u001b[0;32m     37\u001b[0m         feature_cols\u001b[38;5;241m=\u001b[39mfeature_cols\n\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pred_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m         gold_pred, silver_pred, bronze_pred \u001b[38;5;241m=\u001b[39m pred_result\n",
      "Cell \u001b[1;32mIn[13], line 105\u001b[0m, in \u001b[0;36mpredict_extrapolated_noc_year\u001b[1;34m(noc_code, year_value, df_agg, scaler_features, scaler_targets, nn_model, feature_cols, target_cols, max_history_medals, max_close_points)\u001b[0m\n\u001b[0;32m    101\u001b[0m     empty_row[col] \u001b[38;5;241m=\u001b[39m pred_val\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# 4) Now we have a \"predicted\" row for the new year\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#    Separate out features\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m X_unscaled \u001b[38;5;241m=\u001b[39m empty_row[feature_cols]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 5) Scale\u001b[39;00m\n\u001b[0;32m    107\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler_features\u001b[38;5;241m.\u001b[39mtransform(X_unscaled)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:4117\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   4115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 4117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[0;32m   4120\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[0;32m   4121\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[0;32m   4122\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[0;32m   4123\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[0;32m   4124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[0;32m   4125\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:811\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    808\u001b[0m blk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[blkno]\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# Otherwise, slicing along items axis is necessary.\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_can_consolidate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_validate_ndim:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;66;03m# i.e. we dont go through here for DatetimeTZBlock\u001b[39;00m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;66;03m# A non-consolidatable block, it's easy, because there's\u001b[39;00m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;66;03m# only one item and each mgr loc is a copy of that single\u001b[39;00m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;66;03m# item.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m     deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (only_slice \u001b[38;5;129;01mor\u001b[39;00m using_copy_on_write())\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mgr_loc \u001b[38;5;129;01min\u001b[39;00m mgr_locs:\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:223\u001b[0m, in \u001b[0;36mBlock._can_consolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_can_consolidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# We _could_ consolidate for DatetimeTZDtype but don't for now.\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_extension\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:214\u001b[0m, in \u001b[0;36mBlock.is_extension\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_object\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_np_dtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_can_consolidate\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# We _could_ consolidate for DatetimeTZDtype but don't for now.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_and_compare(noc_list, df_agg, scaler_features, scaler_targets, nn_model, feature_cols, encoder_files, check_years=[2012, 2016, 2020, 2024], show_history=False, additional_features=None):\n",
    "    \"\"\"\n",
    "    Predicts medal counts using the trained model and compares with actual data.\n",
    "\n",
    "    Parameters:\n",
    "        noc_list (list): List of NOCs to predict and compare.\n",
    "        df_agg (pd.DataFrame): Aggregated dataframe with historical data.\n",
    "        scaler_features (StandardScaler): Scaler for feature normalization.\n",
    "        scaler_targets (StandardScaler): Scaler for target values.\n",
    "        nn_model (Sequential): Trained neural network model.\n",
    "        feature_cols (list): Columns used for model input.\n",
    "        encoder_files (dict): Dictionary containing paths to saved encoders.\n",
    "        check_years (list): Olympic years to check.\n",
    "        show_history (bool): Whether to display the country's medal count history.\n",
    "        additional_features (dict, optional): Additional text features such as Sport, Event, Team.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of actual vs. predicted medal counts.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for year in check_years:\n",
    "        for noc in noc_list:\n",
    "            print(f\"\\nPredicting for NOC={noc}, Year={year}\")\n",
    "\n",
    "            # Get predictions for NOC and year\n",
    "            try:\n",
    "                pred_result = predict_extrapolated_noc_year(\n",
    "                    noc_code=noc,\n",
    "                    year_value=year,\n",
    "                    df_agg=df_agg,\n",
    "                    scaler_features=scaler_features,\n",
    "                    scaler_targets=scaler_targets,\n",
    "                    nn_model=nn_model,\n",
    "                    feature_cols=feature_cols,\n",
    "                    encoder_files=encoder_files,\n",
    "                    additional_features=additional_features  # Pass extra features\n",
    "                )\n",
    "\n",
    "                if pred_result is not None:\n",
    "                    gold_pred, silver_pred, bronze_pred = pred_result\n",
    "                    print(f\"Predicted medals: Gold={gold_pred:.1f}, Silver={silver_pred:.1f}, Bronze={bronze_pred:.1f}\")\n",
    "                else:\n",
    "                    gold_pred, silver_pred, bronze_pred = None, None, None\n",
    "                    print(f\"Prediction unavailable for NOC={noc}, Year={year}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction failed for NOC={noc}, Year={year}. Error: {str(e)}\")\n",
    "                gold_pred, silver_pred, bronze_pred = None, None, None\n",
    "\n",
    "            # Get actual data\n",
    "            noc_col = f\"NOC_{noc}\"  # Adjusting for one-hot encoded columns\n",
    "            if noc_col not in df_agg.columns:\n",
    "                print(f\"[Check] No data for NOC={noc}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            df_noc_all = df_agg[df_agg[noc_col] == 1].copy()\n",
    "            df_noc_filtered = df_noc_all[df_noc_all[\"Year\"] == year]\n",
    "\n",
    "            if show_history and not df_noc_all.empty:\n",
    "                print(f\"[History] Medal history for NOC={noc} up to year {year}:\")\n",
    "                display(df_noc_all[[\"Year\", \"Medal_Gold\", \"Medal_Silver\", \"Medal_Bronze\"]].tail(10))\n",
    "\n",
    "            if not df_noc_filtered.empty:\n",
    "                gold_actual = df_noc_filtered[\"Medal_Gold\"].values[0]\n",
    "                silver_actual = df_noc_filtered[\"Medal_Silver\"].values[0]\n",
    "                bronze_actual = df_noc_filtered[\"Medal_Bronze\"].values[0]\n",
    "                print(f\"Actual medals: Gold={gold_actual}, Silver={silver_actual}, Bronze={bronze_actual}\")\n",
    "            else:\n",
    "                gold_actual, silver_actual, bronze_actual = None, None, None\n",
    "                print(f\"No actual data for NOC={noc}, Year={year}\")\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"NOC\": noc,\n",
    "                \"Year\": year,\n",
    "                \"Gold_Predicted\": gold_pred,\n",
    "                \"Silver_Predicted\": silver_pred,\n",
    "                \"Bronze_Predicted\": bronze_pred,\n",
    "                \"Gold_Actual\": gold_actual,\n",
    "                \"Silver_Actual\": silver_actual,\n",
    "                \"Bronze_Actual\": bronze_actual\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate absolute differences and percentage errors\n",
    "    results_df[\"Gold_Error_Pct\"] = (abs(results_df[\"Gold_Predicted\"] - results_df[\"Gold_Actual\"]) / results_df[\"Gold_Actual\"]) * 100\n",
    "    results_df[\"Silver_Error_Pct\"] = (abs(results_df[\"Silver_Predicted\"] - results_df[\"Silver_Actual\"]) / results_df[\"Silver_Actual\"]) * 100\n",
    "    results_df[\"Bronze_Error_Pct\"] = (abs(results_df[\"Bronze_Predicted\"] - results_df[\"Bronze_Actual\"]) / results_df[\"Bronze_Actual\"]) * 100\n",
    "\n",
    "    # Replace NaN values with -1\n",
    "    results_df.fillna(-1, inplace=True)\n",
    "\n",
    "    print(\"\\nPrediction vs Actual Summary:\")\n",
    "    display(results_df)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "noc_list = [\"CHN\", \"USA\", \"GBR\", \"RUS\", \"GER\"]\n",
    "encoder_files = {\n",
    "    \"hashers\": \"hashers.pkl\"\n",
    "}\n",
    "\n",
    "comparison_results = predict_and_compare(\n",
    "    noc_list=noc_list,\n",
    "    df_agg=df_agg,\n",
    "    scaler_features=scaler_features,\n",
    "    scaler_targets=scaler_targets,\n",
    "    nn_model=nn_model,\n",
    "    feature_cols=feature_cols,\n",
    "    encoder_files=encoder_files,\n",
    "    show_history=False,\n",
    "    additional_features={\"Sport\": \"Basketball\", \"Event\": \"100m Sprint\"}\n",
    ")\n",
    "\n",
    "def visualize_selected_nocs(results_df, selected_nocs, selected_years, color='Red'):\n",
    "    \"\"\"\n",
    "    Generate heatmaps for selected NOCs and years using a single-color theme with grayscale.\n",
    "\n",
    "    Parameters:\n",
    "        results_df (pd.DataFrame): DataFrame containing prediction vs. actual medal count results.\n",
    "        selected_nocs (list): List of NOCs to visualize.\n",
    "        selected_years (list): List of Olympic years to visualize.\n",
    "        color (str): Color theme for the heatmap (e.g., 'Blues', 'Greens', 'Reds').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the dataframe for selected NOCs and years\n",
    "    filtered_df = results_df[(results_df[\"NOC\"].isin(selected_nocs)) & \n",
    "                             (results_df[\"Year\"].isin(selected_years))]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"No data available for selected NOCs and years. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    # Generate heatmaps for Gold, Silver, and Bronze prediction errors\n",
    "    for metric in [\"Gold_Error_Pct\", \"Silver_Error_Pct\", \"Bronze_Error_Pct\"]:\n",
    "        heatmap_data = filtered_df.pivot(index=\"NOC\", columns=\"Year\", values=metric)\n",
    "\n",
    "        if not heatmap_data.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(heatmap_data, annot=True, cmap=color, fmt=\".1f\", linewidths=0.5)\n",
    "            plt.title(f\"{metric.replace('_', ' ')} Heatmap for Selected NOCs\")\n",
    "            plt.xlabel(\"Year\")\n",
    "            plt.ylabel(\"NOC\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Skipping heatmap for {metric} due to lack of valid data.\")\n",
    "\n",
    "noc_list = [\"CHN\", \"USA\", \"GBR\", \"RUS\", \"GER\"]\n",
    "comparison_results = predict_and_compare(\n",
    "    noc_list=noc_list,\n",
    "    df_agg=df_agg,\n",
    "    scaler_features=scaler_features,\n",
    "    scaler_targets=scaler_targets,\n",
    "    nn_model=nn_model,\n",
    "    feature_cols=feature_cols,\n",
    "    show_history=False\n",
    ")\n",
    "selected_nocs = noc_list\n",
    "selected_years = [2012, 2016, 2020, 2024]\n",
    "\n",
    "visualize_selected_nocs(comparison_results, selected_nocs, selected_years, color=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259382b-c11b-4ed2-8c8e-d01fd85decfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
