{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbbffdf-81bf-455e-a69e-57a20565f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Add, LeakyReLU, Activation\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.activations import gelu\n",
    "\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12f370d-a5a3-4a6d-8ad5-3da09d320c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_25660\\2285358395.py:58: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda subdf: subdf[df_num_cols].sum(numeric_only=True))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Elite_Player</th>\n",
       "      <th>Is_Host</th>\n",
       "      <th>Participation_Count</th>\n",
       "      <th>Retiring_Player</th>\n",
       "      <th>Seed_Player</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>No medal</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>...</th>\n",
       "      <th>NOC_URU</th>\n",
       "      <th>NOC_USA</th>\n",
       "      <th>NOC_UZB</th>\n",
       "      <th>NOC_VAN</th>\n",
       "      <th>NOC_VEN</th>\n",
       "      <th>NOC_VIE</th>\n",
       "      <th>NOC_VIN</th>\n",
       "      <th>NOC_YEM</th>\n",
       "      <th>NOC_ZAM</th>\n",
       "      <th>NOC_ZIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1896</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1896</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>87</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1896</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1896</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>76</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>2024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3097 rows × 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Elite_Player  Is_Host  Participation_Count  Retiring_Player  \\\n",
       "0     1896            20        0                   25               10   \n",
       "1     1896            16        0                   29               10   \n",
       "2     1896            35        0                  132               87   \n",
       "3     1896            30        0                   90               22   \n",
       "4     1896            24        0                  139               76   \n",
       "...    ...           ...      ...                  ...              ...   \n",
       "3092  2024             0        0                   41                4   \n",
       "3093  2024             0        0                    5                0   \n",
       "3094  2024             0        0                    4                0   \n",
       "3095  2024             0        0                   47                0   \n",
       "3096  2024             0        0                    8                0   \n",
       "\n",
       "      Seed_Player  Medal_Bronze  Medal_Gold  No medal  Medal_Silver  ...  \\\n",
       "0               5             1           2         2             0  ...   \n",
       "1               2             2           1         3             1  ...   \n",
       "2              19             3           1         9             2  ...   \n",
       "3              13             2           5        15             4  ...   \n",
       "4              12             3           3        16             3  ...   \n",
       "...           ...           ...         ...       ...           ...  ...   \n",
       "3092            0             0           0        20             0  ...   \n",
       "3093            0             0           0         4             0  ...   \n",
       "3094            0             0           0         4             0  ...   \n",
       "3095            0             1           0        31             0  ...   \n",
       "3096            0             0           0         7             0  ...   \n",
       "\n",
       "      NOC_URU  NOC_USA  NOC_UZB  NOC_VAN  NOC_VEN  NOC_VIE  NOC_VIN  NOC_YEM  \\\n",
       "0         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4         0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "3092      0.0      0.0      0.0      0.0      0.0      1.0      0.0      0.0   \n",
       "3093      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "3094      0.0      0.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "3095      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3096      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      NOC_ZAM  NOC_ZIM  \n",
       "0         0.0      0.0  \n",
       "1         0.0      0.0  \n",
       "2         0.0      0.0  \n",
       "3         0.0      0.0  \n",
       "4         0.0      0.0  \n",
       "...       ...      ...  \n",
       "3092      0.0      0.0  \n",
       "3093      0.0      0.0  \n",
       "3094      0.0      0.0  \n",
       "3095      1.0      0.0  \n",
       "3096      0.0      1.0  \n",
       "\n",
       "[3097 rows x 218 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1) Enable multi-threading in TensorFlow\n",
    "# ------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"20\"\n",
    "tf.config.threading.set_intra_op_parallelism_threads(20)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(20)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load & Basic Cleaning\n",
    "# ------------------------------------------------------------------\n",
    "data_path = \"MCM_2025\\\\2025_MCM-ICM_Problems\\\\2025_Problem_C_Data\\\\summerOly_athletes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df = df[~df['Team'].str.contains(r'\\d|\\.', na=False)]\n",
    "roman_pattern = r'(?:\\s|-)M{0,4}(?:CM|CD|D?C{0,3})(?:XC|XL|L?X{0,3})(?:IX|IV|V?I{0,3})(?:\\s|$)'\n",
    "df = df[~df['Team'].str.contains(roman_pattern, na=False, flags=re.IGNORECASE)]\n",
    "\n",
    "recent_years = [2020, 2024]\n",
    "df = df[df[\"NOC\"].isin(df[df[\"Year\"].isin(recent_years)][\"NOC\"].unique())]\n",
    "\n",
    "# Remove events that appeared <4 times\n",
    "event_counts = df[\"Event\"].value_counts()\n",
    "df = df[df[\"Event\"].isin(event_counts[event_counts >= 4].index)]\n",
    "\n",
    "# Convert Year to int\n",
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Feature Engineering (Row-level)\n",
    "# ------------------------------------------------------------------\n",
    "df[\"Seed_Player\"] = df.groupby(\"Name\")[\"Medal\"].transform(\n",
    "    lambda x: (x == \"Gold\").cumsum() + (x == \"Silver\").cumsum()\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "df[\"Elite_Player\"] = df.groupby(\"Name\")[\"Medal\"].transform(\n",
    "    lambda x: max(0, (x != \"No medal\").sum() - 1 + (x == \"Gold\").sum())\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "df[\"Participation_Count\"] = df.groupby(\"Name\")[\"Year\"].transform(\"count\")\n",
    "df[\"Retiring_Player\"] = df[\"Participation_Count\"].apply(lambda x: max(0, x - 3)).astype(int)\n",
    "\n",
    "host_cities = {2008: \"CHN\", 2012: \"GBR\", 2016: \"BRA\", 2020: \"JPN\", 2024: \"FRA\"}\n",
    "df[\"Is_Host\"] = df.apply(lambda row: 1 if host_cities.get(row['Year'], None) == row['NOC'] else 0, axis=1)\n",
    "\n",
    "df[\"NOC\"] = df[\"NOC\"].astype(str)\n",
    "df[\"Name\"] = df[\"Name\"].astype(str)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Aggregation by (Year, NOC)\n",
    "# ------------------------------------------------------------------\n",
    "df_num_cols = df.select_dtypes(include=[\"number\"]).columns.difference([\"Year\"]).tolist()\n",
    "\n",
    "df_noc_year = (\n",
    "    df.groupby([\"Year\", \"NOC\"], as_index=False)\n",
    "      .apply(lambda subdf: subdf[df_num_cols].sum(numeric_only=True))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Summation of medal counts\n",
    "medal_counts = df.groupby([\"Year\", \"NOC\"])[\"Medal\"].value_counts().unstack(fill_value=0).reset_index()\n",
    "medal_counts.rename(columns={\"Gold\": \"Medal_Gold\", \"Silver\": \"Medal_Silver\", \"Bronze\": \"Medal_Bronze\"}, inplace=True)\n",
    "\n",
    "df_noc_year = df_noc_year.merge(\n",
    "    medal_counts, on=[\"Year\", \"NOC\"], how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) One-Hot Encoding NOC\n",
    "# ------------------------------------------------------------------\n",
    "ohe_noc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_noc = ohe_noc.fit_transform(df_noc_year[[\"NOC\"]])\n",
    "noc_ohe_cols = [f\"NOC_{cat}\" for cat in ohe_noc.categories_[0]]\n",
    "\n",
    "df_noc_year.reset_index(drop=True, inplace=True)\n",
    "noc_ohe_df = pd.DataFrame(encoded_noc, columns=noc_ohe_cols)\n",
    "df_noc_year_ohe = pd.concat([df_noc_year.drop(columns=[\"NOC\"]), noc_ohe_df], axis=1)\n",
    "\n",
    "with open(\"noc_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ohe_noc, f)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) Scaling\n",
    "# ------------------------------------------------------------------\n",
    "target_cols = [\"Medal_Gold\", \"Medal_Silver\", \"Medal_Bronze\"]\n",
    "feature_cols = df_noc_year_ohe.columns.difference([\"Year\"] + target_cols)\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "scaler_targets = StandardScaler()\n",
    "\n",
    "X_all = scaler_features.fit_transform(df_noc_year_ohe[feature_cols])\n",
    "y_all = scaler_targets.fit_transform(df_noc_year_ohe[target_cols])\n",
    "\n",
    "with open(\"scaler_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_features, f)\n",
    "with open(\"scaler_targets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_targets, f)\n",
    "\n",
    "# Keep a reference to unscaled aggregator\n",
    "df_unscaled = df_noc_year_ohe.copy()\n",
    "\n",
    "df_noc_year_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "847b14a2-8599-4247-ab0b-45ee4e83f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 7) Define model building & callbacks\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def build_medal_prediction_model(input_dim):\n",
    "    def huber_loss(y_true, y_pred, delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        condition = tf.abs(error) < delta\n",
    "        return tf.reduce_mean(tf.where(condition, 0.5 * tf.square(error), delta * (tf.abs(error) - 0.5 * delta)))\n",
    "\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    x = Dense(4096, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('swish')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x1 = Dense(2048, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation('swish')(x1)\n",
    "\n",
    "    x2 = Dense(2048, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Activation('swish')(x2)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "\n",
    "    x_residual = Add()([x1, x2])\n",
    "\n",
    "    x3 = Dense(1024, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x_residual)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = Activation('swish')(x3)\n",
    "    x3 = Dropout(0.2)(x3)\n",
    "\n",
    "    x4 = Dense(512, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x3)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Activation('swish')(x4)\n",
    "    x4 = Dropout(0.2)(x4)\n",
    "\n",
    "    # Output layer\n",
    "    outputs = Dense(3, activation=\"linear\")(x4)  # Output: Gold, Silver, Bronze\n",
    "\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-5)\n",
    "    model.compile(optimizer=optimizer, loss=huber_loss, metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Baseline function with a small \"grid\" of approaches\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def build_baseline_model(x_vals, y_vals):\n",
    "    best_mae = float(\"inf\")\n",
    "    best_pred_func = None\n",
    "\n",
    "    # 1) Linear Regression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_vals.reshape(-1,1), y_vals)\n",
    "    mae = mean_absolute_error(y_vals, lr.predict(x_vals.reshape(-1,1)))\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_pred_func = (\"Linear\", lr)\n",
    "\n",
    "    # 2) Polynomial Regression (degrees 2-4)\n",
    "    for deg in [2,3,4]:\n",
    "        poly_pipeline = make_pipeline(PolynomialFeatures(deg), LinearRegression())\n",
    "        poly_pipeline.fit(x_vals.reshape(-1,1), y_vals)\n",
    "        mae_poly = mean_absolute_error(y_vals, poly_pipeline.predict(x_vals.reshape(-1,1)))\n",
    "        if mae_poly < best_mae:\n",
    "            best_mae = mae_poly\n",
    "            best_pred_func = (f\"Poly_{deg}\", poly_pipeline)\n",
    "\n",
    "    # 3) Bayesian Ridge Regression\n",
    "    br = BayesianRidge()\n",
    "    br.fit(x_vals.reshape(-1,1), y_vals)\n",
    "    mae_br = mean_absolute_error(y_vals, br.predict(x_vals.reshape(-1,1)))\n",
    "    if mae_br < best_mae:\n",
    "        best_mae = mae_br\n",
    "        best_pred_func = (\"BayesianRidge\", br)\n",
    "\n",
    "    # 4) Exponential Fit\n",
    "    def exp_func(x, a, b):\n",
    "        return a * np.exp(b * x)\n",
    "    try:\n",
    "        x_shift = x_vals - x_vals.min()\n",
    "        popt, _ = scipy.optimize.curve_fit(exp_func, x_shift, y_vals, maxfev=10000)\n",
    "        mae_exp = mean_absolute_error(y_vals, exp_func(x_shift, *popt))\n",
    "        if mae_exp < best_mae:\n",
    "            best_mae = mae_exp\n",
    "            best_pred_func = (\"Exponential\", lambda x: exp_func(x - x_vals.min(), *popt))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return best_pred_func\n",
    "\n",
    "def predict_baseline_value(model_info, x_target):\n",
    "    \"\"\"\n",
    "    Use the best_pred_func from build_baseline_model to predict a single y value for x_target.\n",
    "    Handles different types of models gracefully.\n",
    "    \"\"\"\n",
    "    name, model = model_info\n",
    "\n",
    "    if name in [\"Linear\", \"BayesianRidge\"] or name.startswith(\"Poly_\"):\n",
    "        return model.predict([[x_target]])[0]\n",
    "\n",
    "    elif name == \"Exponential\":\n",
    "        return model(x_target)\n",
    "\n",
    "    elif name == \"Holt\":\n",
    "        # Forecasting using Holt-Winters\n",
    "        try:\n",
    "            forecast_steps = x_target - model.fittedvalues.index[-1]\n",
    "            if forecast_steps < 1:\n",
    "                forecast_steps = 1\n",
    "\n",
    "            yhat_f = model.forecast(steps=forecast_steps)\n",
    "\n",
    "            if len(yhat_f) > 0:\n",
    "                return yhat_f.iloc[-1] \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Holt-Winters forecast failed for {x_target}: {e}\")\n",
    "            return 0.0 \n",
    "\n",
    "    return 0.0  # Default return if no valid prediction can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ff431f-d2a4-4c07-9dee-92e582b1ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Workflow A: train up to <2024, EXTRAPOLATE to 2024, then compare\n",
    "# ------------------------------------------------------------------\n",
    "def timeseries_cv_train(X_train, y_train, scaler_targets, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model_cv = build_medal_prediction_model(X_fold_train.shape[1])\n",
    "        es = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "        rlr = ReduceLROnPlateau(monitor='val_loss', patience=15, factor=0.5, verbose=1)\n",
    "\n",
    "        model_cv.fit(X_fold_train, y_fold_train, epochs=200, batch_size=16,\n",
    "                     validation_data=(X_fold_val, y_fold_val),\n",
    "                     callbacks=[es, rlr], verbose=2)\n",
    "\n",
    "        val_preds = scaler_targets.inverse_transform(model_cv.predict(X_fold_val))\n",
    "        val_actual = scaler_targets.inverse_transform(y_fold_val)\n",
    "        val_mae = np.mean(np.abs(val_preds - val_actual))\n",
    "        cv_results.append(val_mae)\n",
    "        print(f\"Fold {fold+1} MAE: {val_mae:.3f}\")\n",
    "\n",
    "    print(f\"[TimeSeriesSplit] Average CV MAE: {np.mean(cv_results):.3f}\")\n",
    "\n",
    "    final_model = build_medal_prediction_model(X_train.shape[1])\n",
    "    final_model.fit(X_train, y_train, epochs=300, batch_size=64, verbose=2, callbacks=[es])\n",
    "    return final_model\n",
    "\n",
    "def train_up_to_2024(X_all, y_all, df_agg):\n",
    "    mask_train_2024 = (df_agg[\"Year\"] < 2024)\n",
    "    X_train_2024 = X_all[mask_train_2024]\n",
    "    y_train_2024 = y_all[mask_train_2024]\n",
    "    model_2024 = timeseries_cv_train(X_train_2024, y_train_2024, scaler_targets, n_splits=3)\n",
    "    return model_2024\n",
    "\n",
    "def build_baseline_and_predict_2024(model, df_agg, feature_cols, target_cols, scaler_features, scaler_targets):\n",
    "    df_2024 = df_agg[df_agg[\"Year\"] == 2020].copy()\n",
    "    df_2024[\"Year\"] = 2024\n",
    "\n",
    "    noc_columns = [col for col in df_agg.columns if col.startswith(\"NOC_\")]\n",
    "\n",
    "    if 'NOC' not in df_agg.columns:\n",
    "        df_agg[\"NOC\"] = df_agg[noc_columns].idxmax(axis=1).str.replace(\"NOC_\", \"\")\n",
    "\n",
    "    df_2024[\"NOC\"] = df_2024[noc_columns].idxmax(axis=1).str.replace(\"NOC_\", \"\")\n",
    "\n",
    "    if \"NOC_FRA\" in df_2024.columns:\n",
    "        df_2024[\"Is_Host\"] = df_2024[\"NOC_FRA\"]  # Assuming FRA hosts 2024\n",
    "\n",
    "    numeric_feats = [c for c in feature_cols if c not in noc_columns and c != \"Year\" and c != \"Is_Host\"]\n",
    "\n",
    "    historical_data = df_agg[df_agg[\"Year\"] < 2024].copy()\n",
    "    historical_data[\"NOC\"] = historical_data[noc_columns].idxmax(axis=1).str.replace(\"NOC_\", \"\")\n",
    "\n",
    "    for feat in numeric_feats:\n",
    "        pivot_df = historical_data.pivot(index=\"Year\", columns=\"NOC\", values=feat).fillna(0)\n",
    "\n",
    "        for noc in pivot_df.columns:\n",
    "            xvals = np.array(pivot_df.index, dtype=float)\n",
    "            yvals = pivot_df[noc].values\n",
    "\n",
    "            if len(xvals) < 2:\n",
    "                continue\n",
    "\n",
    "            model_info = build_baseline_model(xvals, yvals)\n",
    "            df_2024.loc[df_2024[\"NOC\"] == noc, feat] = predict_baseline_value(model_info, 2024)\n",
    "\n",
    "    for col in df_agg.columns:\n",
    "        if col not in df_2024.columns:\n",
    "            df_2024[col] = 0\n",
    "\n",
    "    # Scale & predict\n",
    "    X_2024_unscaled = df_2024[feature_cols].values\n",
    "    X_2024_scaled = scaler_features.transform(X_2024_unscaled)\n",
    "\n",
    "    preds_2024_sc = model.predict(X_2024_scaled)\n",
    "    preds_2024 = scaler_targets.inverse_transform(preds_2024_sc)\n",
    "\n",
    "    df_real_2024 = df_agg[df_agg[\"Year\"] == 2024].reset_index(drop=True)\n",
    "    df_real_2024[\"NOC\"] = df_real_2024[noc_columns].idxmax(axis=1).str.replace(\"NOC_\", \"\")\n",
    "\n",
    "    actual_2024_sc = df_real_2024[target_cols].values\n",
    "    actual_2024 = scaler_targets.inverse_transform(actual_2024_sc)\n",
    "\n",
    "    df_result_2024 = pd.DataFrame({\n",
    "        \"NOC\": df_real_2024[\"NOC\"].astype(str),\n",
    "        \"Gold_Actual\": actual_2024[:, 0],\n",
    "        \"Silver_Actual\": actual_2024[:, 1],\n",
    "        \"Bronze_Actual\": actual_2024[:, 2],\n",
    "        \"Gold_Predicted\": preds_2024[:, 0],\n",
    "        \"Silver_Predicted\": preds_2024[:, 1],\n",
    "        \"Bronze_Predicted\": preds_2024[:, 2],\n",
    "    })\n",
    "\n",
    "    return df_result_2024\n",
    "\n",
    "# Visualization for 2024 results\n",
    "def visualize_2024_results(df_result_2024):\n",
    "    df = df_result_2024.copy()\n",
    "    df[\"Gold_Error_%\"] = np.where(df[\"Gold_Actual\"]>0, \n",
    "                                  abs(df[\"Gold_Predicted\"]-df[\"Gold_Actual\"])/df[\"Gold_Actual\"]*100,0)\n",
    "    df[\"Silver_Error_%\"] = np.where(df[\"Silver_Actual\"]>0,\n",
    "                                  abs(df[\"Silver_Predicted\"]-df[\"Silver_Actual\"])/df[\"Silver_Actual\"]*100,0)\n",
    "    df[\"Bronze_Error_%\"] = np.where(df[\"Bronze_Actual\"]>0,\n",
    "                                  abs(df[\"Bronze_Predicted\"]-df[\"Bronze_Actual\"])/df[\"Bronze_Actual\"]*100,0)\n",
    "\n",
    "    error_cols = [\"Gold_Error_%\",\"Silver_Error_%\",\"Bronze_Error_%\"]\n",
    "    filtered_df = df[(df[\"Gold_Actual\"]>0)|(df[\"Silver_Actual\"]>0)|(df[\"Bronze_Actual\"]>0)]\n",
    "\n",
    "    plt.figure(figsize=(18,6))\n",
    "    for i,col in enumerate(error_cols):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.hist(filtered_df[col], bins=50, range=(0,100), edgecolor='black', alpha=0.7)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(\"Error Percentage\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    exceed_100 = (filtered_df[error_cols]>100).sum()/len(filtered_df)*100\n",
    "    print(\"Percentage of data points exceeding 100% error (ignoring zero actual values):\")\n",
    "    print(exceed_100)\n",
    "\n",
    "    # line chart\n",
    "    filtered_df[\"Total_Actual\"] = filtered_df[\"Gold_Actual\"]+filtered_df[\"Silver_Actual\"]+filtered_df[\"Bronze_Actual\"]\n",
    "    filtered_df.sort_values(by=\"Total_Actual\", ascending=False, inplace=True)\n",
    "\n",
    "    nocs = filtered_df[\"NOC\"].values\n",
    "    gold_actual = filtered_df[\"Gold_Actual\"].values\n",
    "    gold_pred = filtered_df[\"Gold_Predicted\"].values\n",
    "    silver_actual = filtered_df[\"Silver_Actual\"].values\n",
    "    silver_pred = filtered_df[\"Silver_Predicted\"].values\n",
    "    bronze_actual = filtered_df[\"Bronze_Actual\"].values\n",
    "    bronze_pred = filtered_df[\"Bronze_Predicted\"].values\n",
    "\n",
    "    # gold\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(nocs, gold_actual,label=\"Actual Gold\",marker='o')\n",
    "    plt.plot(nocs, gold_pred,label=\"Predicted Gold\",marker='x')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"NOC\")\n",
    "    plt.ylabel(\"Gold Medals\")\n",
    "    plt.title(\"Gold Medals: Actual vs Predicted (2024 Extrapolated)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # silver\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(nocs, silver_actual,label=\"Actual Silver\",marker='o')\n",
    "    plt.plot(nocs, silver_pred,label=\"Predicted Silver\",marker='x')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"NOC\")\n",
    "    plt.ylabel(\"Silver Medals\")\n",
    "    plt.title(\"Silver Medals: Actual vs Predicted (2024 Extrapolated)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # bronze\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(nocs, bronze_actual,label=\"Actual Bronze\",marker='o')\n",
    "    plt.plot(nocs, bronze_pred,label=\"Predicted Bronze\",marker='x')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"NOC\")\n",
    "    plt.ylabel(\"Bronze Medals\")\n",
    "    plt.title(\"Bronze Medals: Actual vs Predicted (2024 Extrapolated)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # final MAE\n",
    "    gold_mae = mean_absolute_error(gold_actual, gold_pred)\n",
    "    silver_mae = mean_absolute_error(silver_actual, silver_pred)\n",
    "    bronze_mae = mean_absolute_error(bronze_actual, bronze_pred)\n",
    "    print(f\"Gold Medal MAE: {gold_mae:.2f}\")\n",
    "    print(f\"Silver Medal MAE: {silver_mae:.2f}\")\n",
    "    print(f\"Bronze Medal MAE: {bronze_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ab254a-9018-41b6-aa37-f9c0b3d5cafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: {'loss_function': <function weighted_huber_loss at 0x0000027783E5C680>, 'layer_sizes': (1024, 512, 64), 'dropout_rate': 0.3, 'batch_size': 32, 'activation_function': 'relu', 'focus_top': True}\n",
      "Epoch 1/100\n",
      "91/91 - 2s - 24ms/step - loss: 7.0688 - mae: 1.2630 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "91/91 - 0s - 5ms/step - loss: 5.6527 - mae: 1.0144 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "91/91 - 0s - 5ms/step - loss: 5.1642 - mae: 0.9219 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.8283 - mae: 0.8627 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.6327 - mae: 0.8174 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.4260 - mae: 0.7846 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.2498 - mae: 0.7572 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.0942 - mae: 0.7149 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.9715 - mae: 0.6970 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.7908 - mae: 0.6590 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.8195 - mae: 0.6592 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.7031 - mae: 0.6405 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.5435 - mae: 0.6137 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.5554 - mae: 0.5962 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.5121 - mae: 0.5907 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.4355 - mae: 0.5671 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.3256 - mae: 0.5503 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.3065 - mae: 0.5478 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.2713 - mae: 0.5269 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1367 - mae: 0.5100 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1883 - mae: 0.5235 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1323 - mae: 0.5046 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0855 - mae: 0.4879 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0223 - mae: 0.4838 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0458 - mae: 0.4728 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9743 - mae: 0.4611 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9054 - mae: 0.4510 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9144 - mae: 0.4522 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8656 - mae: 0.4372 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8761 - mae: 0.4377 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8906 - mae: 0.4305 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7843 - mae: 0.4259 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8166 - mae: 0.4219 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7959 - mae: 0.4148 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7445 - mae: 0.4109 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7024 - mae: 0.4004 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7469 - mae: 0.4013 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6261 - mae: 0.3816 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6828 - mae: 0.3922 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6556 - mae: 0.3847 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5739 - mae: 0.3758 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5710 - mae: 0.3731 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4879 - mae: 0.3645 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5903 - mae: 0.3691 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5891 - mae: 0.3652 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5111 - mae: 0.3574 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5351 - mae: 0.3607 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5179 - mae: 0.3475 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4758 - mae: 0.3479 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4618 - mae: 0.3448 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4710 - mae: 0.3449 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4589 - mae: 0.3363 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4164 - mae: 0.3400 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4011 - mae: 0.3394 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2993 - mae: 0.3295 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4066 - mae: 0.3277 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3817 - mae: 0.3288 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3495 - mae: 0.3258 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4117 - mae: 0.3279 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3024 - mae: 0.3235 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3731 - mae: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3330 - mae: 0.3229 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2135 - mae: 0.3065 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2486 - mae: 0.3140 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2447 - mae: 0.3111 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2734 - mae: 0.3140 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2294 - mae: 0.3061 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2138 - mae: 0.3110 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1727 - mae: 0.2976 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1051 - mae: 0.2992 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1448 - mae: 0.2937 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1374 - mae: 0.3051 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1225 - mae: 0.3008 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0885 - mae: 0.2966 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0689 - mae: 0.2943 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0846 - mae: 0.2899 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0749 - mae: 0.2888 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0773 - mae: 0.2922 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0290 - mae: 0.2907 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0331 - mae: 0.2951 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0581 - mae: 0.2934 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9482 - mae: 0.2956 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9570 - mae: 0.2845 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9555 - mae: 0.2790 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9155 - mae: 0.2835 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8310 - mae: 0.2786 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8883 - mae: 0.2784 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8433 - mae: 0.2776 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8712 - mae: 0.2732 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8041 - mae: 0.2714 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8205 - mae: 0.2746 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8294 - mae: 0.2786 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7548 - mae: 0.2595 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7623 - mae: 0.2718 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7314 - mae: 0.2612 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7047 - mae: 0.2613 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7234 - mae: 0.2631 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7289 - mae: 0.2599 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7358 - mae: 0.2698 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7227 - mae: 0.2667 - learning_rate: 1.0000e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Total MAE for current configuration: 116.22\n",
      "Testing parameters: {'loss_function': <function weighted_huber_loss at 0x0000027783E5C680>, 'layer_sizes': (1024, 512, 64), 'dropout_rate': 0.3, 'batch_size': 32, 'activation_function': 'relu', 'focus_top': False}\n",
      "Epoch 1/100\n",
      "91/91 - 2s - 23ms/step - loss: 2.9805 - mae: 1.2720 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7074 - mae: 1.0099 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5785 - mae: 0.8904 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4849 - mae: 0.8064 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4364 - mae: 0.7677 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3875 - mae: 0.7286 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3446 - mae: 0.6958 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3171 - mae: 0.6788 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2661 - mae: 0.6385 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2428 - mae: 0.6261 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2038 - mae: 0.5984 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1679 - mae: 0.5740 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1360 - mae: 0.5540 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1167 - mae: 0.5469 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0726 - mae: 0.5152 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0470 - mae: 0.5024 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0123 - mae: 0.4808 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9740 - mae: 0.4561 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9509 - mae: 0.4468 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9279 - mae: 0.4381 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8923 - mae: 0.4170 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8752 - mae: 0.4148 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8338 - mae: 0.3887 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8173 - mae: 0.3879 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7794 - mae: 0.3660 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7544 - mae: 0.3576 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7407 - mae: 0.3607 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7173 - mae: 0.3545 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6904 - mae: 0.3452 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6753 - mae: 0.3481 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6381 - mae: 0.3292 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6353 - mae: 0.3451 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6089 - mae: 0.3377 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5798 - mae: 0.3279 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.5571 - mae: 0.3249 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5275 - mae: 0.3152 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5071 - mae: 0.3150 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4745 - mae: 0.3030 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4555 - mae: 0.3048 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4259 - mae: 0.2963 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4104 - mae: 0.3021 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3727 - mae: 0.2860 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3680 - mae: 0.3030 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3293 - mae: 0.2862 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3135 - mae: 0.2925 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2830 - mae: 0.2842 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2640 - mae: 0.2877 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2550 - mae: 0.3012 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2265 - mae: 0.2952 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1936 - mae: 0.2844 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1574 - mae: 0.2706 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1434 - mae: 0.2789 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1183 - mae: 0.2759 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0823 - mae: 0.2620 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0716 - mae: 0.2734 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0474 - mae: 0.2710 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0191 - mae: 0.2645 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9943 - mae: 0.2612 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9756 - mae: 0.2639 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9476 - mae: 0.2570 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9320 - mae: 0.2620 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9157 - mae: 0.2660 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8941 - mae: 0.2645 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8689 - mae: 0.2588 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8472 - mae: 0.2562 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8268 - mae: 0.2547 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8140 - mae: 0.2605 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7863 - mae: 0.2508 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7633 - mae: 0.2452 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7483 - mae: 0.2474 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7379 - mae: 0.2536 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7200 - mae: 0.2512 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6942 - mae: 0.2408 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6863 - mae: 0.2473 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6736 - mae: 0.2488 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6592 - mae: 0.2480 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6384 - mae: 0.2404 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6339 - mae: 0.2487 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6127 - mae: 0.2399 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5991 - mae: 0.2383 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5919 - mae: 0.2426 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5739 - mae: 0.2358 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5640 - mae: 0.2367 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5524 - mae: 0.2356 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5403 - mae: 0.2334 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5203 - mae: 0.2231 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5207 - mae: 0.2327 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5164 - mae: 0.2369 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5082 - mae: 0.2370 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4931 - mae: 0.2301 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4852 - mae: 0.2300 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4764 - mae: 0.2288 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4758 - mae: 0.2355 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4647 - mae: 0.2313 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4526 - mae: 0.2258 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4451 - mae: 0.2247 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4354 - mae: 0.2212 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4234 - mae: 0.2153 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4201 - mae: 0.2178 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4260 - mae: 0.2291 - learning_rate: 1.0000e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Total MAE for current configuration: 119.08\n",
      "Testing parameters: {'loss_function': <function weighted_huber_loss at 0x0000027783E5C680>, 'layer_sizes': (1024, 512, 64), 'dropout_rate': 0.3, 'batch_size': 32, 'activation_function': 'swish', 'focus_top': True}\n",
      "Epoch 1/100\n",
      "91/91 - 2s - 24ms/step - loss: 6.0874 - mae: 1.0911 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "91/91 - 1s - 6ms/step - loss: 4.9656 - mae: 0.8865 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "91/91 - 1s - 6ms/step - loss: 4.4283 - mae: 0.7840 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.9752 - mae: 0.6995 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.8478 - mae: 0.6626 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.6588 - mae: 0.6251 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.5119 - mae: 0.6039 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.5057 - mae: 0.5888 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.3254 - mae: 0.5681 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.2760 - mae: 0.5399 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1889 - mae: 0.5300 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1479 - mae: 0.5221 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0617 - mae: 0.4995 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0453 - mae: 0.4944 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9812 - mae: 0.4777 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9182 - mae: 0.4667 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9586 - mae: 0.4604 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9056 - mae: 0.4557 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8888 - mae: 0.4413 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7526 - mae: 0.4225 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7996 - mae: 0.4340 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8698 - mae: 0.4319 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7580 - mae: 0.4131 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.6717 - mae: 0.4044 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6998 - mae: 0.4016 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6744 - mae: 0.4002 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6878 - mae: 0.3973 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5916 - mae: 0.3873 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5801 - mae: 0.3871 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6128 - mae: 0.3882 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5874 - mae: 0.3804 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5044 - mae: 0.3711 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5522 - mae: 0.3659 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4602 - mae: 0.3569 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.5259 - mae: 0.3644 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4746 - mae: 0.3599 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.4208 - mae: 0.3533 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4117 - mae: 0.3538 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4585 - mae: 0.3530 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3918 - mae: 0.3403 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4473 - mae: 0.3450 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3866 - mae: 0.3333 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3439 - mae: 0.3360 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3456 - mae: 0.3454 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3226 - mae: 0.3264 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2895 - mae: 0.3283 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3178 - mae: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2617 - mae: 0.3267 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2711 - mae: 0.3182 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2188 - mae: 0.3233 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.2189 - mae: 0.3190 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "91/91 - 1s - 5ms/step - loss: 2.2455 - mae: 0.3229 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.2245 - mae: 0.3246 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.1903 - mae: 0.3166 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.1689 - mae: 0.3179 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.1415 - mae: 0.3074 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0726 - mae: 0.3011 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0884 - mae: 0.3073 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1194 - mae: 0.3069 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0596 - mae: 0.3034 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0885 - mae: 0.3042 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0554 - mae: 0.3007 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0417 - mae: 0.3009 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0078 - mae: 0.2961 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9752 - mae: 0.2950 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9711 - mae: 0.2975 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.9781 - mae: 0.2989 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.9538 - mae: 0.2929 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.9143 - mae: 0.2874 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.9133 - mae: 0.2963 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.8767 - mae: 0.2820 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8195 - mae: 0.2844 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.8443 - mae: 0.2896 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.8543 - mae: 0.2851 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.8168 - mae: 0.2794 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.7592 - mae: 0.2726 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8308 - mae: 0.2820 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "91/91 - 1s - 5ms/step - loss: 1.7538 - mae: 0.2767 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.7981 - mae: 0.2836 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.7187 - mae: 0.2794 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.7253 - mae: 0.2737 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.6860 - mae: 0.2680 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.7115 - mae: 0.2731 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6406 - mae: 0.2724 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6633 - mae: 0.2677 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.6215 - mae: 0.2704 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.6460 - mae: 0.2702 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6310 - mae: 0.2727 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5996 - mae: 0.2697 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "91/91 - 1s - 5ms/step - loss: 1.6011 - mae: 0.2663 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.5800 - mae: 0.2656 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.5659 - mae: 0.2746 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5275 - mae: 0.2637 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.4899 - mae: 0.2571 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5006 - mae: 0.2620 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "91/91 - 1s - 8ms/step - loss: 1.4890 - mae: 0.2634 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.5271 - mae: 0.2673 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.4903 - mae: 0.2645 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.4779 - mae: 0.2678 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.4709 - mae: 0.2626 - learning_rate: 1.0000e-04\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000027793256FC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Total MAE for current configuration: 118.58\n",
      "Testing parameters: {'loss_function': <function weighted_huber_loss at 0x0000027783E5C680>, 'layer_sizes': (1024, 512, 64), 'dropout_rate': 0.3, 'batch_size': 32, 'activation_function': 'swish', 'focus_top': False}\n",
      "Epoch 1/100\n",
      "91/91 - 2s - 26ms/step - loss: 2.6620 - mae: 0.9469 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4833 - mae: 0.7817 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.3998 - mae: 0.7106 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3294 - mae: 0.6529 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.2783 - mae: 0.6149 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.2318 - mae: 0.5822 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1962 - mae: 0.5607 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1449 - mae: 0.5241 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1076 - mae: 0.5021 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0700 - mae: 0.4803 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0396 - mae: 0.4663 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9942 - mae: 0.4378 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9609 - mae: 0.4220 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9237 - mae: 0.4030 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.8967 - mae: 0.3946 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8619 - mae: 0.3790 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8377 - mae: 0.3746 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.7936 - mae: 0.3509 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.7769 - mae: 0.3551 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7507 - mae: 0.3504 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7209 - mae: 0.3426 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6863 - mae: 0.3304 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.6535 - mae: 0.3205 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6314 - mae: 0.3219 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6106 - mae: 0.3248 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.5758 - mae: 0.3141 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.5522 - mae: 0.3149 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5263 - mae: 0.3138 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5034 - mae: 0.3161 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4669 - mae: 0.3050 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4453 - mae: 0.3091 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4056 - mae: 0.2953 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3774 - mae: 0.2932 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3555 - mae: 0.2976 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3259 - mae: 0.2943 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2905 - mae: 0.2851 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.2590 - mae: 0.2798 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "91/91 - 1s - 7ms/step - loss: 1.2344 - mae: 0.2815 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.2033 - mae: 0.2768 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.1848 - mae: 0.2845 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.1515 - mae: 0.2774 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.1259 - mae: 0.2777 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1028 - mae: 0.2803 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.0717 - mae: 0.2744 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.0380 - mae: 0.2657 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "91/91 - 1s - 6ms/step - loss: 1.0250 - mae: 0.2775 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.9882 - mae: 0.2648 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9705 - mae: 0.2704 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9425 - mae: 0.2653 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.9295 - mae: 0.2749 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.8968 - mae: 0.2644 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.8763 - mae: 0.2658 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8453 - mae: 0.2561 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.8333 - mae: 0.2650 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8078 - mae: 0.2598 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7906 - mae: 0.2623 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7696 - mae: 0.2602 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.7400 - mae: 0.2492 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7244 - mae: 0.2517 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.7002 - mae: 0.2449 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6850 - mae: 0.2462 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.6722 - mae: 0.2496 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6586 - mae: 0.2516 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6297 - mae: 0.2376 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.6255 - mae: 0.2478 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.6061 - mae: 0.2421 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5926 - mae: 0.2421 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.5735 - mae: 0.2360 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5661 - mae: 0.2412 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5613 - mae: 0.2483 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5448 - mae: 0.2433 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5295 - mae: 0.2390 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5173 - mae: 0.2373 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5029 - mae: 0.2330 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4978 - mae: 0.2373 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4899 - mae: 0.2385 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4735 - mae: 0.2310 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4684 - mae: 0.2344 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4578 - mae: 0.2319 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.4500 - mae: 0.2316 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4380 - mae: 0.2270 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.4378 - mae: 0.2339 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.4268 - mae: 0.2296 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.4146 - mae: 0.2239 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4079 - mae: 0.2233 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.3989 - mae: 0.2202 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3988 - mae: 0.2257 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3932 - mae: 0.2256 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3867 - mae: 0.2242 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.3660 - mae: 0.2084 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.3793 - mae: 0.2263 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3700 - mae: 0.2215 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3623 - mae: 0.2179 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3670 - mae: 0.2264 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3599 - mae: 0.2227 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3451 - mae: 0.2114 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3491 - mae: 0.2188 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3388 - mae: 0.2115 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.3480 - mae: 0.2239 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3372 - mae: 0.2161 - learning_rate: 1.0000e-04\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002779037B240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Total MAE for current configuration: 119.29\n",
      "Testing parameters: {'loss_function': <function weighted_huber_loss at 0x0000027783E5C680>, 'layer_sizes': (1024, 512, 64), 'dropout_rate': 0.3, 'batch_size': 32, 'activation_function': 'elu', 'focus_top': True}\n",
      "Epoch 1/100\n",
      "91/91 - 2s - 23ms/step - loss: 6.9040 - mae: 1.2237 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "91/91 - 0s - 5ms/step - loss: 5.5304 - mae: 0.9947 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "91/91 - 0s - 5ms/step - loss: 5.0263 - mae: 0.8924 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.6207 - mae: 0.8386 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.4625 - mae: 0.8115 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.3829 - mae: 0.7781 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.2355 - mae: 0.7582 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "91/91 - 0s - 5ms/step - loss: 4.0508 - mae: 0.7178 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.9678 - mae: 0.7090 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.9279 - mae: 0.6976 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.7843 - mae: 0.6673 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.6925 - mae: 0.6560 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.6694 - mae: 0.6462 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.5447 - mae: 0.6209 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.5598 - mae: 0.6132 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.4334 - mae: 0.5951 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.3971 - mae: 0.5908 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.3703 - mae: 0.5842 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.3243 - mae: 0.5733 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.2839 - mae: 0.5643 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.2761 - mae: 0.5543 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.2072 - mae: 0.5421 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1981 - mae: 0.5346 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1092 - mae: 0.5190 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.1113 - mae: 0.5153 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0637 - mae: 0.5085 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0174 - mae: 0.5063 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "91/91 - 0s - 5ms/step - loss: 3.0726 - mae: 0.5056 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "91/91 - 1s - 6ms/step - loss: 3.0113 - mae: 0.4950 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9082 - mae: 0.4847 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.9340 - mae: 0.4831 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8207 - mae: 0.4717 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8630 - mae: 0.4659 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8564 - mae: 0.4579 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8982 - mae: 0.4600 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8017 - mae: 0.4499 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "91/91 - 1s - 6ms/step - loss: 2.8001 - mae: 0.4509 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.8205 - mae: 0.4495 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7024 - mae: 0.4426 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7510 - mae: 0.4395 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.7121 - mae: 0.4352 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6457 - mae: 0.4272 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6626 - mae: 0.4253 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6929 - mae: 0.4256 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6015 - mae: 0.4181 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6199 - mae: 0.4166 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6145 - mae: 0.4078 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5093 - mae: 0.3960 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5500 - mae: 0.4048 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5109 - mae: 0.3988 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4711 - mae: 0.3915 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4864 - mae: 0.3983 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4687 - mae: 0.3859 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4239 - mae: 0.3884 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4287 - mae: 0.3886 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3813 - mae: 0.3839 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3813 - mae: 0.3789 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3966 - mae: 0.3744 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3071 - mae: 0.3748 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3435 - mae: 0.3750 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2979 - mae: 0.3694 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2901 - mae: 0.3681 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2965 - mae: 0.3651 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2852 - mae: 0.3568 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1770 - mae: 0.3524 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1743 - mae: 0.3587 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0948 - mae: 0.3480 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2027 - mae: 0.3517 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1513 - mae: 0.3483 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1261 - mae: 0.3497 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1341 - mae: 0.3432 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0533 - mae: 0.3459 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0718 - mae: 0.3389 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0643 - mae: 0.3378 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0568 - mae: 0.3374 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0258 - mae: 0.3342 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9608 - mae: 0.3286 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0048 - mae: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9708 - mae: 0.3340 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9659 - mae: 0.3311 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9835 - mae: 0.3328 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9334 - mae: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9117 - mae: 0.3204 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8978 - mae: 0.3244 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8835 - mae: 0.3226 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8514 - mae: 0.3226 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8910 - mae: 0.3356 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8478 - mae: 0.3170 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7981 - mae: 0.3149 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7836 - mae: 0.3130 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7494 - mae: 0.3126 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7449 - mae: 0.3133 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7454 - mae: 0.3216 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7255 - mae: 0.3107 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6986 - mae: 0.3073 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6786 - mae: 0.3082 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7122 - mae: 0.3122 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6618 - mae: 0.3073 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6367 - mae: 0.3068 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6500 - mae: 0.3087 - learning_rate: 1.0000e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Total MAE for current configuration: 118.82\n",
      "Testing parameters: {'loss_function': <function weighted_huber_loss at 0x0000027783E5C680>, 'layer_sizes': (1024, 512, 64), 'dropout_rate': 0.3, 'batch_size': 32, 'activation_function': 'elu', 'focus_top': False}\n",
      "Epoch 1/100\n",
      "91/91 - 2s - 27ms/step - loss: 2.9306 - mae: 1.2179 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.6617 - mae: 0.9598 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.5553 - mae: 0.8632 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4959 - mae: 0.8143 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.4281 - mae: 0.7575 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3841 - mae: 0.7250 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.3454 - mae: 0.6985 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2959 - mae: 0.6617 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2767 - mae: 0.6557 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2462 - mae: 0.6388 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.2007 - mae: 0.6075 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1778 - mae: 0.5993 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1354 - mae: 0.5721 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.1134 - mae: 0.5657 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0756 - mae: 0.5440 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0419 - mae: 0.5268 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "91/91 - 0s - 5ms/step - loss: 2.0218 - mae: 0.5237 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9802 - mae: 0.4996 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9566 - mae: 0.4940 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.9291 - mae: 0.4849 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8945 - mae: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8730 - mae: 0.4671 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8422 - mae: 0.4561 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.8024 - mae: 0.4365 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7825 - mae: 0.4372 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7486 - mae: 0.4244 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.7212 - mae: 0.4185 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6894 - mae: 0.4085 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6594 - mae: 0.4008 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6360 - mae: 0.4001 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.6142 - mae: 0.4013 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5732 - mae: 0.3835 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5544 - mae: 0.3884 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.5203 - mae: 0.3782 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4844 - mae: 0.3664 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4587 - mae: 0.3651 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4356 - mae: 0.3665 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.4131 - mae: 0.3686 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3712 - mae: 0.3514 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3445 - mae: 0.3495 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.3177 - mae: 0.3476 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2920 - mae: 0.3468 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2653 - mae: 0.3451 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.2361 - mae: 0.3408 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1981 - mae: 0.3276 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1651 - mae: 0.3192 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1580 - mae: 0.3367 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.1268 - mae: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0852 - mae: 0.3126 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0567 - mae: 0.3080 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0426 - mae: 0.3175 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "91/91 - 0s - 5ms/step - loss: 1.0166 - mae: 0.3146 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9979 - mae: 0.3188 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9570 - mae: 0.3005 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9368 - mae: 0.3024 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.9068 - mae: 0.2943 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8789 - mae: 0.2878 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8657 - mae: 0.2955 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.8400 - mae: 0.2904 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8126 - mae: 0.2833 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.8009 - mae: 0.2908 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7716 - mae: 0.2805 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7612 - mae: 0.2887 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7305 - mae: 0.2761 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.7265 - mae: 0.2896 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6999 - mae: 0.2798 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.6783 - mae: 0.2746 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.6576 - mae: 0.2697 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.6475 - mae: 0.2747 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.6198 - mae: 0.2618 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.6082 - mae: 0.2646 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5930 - mae: 0.2632 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5727 - mae: 0.2562 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5568 - mae: 0.2531 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5516 - mae: 0.2602 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5417 - mae: 0.2618 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5261 - mae: 0.2574 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5127 - mae: 0.2545 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.5081 - mae: 0.2601 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4930 - mae: 0.2547 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4826 - mae: 0.2537 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4703 - mae: 0.2504 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4622 - mae: 0.2510 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4574 - mae: 0.2545 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "91/91 - 1s - 6ms/step - loss: 0.4445 - mae: 0.2495 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4341 - mae: 0.2464 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4295 - mae: 0.2490 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4219 - mae: 0.2483 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.4017 - mae: 0.2348 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3970 - mae: 0.2365 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3965 - mae: 0.2420 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3818 - mae: 0.2330 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3818 - mae: 0.2381 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3777 - mae: 0.2391 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3729 - mae: 0.2392 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3629 - mae: 0.2339 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3627 - mae: 0.2378 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3529 - mae: 0.2318 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3478 - mae: 0.2309 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "91/91 - 0s - 5ms/step - loss: 0.3496 - mae: 0.2366 - learning_rate: 1.0000e-04\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Total MAE for current configuration: 119.47\n",
      "Testing parameters: {'loss_function': <function weighted_huber_loss at 0x0000027783E5C680>, 'layer_sizes': (1024, 512, 64), 'dropout_rate': 0.3, 'batch_size': 16, 'activation_function': 'relu', 'focus_top': True}\n",
      "Epoch 1/100\n",
      "181/181 - 3s - 15ms/step - loss: 6.9128 - mae: 1.1990 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "181/181 - 1s - 5ms/step - loss: 5.3706 - mae: 0.9419 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "181/181 - 1s - 5ms/step - loss: 4.8006 - mae: 0.8410 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "181/181 - 1s - 5ms/step - loss: 4.3923 - mae: 0.7688 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "181/181 - 1s - 5ms/step - loss: 4.2344 - mae: 0.7468 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "181/181 - 1s - 5ms/step - loss: 4.1580 - mae: 0.7068 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "181/181 - 1s - 5ms/step - loss: 4.0151 - mae: 0.6891 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.8027 - mae: 0.6498 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.6983 - mae: 0.6269 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.5980 - mae: 0.6145 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.5556 - mae: 0.5912 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.4651 - mae: 0.5730 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.4650 - mae: 0.5670 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.3513 - mae: 0.5490 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.2278 - mae: 0.5255 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.1945 - mae: 0.5187 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.1858 - mae: 0.5063 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.0951 - mae: 0.4945 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.0162 - mae: 0.4691 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "181/181 - 1s - 5ms/step - loss: 3.0028 - mae: 0.4699 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.9372 - mae: 0.4568 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.9096 - mae: 0.4469 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.8512 - mae: 0.4377 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.8760 - mae: 0.4335 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.8449 - mae: 0.4275 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.8372 - mae: 0.4160 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.7922 - mae: 0.4070 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.8148 - mae: 0.4106 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.6708 - mae: 0.3955 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.6832 - mae: 0.3991 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.6772 - mae: 0.3869 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.6078 - mae: 0.3835 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.5561 - mae: 0.3744 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.5766 - mae: 0.3745 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.5723 - mae: 0.3694 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.5316 - mae: 0.3638 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.5595 - mae: 0.3680 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.4783 - mae: 0.3615 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.4484 - mae: 0.3583 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.4344 - mae: 0.3594 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.4559 - mae: 0.3521 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.3784 - mae: 0.3433 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.3542 - mae: 0.3466 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.4009 - mae: 0.3401 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.3613 - mae: 0.3439 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.3179 - mae: 0.3353 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.3194 - mae: 0.3382 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.2943 - mae: 0.3377 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.2848 - mae: 0.3313 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.2354 - mae: 0.3299 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.2886 - mae: 0.3311 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.1377 - mae: 0.3214 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.1273 - mae: 0.3134 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.1825 - mae: 0.3240 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.1383 - mae: 0.3158 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.1791 - mae: 0.3211 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "181/181 - 1s - 5ms/step - loss: 2.0624 - mae: 0.3219 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.9916 - mae: 0.3035 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.0102 - mae: 0.3088 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.9797 - mae: 0.2940 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "181/181 - 1s - 4ms/step - loss: 2.0070 - mae: 0.2957 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.8894 - mae: 0.3031 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.9763 - mae: 0.3099 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.8747 - mae: 0.2963 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.8960 - mae: 0.2958 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.8919 - mae: 0.2943 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.8963 - mae: 0.2947 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.8375 - mae: 0.2991 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.7875 - mae: 0.2883 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.7821 - mae: 0.2914 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.8095 - mae: 0.2895 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.7159 - mae: 0.2807 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.7039 - mae: 0.2842 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.7493 - mae: 0.2862 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.7318 - mae: 0.2813 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.6694 - mae: 0.2886 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.6758 - mae: 0.2884 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.6151 - mae: 0.2717 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.6650 - mae: 0.2816 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.5759 - mae: 0.2752 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.5861 - mae: 0.2754 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.5904 - mae: 0.2710 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.5090 - mae: 0.2709 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.5052 - mae: 0.2674 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.4750 - mae: 0.2626 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.5353 - mae: 0.2705 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "181/181 - 1s - 4ms/step - loss: 1.4895 - mae: 0.2616 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "181/181 - 1s - 5ms/step - loss: 1.4698 - mae: 0.2638 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 152\u001b[0m\n\u001b[0;32m    149\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    150\u001b[0m lr_reduce \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_2024, y_train_2024, \n\u001b[0;32m    153\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[0;32m    154\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39mparam_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m    155\u001b[0m           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m    156\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, lr_reduce])\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Evaluate extrapolation performance on 2024 data\u001b[39;00m\n\u001b[0;32m    159\u001b[0m df_result_2024_extrap \u001b[38;5;241m=\u001b[39m build_baseline_and_predict_2024(\n\u001b[0;32m    160\u001b[0m     model, df_unscaled, feature_cols, target_cols, scaler_features, scaler_targets\n\u001b[0;32m    161\u001b[0m )\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mE:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout, Input, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Define custom weighted loss function for top countries\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def weighted_huber_loss(delta=1.0):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        weights_batch = tf.cast(tf.gather(sample_weights, tf.range(tf.shape(y_true)[0])), tf.float32)\n",
    "\n",
    "        error = y_true - y_pred\n",
    "        condition = tf.abs(error) < delta\n",
    "        huber_loss = tf.where(condition, 0.5 * tf.square(error), delta * (tf.abs(error) - 0.5 * delta))\n",
    "\n",
    "        weighted_loss = tf.reduce_mean(weights_batch * huber_loss)\n",
    "        return weighted_loss\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "# Function to create custom weights based on medal counts\n",
    "def generate_sample_weights(y_true, df, top_countries):\n",
    "    weights = np.ones((len(y_true), y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        if df.iloc[i][\"NOC\"] in top_countries:\n",
    "            weights[i] *= 10.0  # Increase weight for top countries\n",
    "    return weights\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Select top 20 medal-winning countries\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Ensure NOC column exists by reconstructing from one-hot encoding\n",
    "if 'NOC' not in df_unscaled.columns:\n",
    "    noc_columns = [col for col in df_unscaled.columns if col.startswith(\"NOC_\")]\n",
    "    df_unscaled[\"NOC\"] = df_unscaled[noc_columns].idxmax(axis=1).str.replace(\"NOC_\", \"\")\n",
    "\n",
    "# Calculate total medals for each NOC\n",
    "df_unscaled['Total_Medals'] = df_unscaled[\"Medal_Gold\"] + df_unscaled[\"Medal_Silver\"] + df_unscaled[\"Medal_Bronze\"]\n",
    "\n",
    "# Identify the top 20 medal-winning countries\n",
    "top_20_countries = df_unscaled.groupby(\"NOC\")[\"Total_Medals\"].sum().nlargest(20).index.tolist()\n",
    "\n",
    "# Generate the sample weights\n",
    "sample_weights = generate_sample_weights(y_all, df_unscaled, top_20_countries)\n",
    "sample_weights = np.array(sample_weights, dtype=np.float32)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Model creation function with flexible architecture\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def build_medal_prediction_model(input_dim, loss_function, layer_sizes, \n",
    "                                 dropout_rate, activation_function, focus_top):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "\n",
    "    x = Dense(layer_sizes[0], activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation_function)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    for size in layer_sizes[1:]:\n",
    "        x1 = Dense(size, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = Activation(activation_function)(x1)\n",
    "\n",
    "        x2 = Dense(size, activation=None, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x1)\n",
    "        x2 = BatchNormalization()(x2)\n",
    "        x2 = Activation(activation_function)(x2)\n",
    "        x2 = Dropout(dropout_rate)(x2)\n",
    "\n",
    "        x = Add()([x1, x2])\n",
    "\n",
    "    outputs = Dense(3, activation=\"linear\")(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    if focus_top:\n",
    "        model.compile(optimizer=AdamW(learning_rate=0.0001, weight_decay=1e-5),\n",
    "                      loss=loss_function(),\n",
    "                      metrics=[\"mae\"])\n",
    "    else:\n",
    "        model.compile(optimizer=AdamW(learning_rate=0.0001, weight_decay=1e-5), \n",
    "                      loss=tf.keras.losses.MeanAbsoluteError(), \n",
    "                      metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Grid Search for Hyperparameter Optimization\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "param_grid = {\n",
    "    \"loss_function\": [weighted_huber_loss],\n",
    "    \"layer_sizes\": [(1024, 512, 64), (2048, 1024, 512, 64), (1024, 512, 64, 64)],\n",
    "    \"dropout_rate\": [0.3, 0.2],\n",
    "    \"batch_size\": [32, 16],\n",
    "    \"activation_function\": [\"relu\", \"swish\", \"elu\"],\n",
    "    \"focus_top\": [True, False]  # Whether to focus on top 20 countries\n",
    "}\n",
    "\n",
    "# Extract training data before 2024\n",
    "mask_train_2024 = (df_unscaled[\"Year\"] < 2024)\n",
    "X_train_2024 = X_all[mask_train_2024]\n",
    "y_train_2024 = y_all[mask_train_2024]\n",
    "\n",
    "X_train_2024 = X_train_2024.astype(np.float32)\n",
    "y_train_2024 = y_train_2024.astype(np.float32)\n",
    "\n",
    "\n",
    "# Store the best model and results\n",
    "best_model = None\n",
    "best_mae = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "for params in product(*param_grid.values()):\n",
    "    param_dict = dict(zip(param_grid.keys(), params))\n",
    "    print(f\"Testing parameters: {param_dict}\")\n",
    "\n",
    "    try:\n",
    "        model = build_medal_prediction_model(\n",
    "            input_dim=X_all.shape[1],\n",
    "            loss_function=param_dict[\"loss_function\"],\n",
    "            layer_sizes=param_dict[\"layer_sizes\"],\n",
    "            dropout_rate=param_dict[\"dropout_rate\"],\n",
    "            activation_function=param_dict[\"activation_function\"],\n",
    "            focus_top=param_dict[\"focus_top\"]\n",
    "        )\n",
    "\n",
    "        # Train on data before 2024\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "        lr_reduce = ReduceLROnPlateau(monitor='val_loss', patience=15, factor=0.5, verbose=1)\n",
    "\n",
    "        model.fit(X_train_2024, y_train_2024, \n",
    "                  epochs=100, \n",
    "                  batch_size=param_dict[\"batch_size\"], \n",
    "                  verbose=2, \n",
    "                  callbacks=[early_stopping, lr_reduce])\n",
    "\n",
    "        # Evaluate extrapolation performance on 2024 data\n",
    "        df_result_2024_extrap = build_baseline_and_predict_2024(\n",
    "            model, df_unscaled, feature_cols, target_cols, scaler_features, scaler_targets\n",
    "        )\n",
    "\n",
    "        # Calculate MAE for each medal type\n",
    "        gold_mae = mean_absolute_error(df_result_2024_extrap[\"Gold_Actual\"], df_result_2024_extrap[\"Gold_Predicted\"])\n",
    "        silver_mae = mean_absolute_error(df_result_2024_extrap[\"Silver_Actual\"], df_result_2024_extrap[\"Silver_Predicted\"])\n",
    "        bronze_mae = mean_absolute_error(df_result_2024_extrap[\"Bronze_Actual\"], df_result_2024_extrap[\"Bronze_Predicted\"])\n",
    "\n",
    "        total_mae = gold_mae + silver_mae + bronze_mae\n",
    "        print(f\"Total MAE for current configuration: {total_mae:.2f}\")\n",
    "\n",
    "        # Track the best model\n",
    "        if total_mae < best_mae:\n",
    "            best_mae = total_mae\n",
    "            best_model = model\n",
    "            best_params = param_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping configuration due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "print(\"\\nBest Model Configuration:\")\n",
    "print(best_params)\n",
    "print(f\"Best Total MAE: {best_mae:.2f}\")\n",
    "\n",
    "# Save the best model\n",
    "if best_model:\n",
    "    best_model.save(\"best_medal_prediction_model_top20.h5\")\n",
    "\n",
    "    # Visualize the best model results\n",
    "    df_result_2024_extrap = build_baseline_and_predict_2024(\n",
    "        best_model, df_unscaled, feature_cols, target_cols, scaler_features, scaler_targets\n",
    "    )\n",
    "    visualize_2024_results(df_result_2024_extrap)\n",
    "else:\n",
    "    print(\"No valid model configurations found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc5a45-8808-4feb-bbba-1166ccb808f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
