{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd832acc-40c2-4111-8a3e-b68630446f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import xgboost as xgb  # for final medal predictions\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "import scipy\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Enable multi-threading in TensorFlow\n",
    "# ------------------------------------------------------------------\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(cpu_count())\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(cpu_count())\n",
    "tf.config.threading.set_intra_op_parallelism_threads(cpu_count())\n",
    "tf.config.threading.set_inter_op_parallelism_threads(cpu_count())\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Load & Basic Cleaning\n",
    "# ------------------------------------------------------------------\n",
    "data_path = \"MCM_2025\\\\2025_MCM-ICM_Problems\\\\2025_Problem_C_Data\\\\summerOly_athletes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Remove invalid Team entries\n",
    "df = df[~df['Team'].str.contains(r'\\d|\\.', na=False)]\n",
    "roman_pattern = r'(?:\\s|-)M{0,4}(?:CM|CD|D?C{0,3})(?:XC|XL|L?X{0,3})(?:IX|IV|V?I{0,3})(?:\\s|$)'\n",
    "df = df[~df['Team'].str.contains(roman_pattern, na=False, flags=re.IGNORECASE)]\n",
    "\n",
    "# Remove countries not present in last two Olympics (2020 & 2024)\n",
    "recent_years = [2020, 2024]\n",
    "df = df[df[\"NOC\"].isin(df[df[\"Year\"].isin(recent_years)][\"NOC\"].unique())]\n",
    "\n",
    "# Remove events that appeared <4 times\n",
    "event_counts = df[\"Event\"].value_counts()\n",
    "df = df[df[\"Event\"].isin(event_counts[event_counts >= 4].index)]\n",
    "\n",
    "# Convert Year to int\n",
    "df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(int)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Feature Engineering (Row-level)\n",
    "# ------------------------------------------------------------------\n",
    "df[\"Seed_Player\"] = df.groupby(\"Name\")[\"Medal\"].transform(\n",
    "    lambda x: (x == \"Gold\").cumsum() + (x == \"Silver\").cumsum()\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "df[\"Elite_Player\"] = df.groupby(\"Name\")[\"Medal\"].transform(\n",
    "    lambda x: max(0, (x != \"No medal\").sum() - 1 + (x == \"Gold\").sum())\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "df[\"Participation_Count\"] = df.groupby(\"Name\")[\"Year\"].transform(\"count\")\n",
    "df[\"Retiring_Player\"] = df[\"Participation_Count\"].apply(lambda x: max(0, x - 3)).astype(int)\n",
    "\n",
    "host_cities = {2008: \"CHN\", 2012: \"GBR\", 2016: \"BRA\", 2020: \"JPN\", 2024: \"FRA\"}\n",
    "df[\"Is_Host\"] = df.apply(lambda row: 1 if host_cities.get(row['Year'], None) == row['NOC'] else 0, axis=1)\n",
    "\n",
    "df[\"NOC\"] = df[\"NOC\"].astype(str)\n",
    "df[\"Name\"] = df[\"Name\"].astype(str)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Aggregation by (Year, NOC)\n",
    "# ------------------------------------------------------------------\n",
    "df_num_cols = df.select_dtypes(include=[\"number\"]).columns.difference([\"Year\"]).tolist()\n",
    "\n",
    "df_noc_year = (\n",
    "    df.groupby([\"Year\", \"NOC\"], as_index=False)\n",
    "      .apply(lambda subdf: subdf[df_num_cols].sum(numeric_only=True))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Summation of medal counts\n",
    "medal_counts = df.groupby([\"Year\", \"NOC\"])[\"Medal\"].value_counts().unstack(fill_value=0).reset_index()\n",
    "medal_counts.rename(columns={\"Gold\":\"Medal_Gold\",\"Silver\":\"Medal_Silver\",\"Bronze\":\"Medal_Bronze\"}, inplace=True)\n",
    "\n",
    "df_noc_year = df_noc_year.merge(medal_counts, on=[\"Year\",\"NOC\"], how=\"left\").fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) One-Hot Encoding NOC\n",
    "# ------------------------------------------------------------------\n",
    "ohe_noc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "encoded_noc = ohe_noc.fit_transform(df_noc_year[[\"NOC\"]])\n",
    "noc_ohe_cols = [f\"NOC_{cat}\" for cat in ohe_noc.categories_[0]]\n",
    "\n",
    "df_noc_year.reset_index(drop=True,inplace=True)\n",
    "noc_ohe_df = pd.DataFrame(encoded_noc, columns=noc_ohe_cols)\n",
    "df_noc_year_ohe = pd.concat([df_noc_year.drop(columns=[\"NOC\"]), noc_ohe_df], axis=1)\n",
    "\n",
    "with open(\"noc_encoder.pkl\",\"wb\") as f:\n",
    "    pickle.dump(ohe_noc,f)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) Scaling\n",
    "# ------------------------------------------------------------------\n",
    "target_cols = [\"Medal_Gold\",\"Medal_Silver\",\"Medal_Bronze\"]\n",
    "feature_cols = df_noc_year_ohe.columns.difference([\"Year\"]+target_cols)\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "scaler_targets = StandardScaler()\n",
    "\n",
    "X_all = scaler_features.fit_transform(df_noc_year_ohe[feature_cols])\n",
    "y_all = scaler_targets.fit_transform(df_noc_year_ohe[target_cols])\n",
    "\n",
    "with open(\"scaler_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(scaler_features,f)\n",
    "with open(\"scaler_targets.pkl\",\"wb\") as f:\n",
    "    pickle.dump(scaler_targets,f)\n",
    "\n",
    "df_unscaled = df_noc_year_ohe.copy()  # aggregator unscaled\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Weighted sample approach\n",
    "# We'll define sample_weight = 1 + (Gold + Silver + Bronze) from the unscaled aggregator\n",
    "# so countries with bigger total medals get more weight\n",
    "# We'll store them in an array that lines up with each row of X_all\n",
    "# ------------------------------------------------------------------\n",
    "row_sums = (df_noc_year[\"Medal_Gold\"] + df_noc_year[\"Medal_Silver\"] + df_noc_year[\"Medal_Bronze\"]).values\n",
    "sample_weight_all = 1.0 + row_sums  # shape = (num_rows,)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# LSTM Baseline code (unchanged, but with debug prints)\n",
    "# ------------------------------------------------------------------\n",
    "def build_lstm_baseline_model(x_vals, y_vals, n_lag=3):\n",
    "    print(f\"[LSTM Debug] Building baseline with LSTM: x_vals length={len(x_vals)}, n_lag={n_lag}\")\n",
    "    if len(x_vals) < n_lag+1:\n",
    "        print(\"[LSTM Debug] Not enough data points for LSTM baseline.\")\n",
    "        return None\n",
    "\n",
    "    x_vals = np.array(x_vals).reshape(-1,1)\n",
    "    y_vals = np.array(y_vals).reshape(-1,1)\n",
    "\n",
    "    print(f\"[LSTM Debug] LSTM training data shape: x_vals={x_vals.shape}, y_vals={y_vals.shape}\")\n",
    "\n",
    "    # Scale y\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    y_scaled = scaler.fit_transform(y_vals)\n",
    "\n",
    "    X, Y = [], []\n",
    "    for i in range(n_lag, len(y_scaled)):\n",
    "        X.append(y_scaled[i-n_lag:i])\n",
    "        Y.append(y_scaled[i])\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    print(f\"[LSTM Debug] LSTM training sequences shape: X={X.shape}, Y={Y.shape}\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_lag,1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    print(f\"[LSTM Debug] Starting LSTM fit for {len(X)} samples.\")\n",
    "    model.fit(X, Y, epochs=50, batch_size=8, verbose=1)\n",
    "    print(\"[LSTM Debug] LSTM training done.\")\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "def predict_lstm_value(model, scaler, x_vals, y_vals, x_target, n_lag=3):\n",
    "    y_vals = np.array(y_vals).reshape(-1,1)\n",
    "    y_scaled = scaler.transform(y_vals)\n",
    "\n",
    "    if len(y_scaled)<n_lag:\n",
    "        return 0.0\n",
    "    X_input = y_scaled[-n_lag:].reshape(1,n_lag,1)\n",
    "    y_pred_sc= model.predict(X_input, verbose=0)\n",
    "    y_pred= scaler.inverse_transform(y_pred_sc)\n",
    "    return y_pred[0][0]\n",
    "\n",
    "def build_baseline_model(x_vals, y_vals):\n",
    "    \"\"\"\n",
    "    We'll only do LSTM baseline now, ignoring other approaches.\n",
    "    \"\"\"\n",
    "    ret = build_lstm_baseline_model(x_vals, y_vals)\n",
    "    if ret is None:\n",
    "        return None\n",
    "    return (\"LSTM\", ret)\n",
    "\n",
    "def predict_baseline_value(model_info, x_target, x_vals, y_vals):\n",
    "    if model_info is None:\n",
    "        return 0.0\n",
    "    name, (model, scaler) = model_info\n",
    "    if name==\"LSTM\":\n",
    "        return predict_lstm_value(model, scaler, x_vals, y_vals, x_target)\n",
    "    return 0.0\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# XGBoost Regressor for final predictions\n",
    "# We'll train 3 separate models: one for Gold, Silver, Bronze\n",
    "# ------------------------------------------------------------------\n",
    "def train_xgboost_timeseries(X_train, Y_train, sample_weight, n_splits=3):\n",
    "    \"\"\"\n",
    "    X_train: shape (N, num_features)\n",
    "    Y_train: shape (N,) for one target\n",
    "    sample_weight: array of shape (N,) with the weighting\n",
    "    We'll do a param grid search on e.g. max_depth, learning_rate, etc.\n",
    "    We'll do TSCV to pick best hyperparams.\n",
    "    \"\"\"\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1, verbosity=0)\n",
    "    param_grid = {\n",
    "        'max_depth':[3,5],\n",
    "        'learning_rate':[0.01,0.05,0.1],\n",
    "        'n_estimators':[100,200],\n",
    "        # add more if you want\n",
    "    }\n",
    "\n",
    "    # We'll do a manual approach to TSCV grid search since sklearn's GridSearchCV doesn't do advanced TS splits by default\n",
    "    best_params= None\n",
    "    best_score= float(\"inf\")\n",
    "\n",
    "    for md in param_grid['max_depth']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for nest in param_grid['n_estimators']:\n",
    "                fold_mae=[]\n",
    "                for fold,(tr_idx,val_idx) in enumerate(tscv.split(X_train)):\n",
    "                    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "                    y_tr, y_val = Y_train[tr_idx], Y_train[val_idx]\n",
    "                    w_tr, w_val = sample_weight[tr_idx], sample_weight[val_idx]\n",
    "\n",
    "                    model_ = xgb.XGBRegressor(objective='reg:squarederror',n_jobs=-1,\n",
    "                                              max_depth=md, learning_rate=lr, n_estimators=nest)\n",
    "                    model_.fit(X_tr, y_tr, sample_weight=w_tr, eval_set=[(X_val, y_val)],\n",
    "                               eval_metric='mae', verbose=False)\n",
    "\n",
    "                    val_preds= model_.predict(X_val)\n",
    "                    mae_ = mean_absolute_error(y_val, val_preds, sample_weight=w_val)\n",
    "                    fold_mae.append(mae_)\n",
    "                mean_mae= np.mean(fold_mae)\n",
    "                if mean_mae < best_score:\n",
    "                    best_score= mean_mae\n",
    "                    best_params= (md, lr, nest)\n",
    "    print(f\"[XGBoost TSCV] Best params => max_depth={best_params[0]}, lr={best_params[1]}, n_estimators={best_params[2]}, score={best_score:.3f}\")\n",
    "\n",
    "    # retrain final\n",
    "    final_xgb= xgb.XGBRegressor(objective='reg:squarederror', n_jobs=-1,\n",
    "                                max_depth=best_params[0], learning_rate=best_params[1], n_estimators=best_params[2])\n",
    "    final_xgb.fit(X_train, Y_train, sample_weight=sample_weight, verbose=False)\n",
    "    return final_xgb\n",
    "\n",
    "def train_xgboost_up_to_year(X_all, y_all, df_agg, sample_weight_all, end_year=2024):\n",
    "    \"\"\"\n",
    "    We'll train 3 separate models for each target: gold,silver,bronze.\n",
    "    We'll do TSCV with the portion of data where Year<end_year\n",
    "    \"\"\"\n",
    "    mask= (df_agg[\"Year\"]<end_year)\n",
    "    X_sub= X_all[mask]\n",
    "    sw_sub= sample_weight_all[mask]\n",
    "\n",
    "    # We'll separate y_all columns => 3 separate arrays\n",
    "    y_gold= y_all[mask,0]\n",
    "    y_silver= y_all[mask,1]\n",
    "    y_bronze= y_all[mask,2]\n",
    "\n",
    "    # Train each with TSCV\n",
    "    gold_model= train_xgboost_timeseries(X_sub, y_gold, sw_sub, n_splits=3)\n",
    "    silver_model= train_xgboost_timeseries(X_sub, y_silver, sw_sub, n_splits=3)\n",
    "    bronze_model= train_xgboost_timeseries(X_sub, y_bronze, sw_sub, n_splits=3)\n",
    "\n",
    "    return gold_model, silver_model, bronze_model\n",
    "\n",
    "def xgboost_predict_3models(models, X_input):\n",
    "    \"\"\"\n",
    "    models = (gold_model, silver_model, bronze_model)\n",
    "    \"\"\"\n",
    "    gold_model, silver_model, bronze_model = models\n",
    "    pred_gold= gold_model.predict(X_input)\n",
    "    pred_silver= silver_model.predict(X_input)\n",
    "    pred_bronze= bronze_model.predict(X_input)\n",
    "    preds= np.vstack([pred_gold,pred_silver,pred_bronze]).T\n",
    "    return preds\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Workflow A for 2024\n",
    "# ------------------------------------------------------------------\n",
    "def build_baseline_and_predict_2024_xgboost(models_3, df_agg, feature_cols, target_cols, scaler_features, scaler_targets):\n",
    "    \"\"\"\n",
    "    We build LSTM baseline for 2024, fill aggregator, then do xgboost_predict_3models.\n",
    "    \"\"\"\n",
    "    df_2024= df_agg[df_agg[\"Year\"]==2020].copy()\n",
    "    df_2024[\"Year\"]=2024\n",
    "\n",
    "    noc_cols= [c for c in df_agg.columns if c.startswith(\"NOC_\")]\n",
    "    df_agg[\"NOC\"] = df_agg[noc_cols].idxmax(axis=1).str.replace(\"NOC_\",\"\")\n",
    "    df_2024[\"NOC\"]= df_2024[noc_cols].idxmax(axis=1).str.replace(\"NOC_\",\"\")\n",
    "\n",
    "    if \"NOC_FRA\" in df_2024.columns:\n",
    "        df_2024[\"Is_Host\"]= df_2024[\"NOC_FRA\"]\n",
    "\n",
    "    numeric_feats= [c for c in feature_cols if c not in noc_cols and c!=\"Year\" and c!=\"Is_Host\"]\n",
    "    hist_data= df_agg[df_agg[\"Year\"]<2024].copy()\n",
    "\n",
    "    hist_data[\"NOC\"]= hist_data[noc_cols].idxmax(axis=1).str.replace(\"NOC_\",\"\")\n",
    "\n",
    "    for feat in numeric_feats:\n",
    "        pivot_df= hist_data.pivot(index=\"NOC\", columns=\"Year\", values=feat).fillna(0)\n",
    "        for idx_noc in pivot_df.index:\n",
    "            xvals= np.array(pivot_df.columns, dtype=float)\n",
    "            yvals= pivot_df.loc[idx_noc].values\n",
    "            if len(xvals)<2:\n",
    "                continue\n",
    "            baseline_info= build_baseline_model(xvals,yvals)\n",
    "            val_2024= predict_baseline_value(baseline_info,2024, xvals,yvals)\n",
    "            noc_col= f\"NOC_{idx_noc}\"\n",
    "            if noc_col in df_2024.columns:\n",
    "                df_2024.loc[df_2024[noc_col]==1, feat] = val_2024\n",
    "\n",
    "    # ensure columns\n",
    "    for col in df_agg.columns:\n",
    "        if col not in df_2024.columns:\n",
    "            df_2024[col]=0\n",
    "\n",
    "    X_2024_unsc= df_2024[feature_cols].values\n",
    "    X_2024_sc= scaler_features.transform(X_2024_unsc)\n",
    "    preds_2024_sc= xgboost_predict_3models(models_3, X_2024_sc)\n",
    "    preds_2024= scaler_targets.inverse_transform(preds_2024_sc)\n",
    "\n",
    "    # get real aggregator for 2024\n",
    "    df_real_2024= df_agg[df_agg[\"Year\"]==2024].reset_index(drop=True)\n",
    "    df_real_2024[\"NOC\"]= df_real_2024[noc_cols].idxmax(axis=1).str.replace(\"NOC_\",\"\")\n",
    "\n",
    "    actual_2024= scaler_targets.inverse_transform(df_real_2024[target_cols].values)\n",
    "\n",
    "    df_res_2024= pd.DataFrame({\n",
    "        \"NOC\": df_real_2024[\"NOC\"].astype(str),\n",
    "        \"Gold_Actual\": actual_2024[:,0],\n",
    "        \"Silver_Actual\": actual_2024[:,1],\n",
    "        \"Bronze_Actual\": actual_2024[:,2],\n",
    "        \"Gold_Predicted\": preds_2024[:,0],\n",
    "        \"Silver_Predicted\": preds_2024[:,1],\n",
    "        \"Bronze_Predicted\": preds_2024[:,2],\n",
    "    })\n",
    "    return df_res_2024\n",
    "\n",
    "def visualize_2024_results(df_res_2024):\n",
    "    df= df_res_2024.copy()\n",
    "    df[\"Gold_Error_%\"] = np.where(df[\"Gold_Actual\"]>0, abs(df[\"Gold_Predicted\"]-df[\"Gold_Actual\"])/df[\"Gold_Actual\"]*100,0)\n",
    "    df[\"Silver_Error_%\"] = np.where(df[\"Silver_Actual\"]>0, abs(df[\"Silver_Predicted\"]-df[\"Silver_Actual\"])/df[\"Silver_Actual\"]*100,0)\n",
    "    df[\"Bronze_Error_%\"] = np.where(df[\"Bronze_Actual\"]>0, abs(df[\"Bronze_Predicted\"]-df[\"Bronze_Actual\"])/df[\"Bronze_Actual\"]*100,0)\n",
    "\n",
    "    error_cols= [\"Gold_Error_%\",\"Silver_Error_%\",\"Bronze_Error_%\"]\n",
    "    filtered_df= df[(df[\"Gold_Actual\"]>0)|(df[\"Silver_Actual\"]>0)|(df[\"Bronze_Actual\"]>0)]\n",
    "\n",
    "    plt.figure(figsize=(18,6))\n",
    "    for i,col in enumerate(error_cols):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        plt.hist(filtered_df[col], bins=50, range=(0,100), edgecolor='black', alpha=0.7)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(\"Error %\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    exceed_100= (filtered_df[error_cols]>100).sum()/len(filtered_df)*100\n",
    "    print(\"Pct of data >100% error:\", exceed_100)\n",
    "\n",
    "    filtered_df[\"Total_Actual\"]=filtered_df[\"Gold_Actual\"]+filtered_df[\"Silver_Actual\"]+filtered_df[\"Bronze_Actual\"]\n",
    "    filtered_df.sort_values(by=\"Total_Actual\", ascending=False, inplace=True)\n",
    "\n",
    "    nocs=filtered_df[\"NOC\"].values\n",
    "    gold_act= filtered_df[\"Gold_Actual\"].values\n",
    "    gold_pred= filtered_df[\"Gold_Predicted\"].values\n",
    "    silver_act= filtered_df[\"Silver_Actual\"].values\n",
    "    silver_pred= filtered_df[\"Silver_Predicted\"].values\n",
    "    bronze_act= filtered_df[\"Bronze_Actual\"].values\n",
    "    bronze_pred= filtered_df[\"Bronze_Predicted\"].values\n",
    "\n",
    "    # gold line\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(nocs,gold_act,label=\"ActualGold\",marker='o')\n",
    "    plt.plot(nocs,gold_pred,label=\"PredGold\",marker='x')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"NOC\")\n",
    "    plt.ylabel(\"Gold Medals\")\n",
    "    plt.title(\"Gold: Actual vs Pred (2024, LSTM baseline + XGBoost)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # silver line\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(nocs,silver_act,label=\"ActualSilver\",marker='o')\n",
    "    plt.plot(nocs,silver_pred,label=\"PredSilver\",marker='x')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"NOC\")\n",
    "    plt.ylabel(\"Silver Medals\")\n",
    "    plt.title(\"Silver: Actual vs Pred (2024, LSTM baseline + XGBoost)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # bronze line\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(nocs,bronze_act,label=\"ActualBronze\",marker='o')\n",
    "    plt.plot(nocs,bronze_pred,label=\"PredBronze\",marker='x')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"NOC\")\n",
    "    plt.ylabel(\"Bronze Medals\")\n",
    "    plt.title(\"Bronze: Actual vs Pred (2024, LSTM baseline + XGBoost)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # final MAE\n",
    "    gold_mae= mean_absolute_error(gold_act,gold_pred)\n",
    "    silver_mae= mean_absolute_error(silver_act,silver_pred)\n",
    "    bronze_mae= mean_absolute_error(bronze_act,bronze_pred)\n",
    "    print(f\"Gold MAE: {gold_mae:.2f}\")\n",
    "    print(f\"Silver MAE: {silver_mae:.2f}\")\n",
    "    print(f\"Bronze MAE: {bronze_mae:.2f}\")\n",
    "\n",
    "# Workflow B for 2028\n",
    "def build_2028_baseline_and_predict_xgboost(models_3, df_agg, feature_cols, target_cols, scaler_features, scaler_targets):\n",
    "    df_2028= df_agg[df_agg[\"Year\"]==2024].copy()\n",
    "    df_2028[\"Year\"]=2028\n",
    "\n",
    "    if \"NOC_USA\" in df_2028.columns:\n",
    "        df_2028[\"Is_Host\"]= df_2028[\"NOC_USA\"]\n",
    "\n",
    "    noc_cols= [c for c in df_agg.columns if c.startswith(\"NOC_\")]\n",
    "    numeric_feats= [c for c in feature_cols if c not in noc_cols and c!=\"Year\" and c!=\"Is_Host\"]\n",
    "\n",
    "    df_agg[\"NOC\"]= df_agg[noc_cols].idxmax(axis=1).str.replace(\"NOC_\",\"\")\n",
    "\n",
    "    for feat in numeric_feats:\n",
    "        pivot_df= df_agg.pivot(index=\"NOC\", columns=\"Year\", values=feat).fillna(0)\n",
    "        for idx_noc in pivot_df.index:\n",
    "            xvals= np.array(pivot_df.columns,dtype=float)\n",
    "            yvals= pivot_df.loc[idx_noc].values\n",
    "            if len(xvals)<2:\n",
    "                continue\n",
    "            baseline_info= build_baseline_model(xvals,yvals)\n",
    "            val_2028= predict_baseline_value(baseline_info,2028,xvals,yvals)\n",
    "            noc_col= f\"NOC_{idx_noc}\"\n",
    "            if noc_col in df_2028.columns:\n",
    "                df_2028.loc[df_2028[noc_col]==1, feat]= val_2028\n",
    "\n",
    "    for col in df_agg.columns:\n",
    "        if col not in df_2028.columns:\n",
    "            df_2028[col]=0\n",
    "\n",
    "    X_2028_unsc= df_2028[feature_cols].values\n",
    "    X_2028_sc= scaler_features.transform(X_2028_unsc)\n",
    "    preds_2028_sc= xgboost_predict_3models(models_3, X_2028_sc)\n",
    "    preds_2028= scaler_targets.inverse_transform(preds_2028_sc)\n",
    "\n",
    "    df_2024_rows= df_noc_year[df_noc_year[\"Year\"]==2024].reset_index(drop=True)\n",
    "\n",
    "    df_res_2028= pd.DataFrame({\n",
    "        \"NOC\": df_2024_rows[\"NOC\"].values,\n",
    "        \"Gold_Predicted\": preds_2028[:,0],\n",
    "        \"Silver_Predicted\": preds_2028[:,1],\n",
    "        \"Bronze_Predicted\": preds_2028[:,2],\n",
    "    })\n",
    "    print(\"\\nPredicted medal counts for 2028 (XGBoost + LSTM baseline):\")\n",
    "    print(df_res_2028.head(20))\n",
    "    return df_res_2028\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Final Execution\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# 1) Train XGBoost up to <2024, do LSTM baseline for 2024\n",
    "gold_2024, silver_2024, bronze_2024 = train_xgboost_up_to_year(X_all, y_all, df_unscaled, sample_weight_all, end_year=2024)\n",
    "models_2024 = (gold_2024, silver_2024, bronze_2024)\n",
    "\n",
    "df_2024_res= build_baseline_and_predict_2024_xgboost(models_2024, df_unscaled, feature_cols, target_cols,\n",
    "                                                     scaler_features, scaler_targets)\n",
    "print(\"\\n**2024 Extrapolated Results (LSTM Baseline + XGBoost)**\")\n",
    "print(df_2024_res.head(30))\n",
    "visualize_2024_results(df_2024_res)\n",
    "\n",
    "# 2) Train XGBoost up to <2028, do LSTM baseline for 2028\n",
    "gold_2028, silver_2028, bronze_2028 = train_xgboost_up_to_year(X_all, y_all, df_unscaled, sample_weight_all, end_year=2028)\n",
    "models_2028= (gold_2028, silver_2028, bronze_2028)\n",
    "\n",
    "df_2028_res= build_2028_baseline_and_predict_xgboost(models_2028, df_unscaled, feature_cols, target_cols,\n",
    "                                                     scaler_features, scaler_targets)\n",
    "print(\"\\n--- Done with 2024 & 2028 XGBoost workflows. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
