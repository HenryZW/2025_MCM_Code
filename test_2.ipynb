{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3aff8be-ba05-4963-a6e4-ba56e68ea3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, AdamW, RMSprop\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e0d79c1-cbdb-44e5-9275-115683a9cd30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial df_raw shape: (249329, 9)\n",
      "After row-level transforms, df_filtered shape: (249329, 6332)\n",
      "Grouped by (Year, NOC) shape: (3222, 6332)\n",
      "After NOC encoding, df_agg shape: (3222, 6565)\n",
      "df_agg_scaled shape: (3222, 6565)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0) Load & Basic Cleaning\n",
    "# ------------------------------------------------------------------\n",
    "data_path = \"C:\\\\Users\\\\Henry Zhu\\\\Desktop\\\\2025_MCM-ICM_Problems\\\\2025_Problem_C_Data\\\\summerOly_athletes.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Remove rows with digits or '.' in 'Team'\n",
    "df_no_digits = data[~data['Team'].str.contains(r'\\d|\\.', na=False)]\n",
    "\n",
    "# Remove rows with Roman numerals in 'Team'\n",
    "roman_pattern = r'(?:\\s|-)M{0,4}(?:CM|CD|D?C{0,3})(?:XC|XL|L?X{0,3})(?:IX|IV|V?I{0,3})(?:\\s|$)'\n",
    "df_no_roman = df_no_digits[~df_no_digits['Team'].str.contains(roman_pattern, na=False, flags=re.IGNORECASE)]\n",
    "\n",
    "df_raw = df_no_roman.copy()\n",
    "\n",
    "for col in [\"Name\",\"Sex\",\"Team\",\"NOC\",\"City\",\"Sport\",\"Event\",\"Medal\"]:\n",
    "    df_raw[col] = df_raw[col].astype(str)\n",
    "\n",
    "# Ensure 'Year' is numeric\n",
    "df_raw[\"Year\"] = pd.to_numeric(df_raw[\"Year\"], errors=\"coerce\")\n",
    "df_raw.dropna(subset=[\"Year\"], inplace=True)\n",
    "df_raw[\"Year\"] = df_raw[\"Year\"].astype(int)\n",
    "\n",
    "print(\"Initial df_raw shape:\", df_raw.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Row-level Transforms (except for NOC)\n",
    "# ------------------------------------------------------------------\n",
    "df_filtered = df_raw.copy()\n",
    "\n",
    "# (1a) Label-encode & scale \"Name\"\n",
    "label_encoder_name = LabelEncoder()\n",
    "df_filtered[\"Name_Label\"] = label_encoder_name.fit_transform(df_filtered[\"Name\"])\n",
    "\n",
    "scaler_name = MinMaxScaler(feature_range=(0,1))\n",
    "df_filtered[\"Name_Label\"] = scaler_name.fit_transform(df_filtered[[\"Name_Label\"]])\n",
    "\n",
    "df_filtered.drop(columns=[\"Name\"], inplace=True)\n",
    "\n",
    "# (1b) FeatureHash \"Team\", \"Sport\", \"Event\" (NOT NOC)\n",
    "n_features = 2100  # reduce or enlarge as desired\n",
    "hasher_team  = FeatureHasher(n_features=n_features, input_type='string')\n",
    "hasher_sport = FeatureHasher(n_features=n_features, input_type='string')\n",
    "hasher_event = FeatureHasher(n_features=n_features, input_type='string')\n",
    "\n",
    "for col, hasher_obj, prefix in [\n",
    "    (\"Team\",  hasher_team,  \"Team_hashed_\"),\n",
    "    (\"Sport\", hasher_sport, \"Sport_hashed_\"),\n",
    "    (\"Event\", hasher_event, \"Event_hashed_\"),\n",
    "]:\n",
    "    hashed = hasher_obj.transform(df_filtered[col].apply(lambda x: [x]))\n",
    "    hashed_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "        hashed,\n",
    "        columns=[f\"{prefix}{i}\" for i in range(n_features)]\n",
    "    )\n",
    "    df_filtered.drop(columns=[col], inplace=True)\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "    hashed_df.reset_index(drop=True, inplace=True)\n",
    "    df_filtered = pd.concat([df_filtered, hashed_df], axis=1)\n",
    "\n",
    "# (1c) One-hot for \"Sex\",\"City\",\"Medal\"\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohe_array = ohe.fit_transform(df_filtered[[\"Sex\",\"City\",\"Medal\"]])\n",
    "ohe_cols  = ohe.get_feature_names_out([\"Sex\",\"City\",\"Medal\"])\n",
    "ohe_df    = pd.DataFrame(ohe_array.toarray(), columns=ohe_cols)\n",
    "\n",
    "df_filtered.drop(columns=[\"Sex\",\"City\",\"Medal\"], inplace=True)\n",
    "df_filtered.reset_index(drop=True, inplace=True)\n",
    "ohe_df.reset_index(drop=True, inplace=True)\n",
    "df_filtered = pd.concat([df_filtered, ohe_df], axis=1)\n",
    "\n",
    "print(\"After row-level transforms, df_filtered shape:\", df_filtered.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Group by (Year, NOC)\n",
    "# ------------------------------------------------------------------\n",
    "# We'll group each country-year as a single row, summing hashed columns.\n",
    "df_filtered[\"Year\"] = df_filtered[\"Year\"].astype(int)\n",
    "\n",
    "grouped_cols = df_filtered.drop(\n",
    "    columns=[\"Medal_Gold\",\"Medal_Silver\",\"Medal_Bronze\",\"Year\"], \n",
    "    errors=\"ignore\"\n",
    ").columns.difference([\"NOC\"])\n",
    "\n",
    "df_agg = df_filtered.groupby([\"Year\",\"NOC\"], as_index=False).agg({c:\"sum\" for c in grouped_cols})\n",
    "df_agg = df_agg.merge(\n",
    "    df_filtered.groupby([\"Year\",\"NOC\"], as_index=False).agg({\n",
    "        \"Medal_Gold\":\"sum\",\"Medal_Silver\":\"sum\",\"Medal_Bronze\":\"sum\"\n",
    "    }),\n",
    "    on=[\"Year\",\"NOC\"], how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Grouped by (Year, NOC) shape:\", df_agg.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Encode NOC at the aggregated level\n",
    "# ------------------------------------------------------------------\n",
    "# We'll one-hot encode NOC here. If you have many NOCs, consider label encoding or hashing NOC alone.\n",
    "ohe_noc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "noc_arr = ohe_noc.fit_transform(df_agg[[\"NOC\"]])\n",
    "noc_cols = [f\"NOC_{cat}\" for cat in ohe_noc.categories_[0]]\n",
    "noc_df   = pd.DataFrame(noc_arr.toarray(), columns=noc_cols)\n",
    "\n",
    "df_agg.reset_index(drop=True, inplace=True)\n",
    "noc_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_agg = pd.concat([df_agg.drop(columns=[\"NOC\"]), noc_df], axis=1)\n",
    "\n",
    "print(\"After NOC encoding, df_agg shape:\", df_agg.shape)\n",
    "\n",
    "# Convert numeric\n",
    "num_cols = df_agg.select_dtypes(include=[\"number\"]).columns\n",
    "df_agg[num_cols] = df_agg[num_cols].astype(\"float32\")\n",
    "df_agg[\"Year\"]   = df_agg[\"Year\"].astype(\"int32\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Scale Features / Targets\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_cols = df_agg.drop(columns=[\"Year\",\"Medal_Gold\",\"Medal_Silver\",\"Medal_Bronze\"]).columns\n",
    "target_cols  = [\"Medal_Gold\",\"Medal_Silver\",\"Medal_Bronze\"]\n",
    "\n",
    "scaler_features = StandardScaler()\n",
    "scaler_targets  = StandardScaler()\n",
    "\n",
    "# We'll store scaled data in new DataFrames\n",
    "df_agg_scaled = df_agg.copy()\n",
    "\n",
    "X_to_scale = df_agg_scaled[feature_cols].values\n",
    "y_to_scale = df_agg_scaled[target_cols].values\n",
    "\n",
    "X_scaled = scaler_features.fit_transform(X_to_scale)\n",
    "y_scaled = scaler_targets.fit_transform(y_to_scale)\n",
    "\n",
    "df_agg_scaled[feature_cols] = X_scaled\n",
    "df_agg_scaled[target_cols]  = y_scaled\n",
    "\n",
    "print(\"df_agg_scaled shape:\", df_agg_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f64c5b3-b5fb-4c65-9db6-7288fbdc07eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2603, 6561) (2603, 3)\n",
      "Test  shape: (619, 6561) (619, 3)\n",
      "\n",
      "Training on 438 samples, validating on 433 samples.\n",
      "Epoch 1/300\n",
      "14/14 - 2s - 143ms/step - loss: 13.5495 - mae: 0.6460 - val_loss: 10.5405 - val_mae: 0.4372 - learning_rate: 5.0000e-04\n",
      "Epoch 2/300\n",
      "14/14 - 0s - 20ms/step - loss: 9.3960 - mae: 0.5120 - val_loss: 8.0177 - val_mae: 0.3940 - learning_rate: 5.0000e-04\n",
      "Epoch 3/300\n",
      "14/14 - 0s - 26ms/step - loss: 7.4397 - mae: 0.4380 - val_loss: 6.8350 - val_mae: 0.3644 - learning_rate: 5.0000e-04\n",
      "Epoch 4/300\n",
      "14/14 - 0s - 20ms/step - loss: 6.4943 - mae: 0.4000 - val_loss: 6.2971 - val_mae: 0.3440 - learning_rate: 5.0000e-04\n",
      "Epoch 5/300\n",
      "14/14 - 0s - 20ms/step - loss: 6.0479 - mae: 0.3629 - val_loss: 6.0279 - val_mae: 0.3372 - learning_rate: 5.0000e-04\n",
      "Epoch 6/300\n",
      "14/14 - 0s - 20ms/step - loss: 5.8045 - mae: 0.3500 - val_loss: 5.8453 - val_mae: 0.3233 - learning_rate: 5.0000e-04\n",
      "Epoch 7/300\n",
      "14/14 - 0s - 20ms/step - loss: 5.5657 - mae: 0.3221 - val_loss: 5.6808 - val_mae: 0.3118 - learning_rate: 5.0000e-04\n",
      "Epoch 8/300\n",
      "14/14 - 0s - 20ms/step - loss: 5.3906 - mae: 0.3141 - val_loss: 5.5262 - val_mae: 0.3069 - learning_rate: 5.0000e-04\n",
      "Epoch 9/300\n",
      "14/14 - 0s - 20ms/step - loss: 5.2420 - mae: 0.3101 - val_loss: 5.3741 - val_mae: 0.3044 - learning_rate: 5.0000e-04\n",
      "Epoch 10/300\n",
      "14/14 - 0s - 20ms/step - loss: 5.1032 - mae: 0.3058 - val_loss: 5.2229 - val_mae: 0.2980 - learning_rate: 5.0000e-04\n",
      "Epoch 11/300\n",
      "14/14 - 0s - 20ms/step - loss: 4.9007 - mae: 0.2923 - val_loss: 5.1083 - val_mae: 0.3012 - learning_rate: 5.0000e-04\n",
      "Epoch 12/300\n",
      "14/14 - 0s - 20ms/step - loss: 4.7948 - mae: 0.2965 - val_loss: 4.9463 - val_mae: 0.3008 - learning_rate: 5.0000e-04\n",
      "Epoch 13/300\n",
      "14/14 - 0s - 19ms/step - loss: 4.6311 - mae: 0.2950 - val_loss: 4.7691 - val_mae: 0.2766 - learning_rate: 5.0000e-04\n",
      "Epoch 14/300\n",
      "14/14 - 0s - 26ms/step - loss: 4.5044 - mae: 0.2910 - val_loss: 4.6607 - val_mae: 0.2899 - learning_rate: 5.0000e-04\n",
      "Epoch 15/300\n",
      "14/14 - 0s - 20ms/step - loss: 4.3100 - mae: 0.2639 - val_loss: 4.5728 - val_mae: 0.3050 - learning_rate: 5.0000e-04\n",
      "Epoch 16/300\n",
      "14/14 - 0s - 20ms/step - loss: 4.1936 - mae: 0.2759 - val_loss: 4.4224 - val_mae: 0.2850 - learning_rate: 5.0000e-04\n",
      "Epoch 17/300\n",
      "14/14 - 0s - 20ms/step - loss: 4.0574 - mae: 0.2614 - val_loss: 4.2718 - val_mae: 0.2684 - learning_rate: 5.0000e-04\n",
      "Epoch 18/300\n",
      "14/14 - 0s - 20ms/step - loss: 3.9332 - mae: 0.2625 - val_loss: 4.1393 - val_mae: 0.2695 - learning_rate: 5.0000e-04\n",
      "Epoch 19/300\n",
      "14/14 - 0s - 20ms/step - loss: 3.7887 - mae: 0.2509 - val_loss: 4.0249 - val_mae: 0.2723 - learning_rate: 5.0000e-04\n",
      "Epoch 20/300\n",
      "14/14 - 0s - 20ms/step - loss: 3.6809 - mae: 0.2507 - val_loss: 3.9053 - val_mae: 0.2722 - learning_rate: 5.0000e-04\n",
      "Epoch 21/300\n",
      "14/14 - 0s - 21ms/step - loss: 3.5372 - mae: 0.2396 - val_loss: 3.8114 - val_mae: 0.2683 - learning_rate: 5.0000e-04\n",
      "Epoch 22/300\n",
      "14/14 - 0s - 21ms/step - loss: 3.4316 - mae: 0.2408 - val_loss: 3.6996 - val_mae: 0.2724 - learning_rate: 5.0000e-04\n",
      "Epoch 23/300\n",
      "14/14 - 0s - 20ms/step - loss: 3.3139 - mae: 0.2414 - val_loss: 3.5529 - val_mae: 0.2621 - learning_rate: 5.0000e-04\n",
      "Epoch 24/300\n",
      "14/14 - 0s - 21ms/step - loss: 3.2359 - mae: 0.2473 - val_loss: 3.4790 - val_mae: 0.2861 - learning_rate: 5.0000e-04\n",
      "Epoch 25/300\n",
      "14/14 - 0s - 20ms/step - loss: 3.1048 - mae: 0.2388 - val_loss: 3.3786 - val_mae: 0.2711 - learning_rate: 5.0000e-04\n",
      "Epoch 26/300\n",
      "14/14 - 0s - 20ms/step - loss: 3.0152 - mae: 0.2270 - val_loss: 3.2822 - val_mae: 0.2705 - learning_rate: 5.0000e-04\n",
      "Epoch 27/300\n",
      "14/14 - 0s - 21ms/step - loss: 2.8861 - mae: 0.2260 - val_loss: 3.1821 - val_mae: 0.2792 - learning_rate: 5.0000e-04\n",
      "Epoch 28/300\n",
      "14/14 - 0s - 20ms/step - loss: 2.7746 - mae: 0.2178 - val_loss: 3.0875 - val_mae: 0.2795 - learning_rate: 5.0000e-04\n",
      "Epoch 29/300\n",
      "14/14 - 0s - 20ms/step - loss: 2.6915 - mae: 0.2176 - val_loss: 2.9990 - val_mae: 0.2691 - learning_rate: 5.0000e-04\n",
      "Epoch 30/300\n",
      "14/14 - 0s - 19ms/step - loss: 2.5967 - mae: 0.2115 - val_loss: 2.9261 - val_mae: 0.2879 - learning_rate: 5.0000e-04\n",
      "Epoch 31/300\n",
      "14/14 - 0s - 21ms/step - loss: 2.4797 - mae: 0.2055 - val_loss: 2.8320 - val_mae: 0.2856 - learning_rate: 5.0000e-04\n",
      "Epoch 32/300\n",
      "14/14 - 0s - 20ms/step - loss: 2.4098 - mae: 0.2113 - val_loss: 2.7329 - val_mae: 0.2780 - learning_rate: 5.0000e-04\n",
      "Epoch 33/300\n",
      "14/14 - 0s - 20ms/step - loss: 2.3437 - mae: 0.2126 - val_loss: 2.6613 - val_mae: 0.2827 - learning_rate: 5.0000e-04\n",
      "Epoch 34/300\n",
      "14/14 - 0s - 20ms/step - loss: 2.2545 - mae: 0.2085 - val_loss: 2.5820 - val_mae: 0.2766 - learning_rate: 5.0000e-04\n",
      "Epoch 35/300\n",
      "14/14 - 0s - 20ms/step - loss: 2.1871 - mae: 0.2073 - val_loss: 2.5379 - val_mae: 0.2792 - learning_rate: 5.0000e-04\n",
      "Epoch 36/300\n",
      "14/14 - 0s - 21ms/step - loss: 2.1004 - mae: 0.2043 - val_loss: 2.4547 - val_mae: 0.2777 - learning_rate: 5.0000e-04\n",
      "Epoch 37/300\n",
      "14/14 - 0s - 20ms/step - loss: 2.0218 - mae: 0.2003 - val_loss: 2.3518 - val_mae: 0.2748 - learning_rate: 5.0000e-04\n",
      "Epoch 38/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.9433 - mae: 0.1919 - val_loss: 2.2870 - val_mae: 0.2755 - learning_rate: 5.0000e-04\n",
      "Epoch 39/300\n",
      "14/14 - 0s - 19ms/step - loss: 1.9003 - mae: 0.2031 - val_loss: 2.2326 - val_mae: 0.2817 - learning_rate: 5.0000e-04\n",
      "Epoch 40/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.8228 - mae: 0.2012 - val_loss: 2.1680 - val_mae: 0.2905 - learning_rate: 5.0000e-04\n",
      "Epoch 41/300\n",
      "14/14 - 0s - 19ms/step - loss: 1.7829 - mae: 0.2004 - val_loss: 2.0802 - val_mae: 0.2768 - learning_rate: 5.0000e-04\n",
      "Epoch 42/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.6988 - mae: 0.1935 - val_loss: 2.0625 - val_mae: 0.2829 - learning_rate: 5.0000e-04\n",
      "Epoch 43/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.6092 - mae: 0.1828 - val_loss: 2.0079 - val_mae: 0.2857 - learning_rate: 5.0000e-04\n",
      "Epoch 44/300\n",
      "14/14 - 0s - 19ms/step - loss: 1.6091 - mae: 0.1969 - val_loss: 1.9347 - val_mae: 0.2851 - learning_rate: 5.0000e-04\n",
      "Epoch 45/300\n",
      "14/14 - 0s - 21ms/step - loss: 1.5194 - mae: 0.1824 - val_loss: 1.8876 - val_mae: 0.2772 - learning_rate: 5.0000e-04\n",
      "Epoch 46/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.4892 - mae: 0.1936 - val_loss: 1.8637 - val_mae: 0.2833 - learning_rate: 5.0000e-04\n",
      "Epoch 47/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.4486 - mae: 0.1958 - val_loss: 1.8085 - val_mae: 0.2883 - learning_rate: 5.0000e-04\n",
      "Epoch 48/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.3939 - mae: 0.1999 - val_loss: 1.7689 - val_mae: 0.2997 - learning_rate: 5.0000e-04\n",
      "Epoch 49/300\n",
      "14/14 - 0s - 21ms/step - loss: 1.3746 - mae: 0.1937 - val_loss: 1.6917 - val_mae: 0.2689 - learning_rate: 5.0000e-04\n",
      "Epoch 50/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.3248 - mae: 0.2006 - val_loss: 1.6859 - val_mae: 0.2887 - learning_rate: 5.0000e-04\n",
      "Epoch 51/300\n",
      "14/14 - 0s - 21ms/step - loss: 1.2264 - mae: 0.1712 - val_loss: 1.6349 - val_mae: 0.2836 - learning_rate: 5.0000e-04\n",
      "Epoch 52/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.2150 - mae: 0.1879 - val_loss: 1.5839 - val_mae: 0.2854 - learning_rate: 5.0000e-04\n",
      "Epoch 53/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.1660 - mae: 0.1846 - val_loss: 1.5227 - val_mae: 0.2756 - learning_rate: 5.0000e-04\n",
      "Epoch 54/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.1204 - mae: 0.1822 - val_loss: 1.5503 - val_mae: 0.2991 - learning_rate: 5.0000e-04\n",
      "Epoch 55/300\n",
      "14/14 - 0s - 19ms/step - loss: 1.0669 - mae: 0.1722 - val_loss: 1.4667 - val_mae: 0.2805 - learning_rate: 5.0000e-04\n",
      "Epoch 56/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.0299 - mae: 0.1728 - val_loss: 1.4106 - val_mae: 0.2810 - learning_rate: 5.0000e-04\n",
      "Epoch 57/300\n",
      "14/14 - 0s - 20ms/step - loss: 1.0022 - mae: 0.1711 - val_loss: 1.4060 - val_mae: 0.2865 - learning_rate: 5.0000e-04\n",
      "Epoch 58/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.9617 - mae: 0.1711 - val_loss: 1.3625 - val_mae: 0.2912 - learning_rate: 5.0000e-04\n",
      "Epoch 59/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.9419 - mae: 0.1738 - val_loss: 1.3390 - val_mae: 0.2858 - learning_rate: 5.0000e-04\n",
      "Epoch 60/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.9045 - mae: 0.1748 - val_loss: 1.3198 - val_mae: 0.2970 - learning_rate: 5.0000e-04\n",
      "Epoch 61/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.8668 - mae: 0.1644 - val_loss: 1.2537 - val_mae: 0.2762 - learning_rate: 5.0000e-04\n",
      "Epoch 62/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.8235 - mae: 0.1594 - val_loss: 1.2308 - val_mae: 0.2805 - learning_rate: 5.0000e-04\n",
      "Epoch 63/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.8073 - mae: 0.1568 - val_loss: 1.2129 - val_mae: 0.2806 - learning_rate: 5.0000e-04\n",
      "Epoch 64/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.7936 - mae: 0.1648 - val_loss: 1.1935 - val_mae: 0.2899 - learning_rate: 5.0000e-04\n",
      "Epoch 65/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.8021 - mae: 0.1769 - val_loss: 1.1745 - val_mae: 0.2772 - learning_rate: 5.0000e-04\n",
      "Epoch 66/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.8000 - mae: 0.1921 - val_loss: 1.1602 - val_mae: 0.2752 - learning_rate: 5.0000e-04\n",
      "Epoch 67/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.7267 - mae: 0.1745 - val_loss: 1.0887 - val_mae: 0.2848 - learning_rate: 5.0000e-04\n",
      "Epoch 68/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.7185 - mae: 0.1803 - val_loss: 1.0944 - val_mae: 0.2851 - learning_rate: 5.0000e-04\n",
      "Epoch 69/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.7148 - mae: 0.1846 - val_loss: 1.0731 - val_mae: 0.2995 - learning_rate: 5.0000e-04\n",
      "Epoch 70/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.6664 - mae: 0.1736 - val_loss: 1.0337 - val_mae: 0.2835 - learning_rate: 5.0000e-04\n",
      "Epoch 71/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.6533 - mae: 0.1686 - val_loss: 1.0144 - val_mae: 0.2742 - learning_rate: 5.0000e-04\n",
      "Epoch 72/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.6228 - mae: 0.1647 - val_loss: 1.0332 - val_mae: 0.3036 - learning_rate: 5.0000e-04\n",
      "Epoch 73/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.6188 - mae: 0.1677 - val_loss: 0.9894 - val_mae: 0.2764 - learning_rate: 5.0000e-04\n",
      "Epoch 74/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.5918 - mae: 0.1607 - val_loss: 0.9796 - val_mae: 0.2850 - learning_rate: 5.0000e-04\n",
      "Epoch 75/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.5992 - mae: 0.1747 - val_loss: 0.9731 - val_mae: 0.2875 - learning_rate: 5.0000e-04\n",
      "Epoch 76/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.5765 - mae: 0.1760 - val_loss: 0.9671 - val_mae: 0.2886 - learning_rate: 5.0000e-04\n",
      "Epoch 77/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.5738 - mae: 0.1769 - val_loss: 0.9581 - val_mae: 0.2928 - learning_rate: 5.0000e-04\n",
      "Epoch 78/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.5365 - mae: 0.1688 - val_loss: 0.9230 - val_mae: 0.2820 - learning_rate: 5.0000e-04\n",
      "Epoch 79/300\n",
      "14/14 - 0s - 23ms/step - loss: 0.5027 - mae: 0.1596 - val_loss: 0.9030 - val_mae: 0.2919 - learning_rate: 5.0000e-04\n",
      "Epoch 80/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.5043 - mae: 0.1689 - val_loss: 0.8715 - val_mae: 0.2693 - learning_rate: 5.0000e-04\n",
      "Epoch 81/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.5045 - mae: 0.1735 - val_loss: 0.8589 - val_mae: 0.2745 - learning_rate: 5.0000e-04\n",
      "Epoch 82/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.4785 - mae: 0.1652 - val_loss: 0.8551 - val_mae: 0.2865 - learning_rate: 5.0000e-04\n",
      "Epoch 83/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.4710 - mae: 0.1661 - val_loss: 0.8689 - val_mae: 0.2839 - learning_rate: 5.0000e-04\n",
      "Epoch 84/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.4581 - mae: 0.1681 - val_loss: 0.8382 - val_mae: 0.2899 - learning_rate: 5.0000e-04\n",
      "Epoch 85/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.4541 - mae: 0.1693 - val_loss: 0.8148 - val_mae: 0.2703 - learning_rate: 5.0000e-04\n",
      "Epoch 86/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.4543 - mae: 0.1703 - val_loss: 0.8114 - val_mae: 0.2865 - learning_rate: 5.0000e-04\n",
      "Epoch 87/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.4288 - mae: 0.1675 - val_loss: 0.7990 - val_mae: 0.2949 - learning_rate: 5.0000e-04\n",
      "Epoch 88/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3925 - mae: 0.1567 - val_loss: 0.7823 - val_mae: 0.2889 - learning_rate: 5.0000e-04\n",
      "Epoch 89/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.4029 - mae: 0.1608 - val_loss: 0.7786 - val_mae: 0.2758 - learning_rate: 5.0000e-04\n",
      "Epoch 90/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.4006 - mae: 0.1675 - val_loss: 0.7412 - val_mae: 0.2883 - learning_rate: 5.0000e-04\n",
      "Epoch 91/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.4115 - mae: 0.1731 - val_loss: 0.7923 - val_mae: 0.2950 - learning_rate: 5.0000e-04\n",
      "Epoch 92/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3785 - mae: 0.1642 - val_loss: 0.7250 - val_mae: 0.2713 - learning_rate: 5.0000e-04\n",
      "Epoch 93/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3742 - mae: 0.1691 - val_loss: 0.7211 - val_mae: 0.2957 - learning_rate: 5.0000e-04\n",
      "Epoch 94/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.3668 - mae: 0.1632 - val_loss: 0.7002 - val_mae: 0.2869 - learning_rate: 5.0000e-04\n",
      "Epoch 95/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3573 - mae: 0.1617 - val_loss: 0.7063 - val_mae: 0.2886 - learning_rate: 5.0000e-04\n",
      "Epoch 96/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3780 - mae: 0.1745 - val_loss: 0.7276 - val_mae: 0.2749 - learning_rate: 5.0000e-04\n",
      "Epoch 97/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3442 - mae: 0.1675 - val_loss: 0.6897 - val_mae: 0.2830 - learning_rate: 5.0000e-04\n",
      "Epoch 98/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3232 - mae: 0.1641 - val_loss: 0.6852 - val_mae: 0.2708 - learning_rate: 5.0000e-04\n",
      "Epoch 99/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3280 - mae: 0.1587 - val_loss: 0.7138 - val_mae: 0.2866 - learning_rate: 5.0000e-04\n",
      "Epoch 100/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.3171 - mae: 0.1552 - val_loss: 0.6634 - val_mae: 0.2732 - learning_rate: 5.0000e-04\n",
      "Epoch 101/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.3044 - mae: 0.1493 - val_loss: 0.6986 - val_mae: 0.2930 - learning_rate: 5.0000e-04\n",
      "Epoch 102/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.3062 - mae: 0.1591 - val_loss: 0.6515 - val_mae: 0.2754 - learning_rate: 5.0000e-04\n",
      "Epoch 103/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2991 - mae: 0.1566 - val_loss: 0.6370 - val_mae: 0.2955 - learning_rate: 5.0000e-04\n",
      "Epoch 104/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2891 - mae: 0.1567 - val_loss: 0.6473 - val_mae: 0.2968 - learning_rate: 5.0000e-04\n",
      "Epoch 105/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2768 - mae: 0.1485 - val_loss: 0.6560 - val_mae: 0.2813 - learning_rate: 5.0000e-04\n",
      "Epoch 106/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2838 - mae: 0.1594 - val_loss: 0.6629 - val_mae: 0.2793 - learning_rate: 5.0000e-04\n",
      "Epoch 107/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2779 - mae: 0.1605 - val_loss: 0.6203 - val_mae: 0.2887 - learning_rate: 5.0000e-04\n",
      "Epoch 108/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2751 - mae: 0.1625 - val_loss: 0.5889 - val_mae: 0.2541 - learning_rate: 5.0000e-04\n",
      "Epoch 109/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2671 - mae: 0.1622 - val_loss: 0.6188 - val_mae: 0.2824 - learning_rate: 5.0000e-04\n",
      "Epoch 110/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.2725 - mae: 0.1668 - val_loss: 0.5666 - val_mae: 0.2652 - learning_rate: 5.0000e-04\n",
      "Epoch 111/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.3018 - mae: 0.1809 - val_loss: 0.6932 - val_mae: 0.3030 - learning_rate: 5.0000e-04\n",
      "Epoch 112/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2881 - mae: 0.1808 - val_loss: 0.5748 - val_mae: 0.2409 - learning_rate: 5.0000e-04\n",
      "Epoch 113/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2720 - mae: 0.1660 - val_loss: 0.5848 - val_mae: 0.2684 - learning_rate: 5.0000e-04\n",
      "Epoch 114/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.3047 - mae: 0.1674 - val_loss: 0.5254 - val_mae: 0.2381 - learning_rate: 5.0000e-04\n",
      "Epoch 115/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2757 - mae: 0.1733 - val_loss: 0.5503 - val_mae: 0.2456 - learning_rate: 5.0000e-04\n",
      "Epoch 116/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2840 - mae: 0.1801 - val_loss: 0.6075 - val_mae: 0.2653 - learning_rate: 5.0000e-04\n",
      "Epoch 117/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2652 - mae: 0.1697 - val_loss: 0.5914 - val_mae: 0.2579 - learning_rate: 5.0000e-04\n",
      "Epoch 118/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2743 - mae: 0.1748 - val_loss: 0.5731 - val_mae: 0.2886 - learning_rate: 5.0000e-04\n",
      "Epoch 119/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2635 - mae: 0.1696 - val_loss: 0.5731 - val_mae: 0.2632 - learning_rate: 5.0000e-04\n",
      "Epoch 120/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2661 - mae: 0.1738 - val_loss: 0.5462 - val_mae: 0.2578 - learning_rate: 5.0000e-04\n",
      "Epoch 121/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2576 - mae: 0.1635 - val_loss: 0.5738 - val_mae: 0.2643 - learning_rate: 5.0000e-04\n",
      "Epoch 122/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2402 - mae: 0.1559 - val_loss: 0.5361 - val_mae: 0.2644 - learning_rate: 5.0000e-04\n",
      "Epoch 123/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2326 - mae: 0.1540 - val_loss: 0.5712 - val_mae: 0.2687 - learning_rate: 5.0000e-04\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "14/14 - 0s - 20ms/step - loss: 0.2257 - mae: 0.1540 - val_loss: 0.5943 - val_mae: 0.2907 - learning_rate: 5.0000e-04\n",
      "Epoch 125/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.2160 - mae: 0.1490 - val_loss: 0.5703 - val_mae: 0.2688 - learning_rate: 2.5000e-04\n",
      "Epoch 126/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2125 - mae: 0.1387 - val_loss: 0.5479 - val_mae: 0.2632 - learning_rate: 2.5000e-04\n",
      "Epoch 127/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.2043 - mae: 0.1331 - val_loss: 0.5782 - val_mae: 0.2778 - learning_rate: 2.5000e-04\n",
      "Epoch 128/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.2094 - mae: 0.1350 - val_loss: 0.5632 - val_mae: 0.2789 - learning_rate: 2.5000e-04\n",
      "Epoch 129/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1893 - mae: 0.1261 - val_loss: 0.5485 - val_mae: 0.2754 - learning_rate: 2.5000e-04\n",
      "Epoch 130/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1927 - mae: 0.1294 - val_loss: 0.5554 - val_mae: 0.2822 - learning_rate: 2.5000e-04\n",
      "Epoch 131/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1754 - mae: 0.1244 - val_loss: 0.5646 - val_mae: 0.2854 - learning_rate: 2.5000e-04\n",
      "Epoch 132/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1790 - mae: 0.1258 - val_loss: 0.5509 - val_mae: 0.2822 - learning_rate: 2.5000e-04\n",
      "Epoch 133/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1720 - mae: 0.1217 - val_loss: 0.5450 - val_mae: 0.2864 - learning_rate: 2.5000e-04\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "14/14 - 0s - 21ms/step - loss: 0.1996 - mae: 0.1437 - val_loss: 0.5483 - val_mae: 0.2840 - learning_rate: 2.5000e-04\n",
      "Epoch 135/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1687 - mae: 0.1248 - val_loss: 0.5421 - val_mae: 0.2804 - learning_rate: 1.2500e-04\n",
      "Epoch 136/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1670 - mae: 0.1214 - val_loss: 0.5314 - val_mae: 0.2776 - learning_rate: 1.2500e-04\n",
      "Epoch 137/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1649 - mae: 0.1147 - val_loss: 0.5205 - val_mae: 0.2744 - learning_rate: 1.2500e-04\n",
      "Epoch 138/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1614 - mae: 0.1178 - val_loss: 0.5268 - val_mae: 0.2790 - learning_rate: 1.2500e-04\n",
      "Epoch 139/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1642 - mae: 0.1201 - val_loss: 0.5184 - val_mae: 0.2806 - learning_rate: 1.2500e-04\n",
      "Epoch 140/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1585 - mae: 0.1149 - val_loss: 0.5331 - val_mae: 0.2868 - learning_rate: 1.2500e-04\n",
      "Epoch 141/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1596 - mae: 0.1172 - val_loss: 0.5295 - val_mae: 0.2845 - learning_rate: 1.2500e-04\n",
      "Epoch 142/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1616 - mae: 0.1222 - val_loss: 0.5213 - val_mae: 0.2812 - learning_rate: 1.2500e-04\n",
      "Epoch 143/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1541 - mae: 0.1182 - val_loss: 0.5106 - val_mae: 0.2743 - learning_rate: 1.2500e-04\n",
      "Epoch 144/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1598 - mae: 0.1164 - val_loss: 0.5270 - val_mae: 0.2714 - learning_rate: 1.2500e-04\n",
      "Epoch 145/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1480 - mae: 0.1167 - val_loss: 0.5409 - val_mae: 0.2760 - learning_rate: 1.2500e-04\n",
      "Epoch 146/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1491 - mae: 0.1165 - val_loss: 0.5213 - val_mae: 0.2749 - learning_rate: 1.2500e-04\n",
      "Epoch 147/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1428 - mae: 0.1120 - val_loss: 0.5121 - val_mae: 0.2779 - learning_rate: 1.2500e-04\n",
      "Epoch 148/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1523 - mae: 0.1232 - val_loss: 0.5118 - val_mae: 0.2782 - learning_rate: 1.2500e-04\n",
      "Epoch 149/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1483 - mae: 0.1189 - val_loss: 0.5209 - val_mae: 0.2725 - learning_rate: 1.2500e-04\n",
      "Epoch 150/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1432 - mae: 0.1159 - val_loss: 0.5217 - val_mae: 0.2730 - learning_rate: 1.2500e-04\n",
      "Epoch 151/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1395 - mae: 0.1129 - val_loss: 0.5025 - val_mae: 0.2696 - learning_rate: 1.2500e-04\n",
      "Epoch 152/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1441 - mae: 0.1192 - val_loss: 0.4952 - val_mae: 0.2723 - learning_rate: 1.2500e-04\n",
      "Epoch 153/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1463 - mae: 0.1188 - val_loss: 0.4998 - val_mae: 0.2758 - learning_rate: 1.2500e-04\n",
      "Epoch 154/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1389 - mae: 0.1168 - val_loss: 0.5028 - val_mae: 0.2724 - learning_rate: 1.2500e-04\n",
      "Epoch 155/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1426 - mae: 0.1211 - val_loss: 0.5028 - val_mae: 0.2701 - learning_rate: 1.2500e-04\n",
      "Epoch 156/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1348 - mae: 0.1163 - val_loss: 0.4958 - val_mae: 0.2703 - learning_rate: 1.2500e-04\n",
      "Epoch 157/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1273 - mae: 0.1103 - val_loss: 0.4959 - val_mae: 0.2726 - learning_rate: 1.2500e-04\n",
      "Epoch 158/300\n",
      "14/14 - 0s - 23ms/step - loss: 0.1405 - mae: 0.1193 - val_loss: 0.4936 - val_mae: 0.2700 - learning_rate: 1.2500e-04\n",
      "Epoch 159/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1349 - mae: 0.1147 - val_loss: 0.4884 - val_mae: 0.2636 - learning_rate: 1.2500e-04\n",
      "Epoch 160/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1292 - mae: 0.1169 - val_loss: 0.4891 - val_mae: 0.2659 - learning_rate: 1.2500e-04\n",
      "Epoch 161/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1308 - mae: 0.1144 - val_loss: 0.5011 - val_mae: 0.2755 - learning_rate: 1.2500e-04\n",
      "Epoch 162/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1189 - mae: 0.1088 - val_loss: 0.4890 - val_mae: 0.2681 - learning_rate: 1.2500e-04\n",
      "Epoch 163/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1327 - mae: 0.1197 - val_loss: 0.4914 - val_mae: 0.2665 - learning_rate: 1.2500e-04\n",
      "Epoch 164/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1221 - mae: 0.1109 - val_loss: 0.4932 - val_mae: 0.2700 - learning_rate: 1.2500e-04\n",
      "Epoch 165/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1281 - mae: 0.1133 - val_loss: 0.4709 - val_mae: 0.2668 - learning_rate: 1.2500e-04\n",
      "Epoch 166/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1310 - mae: 0.1205 - val_loss: 0.4937 - val_mae: 0.2746 - learning_rate: 1.2500e-04\n",
      "Epoch 167/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1297 - mae: 0.1225 - val_loss: 0.4768 - val_mae: 0.2661 - learning_rate: 1.2500e-04\n",
      "Epoch 168/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1280 - mae: 0.1203 - val_loss: 0.4958 - val_mae: 0.2732 - learning_rate: 1.2500e-04\n",
      "Epoch 169/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1267 - mae: 0.1175 - val_loss: 0.4712 - val_mae: 0.2605 - learning_rate: 1.2500e-04\n",
      "Epoch 170/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1240 - mae: 0.1196 - val_loss: 0.4624 - val_mae: 0.2597 - learning_rate: 1.2500e-04\n",
      "Epoch 171/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1442 - mae: 0.1292 - val_loss: 0.4671 - val_mae: 0.2564 - learning_rate: 1.2500e-04\n",
      "Epoch 172/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1169 - mae: 0.1138 - val_loss: 0.4558 - val_mae: 0.2551 - learning_rate: 1.2500e-04\n",
      "Epoch 173/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1199 - mae: 0.1136 - val_loss: 0.4703 - val_mae: 0.2617 - learning_rate: 1.2500e-04\n",
      "Epoch 174/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1264 - mae: 0.1198 - val_loss: 0.4534 - val_mae: 0.2654 - learning_rate: 1.2500e-04\n",
      "Epoch 175/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1222 - mae: 0.1218 - val_loss: 0.4624 - val_mae: 0.2649 - learning_rate: 1.2500e-04\n",
      "Epoch 176/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1141 - mae: 0.1192 - val_loss: 0.4661 - val_mae: 0.2628 - learning_rate: 1.2500e-04\n",
      "Epoch 177/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1276 - mae: 0.1224 - val_loss: 0.4536 - val_mae: 0.2647 - learning_rate: 1.2500e-04\n",
      "Epoch 178/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1326 - mae: 0.1274 - val_loss: 0.4722 - val_mae: 0.2646 - learning_rate: 1.2500e-04\n",
      "Epoch 179/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1134 - mae: 0.1165 - val_loss: 0.4634 - val_mae: 0.2569 - learning_rate: 1.2500e-04\n",
      "Epoch 180/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1118 - mae: 0.1164 - val_loss: 0.4304 - val_mae: 0.2631 - learning_rate: 1.2500e-04\n",
      "Epoch 181/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1214 - mae: 0.1269 - val_loss: 0.4815 - val_mae: 0.2759 - learning_rate: 1.2500e-04\n",
      "Epoch 182/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1232 - mae: 0.1272 - val_loss: 0.4467 - val_mae: 0.2562 - learning_rate: 1.2500e-04\n",
      "Epoch 183/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1401 - mae: 0.1302 - val_loss: 0.4299 - val_mae: 0.2530 - learning_rate: 1.2500e-04\n",
      "Epoch 184/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1206 - mae: 0.1255 - val_loss: 0.4166 - val_mae: 0.2431 - learning_rate: 1.2500e-04\n",
      "Epoch 185/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1108 - mae: 0.1176 - val_loss: 0.4502 - val_mae: 0.2587 - learning_rate: 1.2500e-04\n",
      "Epoch 186/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1153 - mae: 0.1181 - val_loss: 0.4304 - val_mae: 0.2562 - learning_rate: 1.2500e-04\n",
      "Epoch 187/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1140 - mae: 0.1192 - val_loss: 0.4433 - val_mae: 0.2571 - learning_rate: 1.2500e-04\n",
      "Epoch 188/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1207 - mae: 0.1294 - val_loss: 0.4283 - val_mae: 0.2555 - learning_rate: 1.2500e-04\n",
      "Epoch 189/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1095 - mae: 0.1164 - val_loss: 0.4573 - val_mae: 0.2601 - learning_rate: 1.2500e-04\n",
      "Epoch 190/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1083 - mae: 0.1207 - val_loss: 0.4297 - val_mae: 0.2593 - learning_rate: 1.2500e-04\n",
      "Epoch 191/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1066 - mae: 0.1161 - val_loss: 0.4419 - val_mae: 0.2634 - learning_rate: 1.2500e-04\n",
      "Epoch 192/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1147 - mae: 0.1276 - val_loss: 0.4374 - val_mae: 0.2515 - learning_rate: 1.2500e-04\n",
      "Epoch 193/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.1134 - mae: 0.1224 - val_loss: 0.4199 - val_mae: 0.2499 - learning_rate: 1.2500e-04\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "14/14 - 0s - 20ms/step - loss: 0.1121 - mae: 0.1188 - val_loss: 0.4480 - val_mae: 0.2641 - learning_rate: 1.2500e-04\n",
      "Epoch 195/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1065 - mae: 0.1189 - val_loss: 0.4515 - val_mae: 0.2618 - learning_rate: 6.2500e-05\n",
      "Epoch 196/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.0988 - mae: 0.1072 - val_loss: 0.4322 - val_mae: 0.2520 - learning_rate: 6.2500e-05\n",
      "Epoch 197/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1022 - mae: 0.1089 - val_loss: 0.4263 - val_mae: 0.2509 - learning_rate: 6.2500e-05\n",
      "Epoch 198/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0998 - mae: 0.1111 - val_loss: 0.4357 - val_mae: 0.2565 - learning_rate: 6.2500e-05\n",
      "Epoch 199/300\n",
      "14/14 - 0s - 21ms/step - loss: 0.1016 - mae: 0.1096 - val_loss: 0.4382 - val_mae: 0.2545 - learning_rate: 6.2500e-05\n",
      "Epoch 200/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1001 - mae: 0.1064 - val_loss: 0.4217 - val_mae: 0.2492 - learning_rate: 6.2500e-05\n",
      "Epoch 201/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1043 - mae: 0.1128 - val_loss: 0.4414 - val_mae: 0.2563 - learning_rate: 6.2500e-05\n",
      "Epoch 202/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.1021 - mae: 0.1124 - val_loss: 0.4412 - val_mae: 0.2578 - learning_rate: 6.2500e-05\n",
      "Epoch 203/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.1029 - mae: 0.1133 - val_loss: 0.4441 - val_mae: 0.2571 - learning_rate: 6.2500e-05\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 204: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "14/14 - 0s - 20ms/step - loss: 0.1037 - mae: 0.1131 - val_loss: 0.4279 - val_mae: 0.2538 - learning_rate: 6.2500e-05\n",
      "Epoch 205/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0961 - mae: 0.1064 - val_loss: 0.4327 - val_mae: 0.2548 - learning_rate: 3.1250e-05\n",
      "Epoch 206/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0985 - mae: 0.1035 - val_loss: 0.4325 - val_mae: 0.2533 - learning_rate: 3.1250e-05\n",
      "Epoch 207/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0999 - mae: 0.1096 - val_loss: 0.4315 - val_mae: 0.2512 - learning_rate: 3.1250e-05\n",
      "Epoch 208/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.0903 - mae: 0.1001 - val_loss: 0.4353 - val_mae: 0.2527 - learning_rate: 3.1250e-05\n",
      "Epoch 209/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.0961 - mae: 0.1065 - val_loss: 0.4381 - val_mae: 0.2532 - learning_rate: 3.1250e-05\n",
      "Epoch 210/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0885 - mae: 0.0983 - val_loss: 0.4292 - val_mae: 0.2505 - learning_rate: 3.1250e-05\n",
      "Epoch 211/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0930 - mae: 0.1040 - val_loss: 0.4274 - val_mae: 0.2521 - learning_rate: 3.1250e-05\n",
      "Epoch 212/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.0931 - mae: 0.1030 - val_loss: 0.4228 - val_mae: 0.2521 - learning_rate: 3.1250e-05\n",
      "Epoch 213/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0893 - mae: 0.0984 - val_loss: 0.4284 - val_mae: 0.2541 - learning_rate: 3.1250e-05\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 214: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "14/14 - 0s - 20ms/step - loss: 0.0992 - mae: 0.1061 - val_loss: 0.4264 - val_mae: 0.2524 - learning_rate: 3.1250e-05\n",
      "Epoch 215/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.0942 - mae: 0.1023 - val_loss: 0.4243 - val_mae: 0.2508 - learning_rate: 1.5625e-05\n",
      "Epoch 216/300\n",
      "14/14 - 0s - 22ms/step - loss: 0.0921 - mae: 0.1015 - val_loss: 0.4217 - val_mae: 0.2506 - learning_rate: 1.5625e-05\n",
      "Epoch 217/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0946 - mae: 0.1014 - val_loss: 0.4264 - val_mae: 0.2522 - learning_rate: 1.5625e-05\n",
      "Epoch 218/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.0846 - mae: 0.0964 - val_loss: 0.4282 - val_mae: 0.2524 - learning_rate: 1.5625e-05\n",
      "Epoch 219/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.0922 - mae: 0.1008 - val_loss: 0.4259 - val_mae: 0.2523 - learning_rate: 1.5625e-05\n",
      "Epoch 220/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0904 - mae: 0.0996 - val_loss: 0.4254 - val_mae: 0.2517 - learning_rate: 1.5625e-05\n",
      "Epoch 221/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0895 - mae: 0.1028 - val_loss: 0.4279 - val_mae: 0.2519 - learning_rate: 1.5625e-05\n",
      "Epoch 222/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0867 - mae: 0.0998 - val_loss: 0.4292 - val_mae: 0.2518 - learning_rate: 1.5625e-05\n",
      "Epoch 223/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0819 - mae: 0.0927 - val_loss: 0.4293 - val_mae: 0.2522 - learning_rate: 1.5625e-05\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 224: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "14/14 - 0s - 20ms/step - loss: 0.0890 - mae: 0.1011 - val_loss: 0.4334 - val_mae: 0.2545 - learning_rate: 1.5625e-05\n",
      "Epoch 225/300\n",
      "14/14 - 0s - 19ms/step - loss: 0.0979 - mae: 0.1050 - val_loss: 0.4349 - val_mae: 0.2550 - learning_rate: 7.8125e-06\n",
      "Epoch 226/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0936 - mae: 0.1019 - val_loss: 0.4358 - val_mae: 0.2544 - learning_rate: 7.8125e-06\n",
      "Epoch 227/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0940 - mae: 0.1018 - val_loss: 0.4337 - val_mae: 0.2535 - learning_rate: 7.8125e-06\n",
      "Epoch 228/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0882 - mae: 0.0988 - val_loss: 0.4324 - val_mae: 0.2531 - learning_rate: 7.8125e-06\n",
      "Epoch 229/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0849 - mae: 0.0963 - val_loss: 0.4318 - val_mae: 0.2531 - learning_rate: 7.8125e-06\n",
      "Epoch 230/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0866 - mae: 0.0951 - val_loss: 0.4290 - val_mae: 0.2527 - learning_rate: 7.8125e-06\n",
      "Epoch 231/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0876 - mae: 0.1029 - val_loss: 0.4281 - val_mae: 0.2530 - learning_rate: 7.8125e-06\n",
      "Epoch 232/300\n",
      "14/14 - 0s - 20ms/step - loss: 0.0931 - mae: 0.1037 - val_loss: 0.4290 - val_mae: 0.2535 - learning_rate: 7.8125e-06\n",
      "Epoch 233/300\n",
      "14/14 - 0s - 25ms/step - loss: 0.0842 - mae: 0.0959 - val_loss: 0.4298 - val_mae: 0.2531 - learning_rate: 7.8125e-06\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 234: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "14/14 - 0s - 19ms/step - loss: 0.0873 - mae: 0.0963 - val_loss: 0.4286 - val_mae: 0.2520 - learning_rate: 7.8125e-06\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Fold Validation MAE: 0.2431\n",
      "\n",
      "Training on 871 samples, validating on 433 samples.\n",
      "Epoch 1/300\n",
      "28/28 - 1s - 19ms/step - loss: 0.3674 - mae: 0.2847 - val_loss: 1.1304 - val_mae: 0.3217 - learning_rate: 3.9063e-06\n",
      "Epoch 2/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.3011 - mae: 0.2573 - val_loss: 1.1221 - val_mae: 0.3243 - learning_rate: 3.9063e-06\n",
      "Epoch 3/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.2770 - mae: 0.2457 - val_loss: 1.0846 - val_mae: 0.3225 - learning_rate: 3.9063e-06\n",
      "Epoch 4/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.2588 - mae: 0.2348 - val_loss: 1.0755 - val_mae: 0.3245 - learning_rate: 3.9063e-06\n",
      "Epoch 5/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.2422 - mae: 0.2229 - val_loss: 1.0535 - val_mae: 0.3240 - learning_rate: 3.9063e-06\n",
      "Epoch 6/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.2244 - mae: 0.2197 - val_loss: 1.0484 - val_mae: 0.3249 - learning_rate: 3.9063e-06\n",
      "Epoch 7/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.2226 - mae: 0.2152 - val_loss: 1.0363 - val_mae: 0.3247 - learning_rate: 3.9063e-06\n",
      "Epoch 8/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.2096 - mae: 0.2100 - val_loss: 1.0205 - val_mae: 0.3247 - learning_rate: 3.9063e-06\n",
      "Epoch 9/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.2036 - mae: 0.2067 - val_loss: 1.0128 - val_mae: 0.3248 - learning_rate: 3.9063e-06\n",
      "Epoch 10/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1871 - mae: 0.1970 - val_loss: 1.0162 - val_mae: 0.3257 - learning_rate: 3.9063e-06\n",
      "Epoch 11/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1803 - mae: 0.1935 - val_loss: 0.9984 - val_mae: 0.3237 - learning_rate: 3.9063e-06\n",
      "Epoch 12/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1830 - mae: 0.1925 - val_loss: 1.0222 - val_mae: 0.3273 - learning_rate: 3.9063e-06\n",
      "Epoch 13/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1765 - mae: 0.1904 - val_loss: 1.0211 - val_mae: 0.3277 - learning_rate: 3.9063e-06\n",
      "Epoch 14/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1660 - mae: 0.1837 - val_loss: 1.0089 - val_mae: 0.3251 - learning_rate: 3.9063e-06\n",
      "Epoch 15/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1647 - mae: 0.1835 - val_loss: 1.0038 - val_mae: 0.3239 - learning_rate: 3.9063e-06\n",
      "Epoch 16/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1649 - mae: 0.1824 - val_loss: 0.9959 - val_mae: 0.3223 - learning_rate: 3.9063e-06\n",
      "Epoch 17/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1588 - mae: 0.1785 - val_loss: 1.0092 - val_mae: 0.3238 - learning_rate: 3.9063e-06\n",
      "Epoch 18/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1566 - mae: 0.1740 - val_loss: 0.9918 - val_mae: 0.3215 - learning_rate: 3.9063e-06\n",
      "Epoch 19/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1576 - mae: 0.1771 - val_loss: 0.9917 - val_mae: 0.3209 - learning_rate: 3.9063e-06\n",
      "Epoch 20/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1540 - mae: 0.1743 - val_loss: 0.9829 - val_mae: 0.3183 - learning_rate: 3.9063e-06\n",
      "Epoch 21/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1528 - mae: 0.1716 - val_loss: 0.9696 - val_mae: 0.3166 - learning_rate: 3.9063e-06\n",
      "Epoch 22/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1501 - mae: 0.1688 - val_loss: 0.9875 - val_mae: 0.3182 - learning_rate: 3.9063e-06\n",
      "Epoch 23/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1425 - mae: 0.1648 - val_loss: 0.9848 - val_mae: 0.3172 - learning_rate: 3.9063e-06\n",
      "Epoch 24/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1457 - mae: 0.1648 - val_loss: 0.9892 - val_mae: 0.3173 - learning_rate: 3.9063e-06\n",
      "Epoch 25/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1425 - mae: 0.1612 - val_loss: 0.9866 - val_mae: 0.3161 - learning_rate: 3.9063e-06\n",
      "Epoch 26/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1428 - mae: 0.1642 - val_loss: 0.9849 - val_mae: 0.3141 - learning_rate: 3.9063e-06\n",
      "Epoch 27/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1417 - mae: 0.1591 - val_loss: 0.9906 - val_mae: 0.3144 - learning_rate: 3.9063e-06\n",
      "Epoch 28/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1430 - mae: 0.1615 - val_loss: 0.9766 - val_mae: 0.3126 - learning_rate: 3.9063e-06\n",
      "Epoch 29/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1378 - mae: 0.1578 - val_loss: 0.9949 - val_mae: 0.3147 - learning_rate: 3.9063e-06\n",
      "Epoch 30/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1399 - mae: 0.1575 - val_loss: 0.9953 - val_mae: 0.3141 - learning_rate: 3.9063e-06\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "28/28 - 0s - 16ms/step - loss: 0.1414 - mae: 0.1592 - val_loss: 0.9842 - val_mae: 0.3118 - learning_rate: 3.9063e-06\n",
      "Epoch 32/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1386 - mae: 0.1596 - val_loss: 0.9960 - val_mae: 0.3131 - learning_rate: 1.9531e-06\n",
      "Epoch 33/300\n",
      "28/28 - 1s - 19ms/step - loss: 0.1341 - mae: 0.1546 - val_loss: 0.9963 - val_mae: 0.3129 - learning_rate: 1.9531e-06\n",
      "Epoch 34/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1375 - mae: 0.1572 - val_loss: 1.0053 - val_mae: 0.3139 - learning_rate: 1.9531e-06\n",
      "Epoch 35/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1355 - mae: 0.1532 - val_loss: 0.9944 - val_mae: 0.3121 - learning_rate: 1.9531e-06\n",
      "Epoch 36/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1330 - mae: 0.1534 - val_loss: 0.9847 - val_mae: 0.3110 - learning_rate: 1.9531e-06\n",
      "Epoch 37/300\n",
      "28/28 - 1s - 18ms/step - loss: 0.1338 - mae: 0.1544 - val_loss: 0.9815 - val_mae: 0.3103 - learning_rate: 1.9531e-06\n",
      "Epoch 38/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1349 - mae: 0.1541 - val_loss: 0.9721 - val_mae: 0.3089 - learning_rate: 1.9531e-06\n",
      "Epoch 39/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1295 - mae: 0.1486 - val_loss: 0.9732 - val_mae: 0.3088 - learning_rate: 1.9531e-06\n",
      "Epoch 40/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1368 - mae: 0.1548 - val_loss: 0.9731 - val_mae: 0.3088 - learning_rate: 1.9531e-06\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "28/28 - 0s - 16ms/step - loss: 0.1421 - mae: 0.1548 - val_loss: 0.9722 - val_mae: 0.3084 - learning_rate: 1.9531e-06\n",
      "Epoch 42/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1299 - mae: 0.1489 - val_loss: 0.9770 - val_mae: 0.3091 - learning_rate: 9.7656e-07\n",
      "Epoch 43/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1315 - mae: 0.1497 - val_loss: 0.9740 - val_mae: 0.3087 - learning_rate: 9.7656e-07\n",
      "Epoch 44/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1448 - mae: 0.1551 - val_loss: 0.9747 - val_mae: 0.3086 - learning_rate: 9.7656e-07\n",
      "Epoch 45/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1307 - mae: 0.1474 - val_loss: 0.9803 - val_mae: 0.3092 - learning_rate: 9.7656e-07\n",
      "Epoch 46/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1296 - mae: 0.1499 - val_loss: 0.9820 - val_mae: 0.3096 - learning_rate: 9.7656e-07\n",
      "Epoch 47/300\n",
      "28/28 - 0s - 17ms/step - loss: 0.1330 - mae: 0.1504 - val_loss: 0.9821 - val_mae: 0.3093 - learning_rate: 9.7656e-07\n",
      "Epoch 48/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1352 - mae: 0.1517 - val_loss: 0.9832 - val_mae: 0.3094 - learning_rate: 9.7656e-07\n",
      "Epoch 49/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1286 - mae: 0.1498 - val_loss: 0.9839 - val_mae: 0.3094 - learning_rate: 9.7656e-07\n",
      "Epoch 50/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1317 - mae: 0.1511 - val_loss: 0.9837 - val_mae: 0.3093 - learning_rate: 9.7656e-07\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "28/28 - 0s - 16ms/step - loss: 0.1300 - mae: 0.1505 - val_loss: 0.9806 - val_mae: 0.3083 - learning_rate: 9.7656e-07\n",
      "Epoch 52/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1400 - mae: 0.1526 - val_loss: 0.9823 - val_mae: 0.3084 - learning_rate: 4.8828e-07\n",
      "Epoch 53/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1303 - mae: 0.1477 - val_loss: 0.9836 - val_mae: 0.3084 - learning_rate: 4.8828e-07\n",
      "Epoch 54/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1300 - mae: 0.1475 - val_loss: 0.9824 - val_mae: 0.3082 - learning_rate: 4.8828e-07\n",
      "Epoch 55/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1302 - mae: 0.1478 - val_loss: 0.9838 - val_mae: 0.3086 - learning_rate: 4.8828e-07\n",
      "Epoch 56/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1282 - mae: 0.1476 - val_loss: 0.9841 - val_mae: 0.3087 - learning_rate: 4.8828e-07\n",
      "Epoch 57/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1314 - mae: 0.1500 - val_loss: 0.9835 - val_mae: 0.3085 - learning_rate: 4.8828e-07\n",
      "Epoch 58/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1309 - mae: 0.1513 - val_loss: 0.9823 - val_mae: 0.3080 - learning_rate: 4.8828e-07\n",
      "Epoch 59/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1315 - mae: 0.1492 - val_loss: 0.9844 - val_mae: 0.3083 - learning_rate: 4.8828e-07\n",
      "Epoch 60/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1291 - mae: 0.1497 - val_loss: 0.9869 - val_mae: 0.3085 - learning_rate: 4.8828e-07\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "28/28 - 0s - 16ms/step - loss: 0.1335 - mae: 0.1485 - val_loss: 0.9867 - val_mae: 0.3089 - learning_rate: 4.8828e-07\n",
      "Epoch 62/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1324 - mae: 0.1467 - val_loss: 0.9856 - val_mae: 0.3085 - learning_rate: 2.4414e-07\n",
      "Epoch 63/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1274 - mae: 0.1484 - val_loss: 0.9850 - val_mae: 0.3082 - learning_rate: 2.4414e-07\n",
      "Epoch 64/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1281 - mae: 0.1460 - val_loss: 0.9858 - val_mae: 0.3083 - learning_rate: 2.4414e-07\n",
      "Epoch 65/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1270 - mae: 0.1454 - val_loss: 0.9847 - val_mae: 0.3082 - learning_rate: 2.4414e-07\n",
      "Epoch 66/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1307 - mae: 0.1503 - val_loss: 0.9827 - val_mae: 0.3081 - learning_rate: 2.4414e-07\n",
      "Epoch 67/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1275 - mae: 0.1482 - val_loss: 0.9827 - val_mae: 0.3079 - learning_rate: 2.4414e-07\n",
      "Epoch 68/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1271 - mae: 0.1456 - val_loss: 0.9819 - val_mae: 0.3076 - learning_rate: 2.4414e-07\n",
      "Epoch 69/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1320 - mae: 0.1489 - val_loss: 0.9828 - val_mae: 0.3076 - learning_rate: 2.4414e-07\n",
      "Epoch 70/300\n",
      "28/28 - 0s - 16ms/step - loss: 0.1331 - mae: 0.1499 - val_loss: 0.9818 - val_mae: 0.3075 - learning_rate: 2.4414e-07\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "28/28 - 0s - 16ms/step - loss: 0.1383 - mae: 0.1538 - val_loss: 0.9810 - val_mae: 0.3074 - learning_rate: 2.4414e-07\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Fold Validation MAE: 0.3166\n",
      "\n",
      "Training on 1304 samples, validating on 433 samples.\n",
      "Epoch 1/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3759 - mae: 0.2577 - val_loss: 0.5622 - val_mae: 0.2625 - learning_rate: 1.2207e-07\n",
      "Epoch 2/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3775 - mae: 0.2622 - val_loss: 0.5483 - val_mae: 0.2626 - learning_rate: 1.2207e-07\n",
      "Epoch 3/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3785 - mae: 0.2613 - val_loss: 0.5386 - val_mae: 0.2627 - learning_rate: 1.2207e-07\n",
      "Epoch 4/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3906 - mae: 0.2625 - val_loss: 0.5313 - val_mae: 0.2624 - learning_rate: 1.2207e-07\n",
      "Epoch 5/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3711 - mae: 0.2596 - val_loss: 0.5268 - val_mae: 0.2621 - learning_rate: 1.2207e-07\n",
      "Epoch 6/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3870 - mae: 0.2613 - val_loss: 0.5236 - val_mae: 0.2624 - learning_rate: 1.2207e-07\n",
      "Epoch 7/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3865 - mae: 0.2620 - val_loss: 0.5206 - val_mae: 0.2623 - learning_rate: 1.2207e-07\n",
      "Epoch 8/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3713 - mae: 0.2559 - val_loss: 0.5188 - val_mae: 0.2620 - learning_rate: 1.2207e-07\n",
      "Epoch 9/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3753 - mae: 0.2581 - val_loss: 0.5173 - val_mae: 0.2617 - learning_rate: 1.2207e-07\n",
      "Epoch 10/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3744 - mae: 0.2588 - val_loss: 0.5160 - val_mae: 0.2616 - learning_rate: 1.2207e-07\n",
      "Epoch 11/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3735 - mae: 0.2617 - val_loss: 0.5153 - val_mae: 0.2616 - learning_rate: 1.2207e-07\n",
      "Epoch 12/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3812 - mae: 0.2609 - val_loss: 0.5146 - val_mae: 0.2617 - learning_rate: 1.2207e-07\n",
      "Epoch 13/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3769 - mae: 0.2608 - val_loss: 0.5140 - val_mae: 0.2616 - learning_rate: 1.2207e-07\n",
      "Epoch 14/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3800 - mae: 0.2616 - val_loss: 0.5133 - val_mae: 0.2618 - learning_rate: 1.2207e-07\n",
      "Epoch 15/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3664 - mae: 0.2564 - val_loss: 0.5131 - val_mae: 0.2618 - learning_rate: 1.2207e-07\n",
      "Epoch 16/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3601 - mae: 0.2550 - val_loss: 0.5120 - val_mae: 0.2615 - learning_rate: 1.2207e-07\n",
      "Epoch 17/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3553 - mae: 0.2550 - val_loss: 0.5111 - val_mae: 0.2614 - learning_rate: 1.2207e-07\n",
      "Epoch 18/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3816 - mae: 0.2610 - val_loss: 0.5110 - val_mae: 0.2614 - learning_rate: 1.2207e-07\n",
      "Epoch 19/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3582 - mae: 0.2539 - val_loss: 0.5109 - val_mae: 0.2615 - learning_rate: 1.2207e-07\n",
      "Epoch 20/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3816 - mae: 0.2603 - val_loss: 0.5101 - val_mae: 0.2613 - learning_rate: 1.2207e-07\n",
      "Epoch 21/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3705 - mae: 0.2546 - val_loss: 0.5094 - val_mae: 0.2612 - learning_rate: 1.2207e-07\n",
      "Epoch 22/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3633 - mae: 0.2559 - val_loss: 0.5087 - val_mae: 0.2610 - learning_rate: 1.2207e-07\n",
      "Epoch 23/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3664 - mae: 0.2533 - val_loss: 0.5086 - val_mae: 0.2608 - learning_rate: 1.2207e-07\n",
      "Epoch 24/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3531 - mae: 0.2532 - val_loss: 0.5086 - val_mae: 0.2609 - learning_rate: 1.2207e-07\n",
      "Epoch 25/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3511 - mae: 0.2507 - val_loss: 0.5079 - val_mae: 0.2608 - learning_rate: 1.2207e-07\n",
      "Epoch 26/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3491 - mae: 0.2506 - val_loss: 0.5076 - val_mae: 0.2606 - learning_rate: 1.2207e-07\n",
      "Epoch 27/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3630 - mae: 0.2522 - val_loss: 0.5067 - val_mae: 0.2604 - learning_rate: 1.2207e-07\n",
      "Epoch 28/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3532 - mae: 0.2534 - val_loss: 0.5064 - val_mae: 0.2606 - learning_rate: 1.2207e-07\n",
      "Epoch 29/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3648 - mae: 0.2507 - val_loss: 0.5059 - val_mae: 0.2605 - learning_rate: 1.2207e-07\n",
      "Epoch 30/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3678 - mae: 0.2554 - val_loss: 0.5054 - val_mae: 0.2605 - learning_rate: 1.2207e-07\n",
      "Epoch 31/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3553 - mae: 0.2498 - val_loss: 0.5056 - val_mae: 0.2603 - learning_rate: 1.2207e-07\n",
      "Epoch 32/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3407 - mae: 0.2491 - val_loss: 0.5049 - val_mae: 0.2602 - learning_rate: 1.2207e-07\n",
      "Epoch 33/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3569 - mae: 0.2540 - val_loss: 0.5047 - val_mae: 0.2605 - learning_rate: 1.2207e-07\n",
      "Epoch 34/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3486 - mae: 0.2506 - val_loss: 0.5042 - val_mae: 0.2602 - learning_rate: 1.2207e-07\n",
      "Epoch 35/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3584 - mae: 0.2521 - val_loss: 0.5038 - val_mae: 0.2600 - learning_rate: 1.2207e-07\n",
      "Epoch 36/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3610 - mae: 0.2514 - val_loss: 0.5039 - val_mae: 0.2602 - learning_rate: 1.2207e-07\n",
      "Epoch 37/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3572 - mae: 0.2505 - val_loss: 0.5028 - val_mae: 0.2596 - learning_rate: 1.2207e-07\n",
      "Epoch 38/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3532 - mae: 0.2493 - val_loss: 0.5016 - val_mae: 0.2594 - learning_rate: 1.2207e-07\n",
      "Epoch 39/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3420 - mae: 0.2476 - val_loss: 0.5007 - val_mae: 0.2595 - learning_rate: 1.2207e-07\n",
      "Epoch 40/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3622 - mae: 0.2525 - val_loss: 0.5005 - val_mae: 0.2593 - learning_rate: 1.2207e-07\n",
      "Epoch 41/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3591 - mae: 0.2483 - val_loss: 0.5008 - val_mae: 0.2595 - learning_rate: 1.2207e-07\n",
      "Epoch 42/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3319 - mae: 0.2472 - val_loss: 0.5002 - val_mae: 0.2594 - learning_rate: 1.2207e-07\n",
      "Epoch 43/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3530 - mae: 0.2513 - val_loss: 0.4995 - val_mae: 0.2589 - learning_rate: 1.2207e-07\n",
      "Epoch 44/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3517 - mae: 0.2490 - val_loss: 0.4998 - val_mae: 0.2586 - learning_rate: 1.2207e-07\n",
      "Epoch 45/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3487 - mae: 0.2482 - val_loss: 0.4997 - val_mae: 0.2586 - learning_rate: 1.2207e-07\n",
      "Epoch 46/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3629 - mae: 0.2538 - val_loss: 0.4995 - val_mae: 0.2591 - learning_rate: 1.2207e-07\n",
      "Epoch 47/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3436 - mae: 0.2498 - val_loss: 0.4989 - val_mae: 0.2591 - learning_rate: 1.2207e-07\n",
      "Epoch 48/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3465 - mae: 0.2477 - val_loss: 0.4988 - val_mae: 0.2588 - learning_rate: 1.2207e-07\n",
      "Epoch 49/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3506 - mae: 0.2488 - val_loss: 0.4983 - val_mae: 0.2588 - learning_rate: 1.2207e-07\n",
      "Epoch 50/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3517 - mae: 0.2499 - val_loss: 0.4986 - val_mae: 0.2587 - learning_rate: 1.2207e-07\n",
      "Epoch 51/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3493 - mae: 0.2444 - val_loss: 0.4978 - val_mae: 0.2587 - learning_rate: 1.2207e-07\n",
      "Epoch 52/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3513 - mae: 0.2499 - val_loss: 0.4982 - val_mae: 0.2583 - learning_rate: 1.2207e-07\n",
      "Epoch 53/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3403 - mae: 0.2476 - val_loss: 0.4977 - val_mae: 0.2582 - learning_rate: 1.2207e-07\n",
      "Epoch 54/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3411 - mae: 0.2450 - val_loss: 0.4969 - val_mae: 0.2584 - learning_rate: 1.2207e-07\n",
      "Epoch 55/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3438 - mae: 0.2508 - val_loss: 0.4960 - val_mae: 0.2580 - learning_rate: 1.2207e-07\n",
      "Epoch 56/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3506 - mae: 0.2505 - val_loss: 0.4958 - val_mae: 0.2578 - learning_rate: 1.2207e-07\n",
      "Epoch 57/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3454 - mae: 0.2477 - val_loss: 0.4960 - val_mae: 0.2577 - learning_rate: 1.2207e-07\n",
      "Epoch 58/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3289 - mae: 0.2424 - val_loss: 0.4955 - val_mae: 0.2576 - learning_rate: 1.2207e-07\n",
      "Epoch 59/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3502 - mae: 0.2476 - val_loss: 0.4952 - val_mae: 0.2578 - learning_rate: 1.2207e-07\n",
      "Epoch 60/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3368 - mae: 0.2433 - val_loss: 0.4958 - val_mae: 0.2580 - learning_rate: 1.2207e-07\n",
      "Epoch 61/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3282 - mae: 0.2420 - val_loss: 0.4955 - val_mae: 0.2577 - learning_rate: 1.2207e-07\n",
      "Epoch 62/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3443 - mae: 0.2442 - val_loss: 0.4948 - val_mae: 0.2579 - learning_rate: 1.2207e-07\n",
      "Epoch 63/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3371 - mae: 0.2443 - val_loss: 0.4948 - val_mae: 0.2579 - learning_rate: 1.2207e-07\n",
      "Epoch 64/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3366 - mae: 0.2458 - val_loss: 0.4947 - val_mae: 0.2582 - learning_rate: 1.2207e-07\n",
      "Epoch 65/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3384 - mae: 0.2456 - val_loss: 0.4944 - val_mae: 0.2578 - learning_rate: 1.2207e-07\n",
      "Epoch 66/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3280 - mae: 0.2429 - val_loss: 0.4939 - val_mae: 0.2574 - learning_rate: 1.2207e-07\n",
      "Epoch 67/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3382 - mae: 0.2460 - val_loss: 0.4933 - val_mae: 0.2573 - learning_rate: 1.2207e-07\n",
      "Epoch 68/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3349 - mae: 0.2430 - val_loss: 0.4931 - val_mae: 0.2573 - learning_rate: 1.2207e-07\n",
      "Epoch 69/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3322 - mae: 0.2416 - val_loss: 0.4929 - val_mae: 0.2574 - learning_rate: 1.2207e-07\n",
      "Epoch 70/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3347 - mae: 0.2414 - val_loss: 0.4932 - val_mae: 0.2574 - learning_rate: 1.2207e-07\n",
      "Epoch 71/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3207 - mae: 0.2401 - val_loss: 0.4927 - val_mae: 0.2572 - learning_rate: 1.2207e-07\n",
      "Epoch 72/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3428 - mae: 0.2437 - val_loss: 0.4930 - val_mae: 0.2574 - learning_rate: 1.2207e-07\n",
      "Epoch 73/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3376 - mae: 0.2467 - val_loss: 0.4924 - val_mae: 0.2571 - learning_rate: 1.2207e-07\n",
      "Epoch 74/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3359 - mae: 0.2450 - val_loss: 0.4919 - val_mae: 0.2569 - learning_rate: 1.2207e-07\n",
      "Epoch 75/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3224 - mae: 0.2387 - val_loss: 0.4913 - val_mae: 0.2566 - learning_rate: 1.2207e-07\n",
      "Epoch 76/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3395 - mae: 0.2447 - val_loss: 0.4910 - val_mae: 0.2568 - learning_rate: 1.2207e-07\n",
      "Epoch 77/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3253 - mae: 0.2428 - val_loss: 0.4900 - val_mae: 0.2565 - learning_rate: 1.2207e-07\n",
      "Epoch 78/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3384 - mae: 0.2401 - val_loss: 0.4900 - val_mae: 0.2565 - learning_rate: 1.2207e-07\n",
      "Epoch 79/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3220 - mae: 0.2424 - val_loss: 0.4896 - val_mae: 0.2563 - learning_rate: 1.2207e-07\n",
      "Epoch 80/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3269 - mae: 0.2396 - val_loss: 0.4892 - val_mae: 0.2564 - learning_rate: 1.2207e-07\n",
      "Epoch 81/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3434 - mae: 0.2448 - val_loss: 0.4887 - val_mae: 0.2560 - learning_rate: 1.2207e-07\n",
      "Epoch 82/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3353 - mae: 0.2419 - val_loss: 0.4884 - val_mae: 0.2559 - learning_rate: 1.2207e-07\n",
      "Epoch 83/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3270 - mae: 0.2425 - val_loss: 0.4877 - val_mae: 0.2559 - learning_rate: 1.2207e-07\n",
      "Epoch 84/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3198 - mae: 0.2373 - val_loss: 0.4877 - val_mae: 0.2558 - learning_rate: 1.2207e-07\n",
      "Epoch 85/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3231 - mae: 0.2390 - val_loss: 0.4876 - val_mae: 0.2559 - learning_rate: 1.2207e-07\n",
      "Epoch 86/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3290 - mae: 0.2411 - val_loss: 0.4874 - val_mae: 0.2562 - learning_rate: 1.2207e-07\n",
      "Epoch 87/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3302 - mae: 0.2425 - val_loss: 0.4877 - val_mae: 0.2565 - learning_rate: 1.2207e-07\n",
      "Epoch 88/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3225 - mae: 0.2418 - val_loss: 0.4873 - val_mae: 0.2559 - learning_rate: 1.2207e-07\n",
      "Epoch 89/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3273 - mae: 0.2410 - val_loss: 0.4873 - val_mae: 0.2559 - learning_rate: 1.2207e-07\n",
      "Epoch 90/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3250 - mae: 0.2375 - val_loss: 0.4865 - val_mae: 0.2556 - learning_rate: 1.2207e-07\n",
      "Epoch 91/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3377 - mae: 0.2438 - val_loss: 0.4866 - val_mae: 0.2558 - learning_rate: 1.2207e-07\n",
      "Epoch 92/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3391 - mae: 0.2429 - val_loss: 0.4861 - val_mae: 0.2555 - learning_rate: 1.2207e-07\n",
      "Epoch 93/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3387 - mae: 0.2409 - val_loss: 0.4859 - val_mae: 0.2555 - learning_rate: 1.2207e-07\n",
      "Epoch 94/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3244 - mae: 0.2397 - val_loss: 0.4863 - val_mae: 0.2556 - learning_rate: 1.2207e-07\n",
      "Epoch 95/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3196 - mae: 0.2420 - val_loss: 0.4857 - val_mae: 0.2553 - learning_rate: 1.2207e-07\n",
      "Epoch 96/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3194 - mae: 0.2370 - val_loss: 0.4856 - val_mae: 0.2553 - learning_rate: 1.2207e-07\n",
      "Epoch 97/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3187 - mae: 0.2409 - val_loss: 0.4857 - val_mae: 0.2552 - learning_rate: 1.2207e-07\n",
      "Epoch 98/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3225 - mae: 0.2389 - val_loss: 0.4854 - val_mae: 0.2552 - learning_rate: 1.2207e-07\n",
      "Epoch 99/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3121 - mae: 0.2392 - val_loss: 0.4854 - val_mae: 0.2556 - learning_rate: 1.2207e-07\n",
      "Epoch 100/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3316 - mae: 0.2416 - val_loss: 0.4848 - val_mae: 0.2555 - learning_rate: 1.2207e-07\n",
      "Epoch 101/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3144 - mae: 0.2371 - val_loss: 0.4848 - val_mae: 0.2555 - learning_rate: 1.2207e-07\n",
      "Epoch 102/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3239 - mae: 0.2373 - val_loss: 0.4847 - val_mae: 0.2555 - learning_rate: 1.2207e-07\n",
      "Epoch 103/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3046 - mae: 0.2373 - val_loss: 0.4845 - val_mae: 0.2551 - learning_rate: 1.2207e-07\n",
      "Epoch 104/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3204 - mae: 0.2363 - val_loss: 0.4842 - val_mae: 0.2551 - learning_rate: 1.2207e-07\n",
      "Epoch 105/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3121 - mae: 0.2358 - val_loss: 0.4837 - val_mae: 0.2549 - learning_rate: 1.2207e-07\n",
      "Epoch 106/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3301 - mae: 0.2413 - val_loss: 0.4832 - val_mae: 0.2547 - learning_rate: 1.2207e-07\n",
      "Epoch 107/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2982 - mae: 0.2333 - val_loss: 0.4829 - val_mae: 0.2545 - learning_rate: 1.2207e-07\n",
      "Epoch 108/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3285 - mae: 0.2385 - val_loss: 0.4830 - val_mae: 0.2542 - learning_rate: 1.2207e-07\n",
      "Epoch 109/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3148 - mae: 0.2353 - val_loss: 0.4826 - val_mae: 0.2544 - learning_rate: 1.2207e-07\n",
      "Epoch 110/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3150 - mae: 0.2378 - val_loss: 0.4827 - val_mae: 0.2546 - learning_rate: 1.2207e-07\n",
      "Epoch 111/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3123 - mae: 0.2369 - val_loss: 0.4821 - val_mae: 0.2544 - learning_rate: 1.2207e-07\n",
      "Epoch 112/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3112 - mae: 0.2363 - val_loss: 0.4816 - val_mae: 0.2538 - learning_rate: 1.2207e-07\n",
      "Epoch 113/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3006 - mae: 0.2341 - val_loss: 0.4815 - val_mae: 0.2538 - learning_rate: 1.2207e-07\n",
      "Epoch 114/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3252 - mae: 0.2397 - val_loss: 0.4818 - val_mae: 0.2538 - learning_rate: 1.2207e-07\n",
      "Epoch 115/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3344 - mae: 0.2391 - val_loss: 0.4812 - val_mae: 0.2539 - learning_rate: 1.2207e-07\n",
      "Epoch 116/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3054 - mae: 0.2339 - val_loss: 0.4814 - val_mae: 0.2540 - learning_rate: 1.2207e-07\n",
      "Epoch 117/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3260 - mae: 0.2356 - val_loss: 0.4811 - val_mae: 0.2539 - learning_rate: 1.2207e-07\n",
      "Epoch 118/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3043 - mae: 0.2342 - val_loss: 0.4806 - val_mae: 0.2537 - learning_rate: 1.2207e-07\n",
      "Epoch 119/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3127 - mae: 0.2363 - val_loss: 0.4799 - val_mae: 0.2535 - learning_rate: 1.2207e-07\n",
      "Epoch 120/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3110 - mae: 0.2359 - val_loss: 0.4795 - val_mae: 0.2532 - learning_rate: 1.2207e-07\n",
      "Epoch 121/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2927 - mae: 0.2325 - val_loss: 0.4796 - val_mae: 0.2535 - learning_rate: 1.2207e-07\n",
      "Epoch 122/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3068 - mae: 0.2368 - val_loss: 0.4803 - val_mae: 0.2534 - learning_rate: 1.2207e-07\n",
      "Epoch 123/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3147 - mae: 0.2400 - val_loss: 0.4803 - val_mae: 0.2536 - learning_rate: 1.2207e-07\n",
      "Epoch 124/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3196 - mae: 0.2371 - val_loss: 0.4804 - val_mae: 0.2538 - learning_rate: 1.2207e-07\n",
      "Epoch 125/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3154 - mae: 0.2346 - val_loss: 0.4797 - val_mae: 0.2535 - learning_rate: 1.2207e-07\n",
      "Epoch 126/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3176 - mae: 0.2382 - val_loss: 0.4800 - val_mae: 0.2537 - learning_rate: 1.2207e-07\n",
      "Epoch 127/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3106 - mae: 0.2358 - val_loss: 0.4800 - val_mae: 0.2535 - learning_rate: 1.2207e-07\n",
      "Epoch 128/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3093 - mae: 0.2322 - val_loss: 0.4791 - val_mae: 0.2533 - learning_rate: 1.2207e-07\n",
      "Epoch 129/300\n",
      "41/41 - 1s - 18ms/step - loss: 0.3011 - mae: 0.2330 - val_loss: 0.4787 - val_mae: 0.2531 - learning_rate: 1.2207e-07\n",
      "Epoch 130/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3144 - mae: 0.2344 - val_loss: 0.4781 - val_mae: 0.2529 - learning_rate: 1.2207e-07\n",
      "Epoch 131/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3169 - mae: 0.2338 - val_loss: 0.4780 - val_mae: 0.2528 - learning_rate: 1.2207e-07\n",
      "Epoch 132/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3107 - mae: 0.2330 - val_loss: 0.4776 - val_mae: 0.2528 - learning_rate: 1.2207e-07\n",
      "Epoch 133/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3104 - mae: 0.2376 - val_loss: 0.4780 - val_mae: 0.2529 - learning_rate: 1.2207e-07\n",
      "Epoch 134/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3045 - mae: 0.2354 - val_loss: 0.4785 - val_mae: 0.2533 - learning_rate: 1.2207e-07\n",
      "Epoch 135/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3013 - mae: 0.2307 - val_loss: 0.4788 - val_mae: 0.2535 - learning_rate: 1.2207e-07\n",
      "Epoch 136/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2936 - mae: 0.2301 - val_loss: 0.4789 - val_mae: 0.2530 - learning_rate: 1.2207e-07\n",
      "Epoch 137/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3013 - mae: 0.2317 - val_loss: 0.4794 - val_mae: 0.2527 - learning_rate: 1.2207e-07\n",
      "Epoch 138/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3061 - mae: 0.2364 - val_loss: 0.4785 - val_mae: 0.2523 - learning_rate: 1.2207e-07\n",
      "Epoch 139/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3141 - mae: 0.2327 - val_loss: 0.4785 - val_mae: 0.2525 - learning_rate: 1.2207e-07\n",
      "Epoch 140/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3211 - mae: 0.2367 - val_loss: 0.4783 - val_mae: 0.2525 - learning_rate: 1.2207e-07\n",
      "Epoch 141/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3009 - mae: 0.2328 - val_loss: 0.4780 - val_mae: 0.2527 - learning_rate: 1.2207e-07\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "41/41 - 1s - 15ms/step - loss: 0.3278 - mae: 0.2360 - val_loss: 0.4778 - val_mae: 0.2529 - learning_rate: 1.2207e-07\n",
      "Epoch 143/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2883 - mae: 0.2294 - val_loss: 0.4776 - val_mae: 0.2524 - learning_rate: 6.1035e-08\n",
      "Epoch 144/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3223 - mae: 0.2361 - val_loss: 0.4776 - val_mae: 0.2528 - learning_rate: 6.1035e-08\n",
      "Epoch 145/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3063 - mae: 0.2340 - val_loss: 0.4776 - val_mae: 0.2525 - learning_rate: 6.1035e-08\n",
      "Epoch 146/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3059 - mae: 0.2348 - val_loss: 0.4775 - val_mae: 0.2524 - learning_rate: 6.1035e-08\n",
      "Epoch 147/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3014 - mae: 0.2302 - val_loss: 0.4777 - val_mae: 0.2525 - learning_rate: 6.1035e-08\n",
      "Epoch 148/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3009 - mae: 0.2297 - val_loss: 0.4775 - val_mae: 0.2524 - learning_rate: 6.1035e-08\n",
      "Epoch 149/300\n",
      "41/41 - 1s - 18ms/step - loss: 0.2973 - mae: 0.2299 - val_loss: 0.4771 - val_mae: 0.2521 - learning_rate: 6.1035e-08\n",
      "Epoch 150/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3135 - mae: 0.2346 - val_loss: 0.4775 - val_mae: 0.2522 - learning_rate: 6.1035e-08\n",
      "Epoch 151/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2991 - mae: 0.2339 - val_loss: 0.4767 - val_mae: 0.2523 - learning_rate: 6.1035e-08\n",
      "Epoch 152/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3147 - mae: 0.2347 - val_loss: 0.4761 - val_mae: 0.2520 - learning_rate: 6.1035e-08\n",
      "Epoch 153/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3030 - mae: 0.2319 - val_loss: 0.4766 - val_mae: 0.2524 - learning_rate: 6.1035e-08\n",
      "Epoch 154/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2984 - mae: 0.2312 - val_loss: 0.4766 - val_mae: 0.2523 - learning_rate: 6.1035e-08\n",
      "Epoch 155/300\n",
      "41/41 - 1s - 19ms/step - loss: 0.2994 - mae: 0.2303 - val_loss: 0.4767 - val_mae: 0.2522 - learning_rate: 6.1035e-08\n",
      "Epoch 156/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3091 - mae: 0.2327 - val_loss: 0.4764 - val_mae: 0.2524 - learning_rate: 6.1035e-08\n",
      "Epoch 157/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3171 - mae: 0.2333 - val_loss: 0.4765 - val_mae: 0.2523 - learning_rate: 6.1035e-08\n",
      "Epoch 158/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2979 - mae: 0.2303 - val_loss: 0.4764 - val_mae: 0.2526 - learning_rate: 6.1035e-08\n",
      "Epoch 159/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3000 - mae: 0.2313 - val_loss: 0.4761 - val_mae: 0.2522 - learning_rate: 6.1035e-08\n",
      "Epoch 160/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2973 - mae: 0.2356 - val_loss: 0.4759 - val_mae: 0.2524 - learning_rate: 6.1035e-08\n",
      "Epoch 161/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3063 - mae: 0.2319 - val_loss: 0.4760 - val_mae: 0.2522 - learning_rate: 6.1035e-08\n",
      "Epoch 162/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3199 - mae: 0.2329 - val_loss: 0.4757 - val_mae: 0.2519 - learning_rate: 6.1035e-08\n",
      "Epoch 163/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2814 - mae: 0.2290 - val_loss: 0.4763 - val_mae: 0.2516 - learning_rate: 6.1035e-08\n",
      "Epoch 164/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3018 - mae: 0.2337 - val_loss: 0.4766 - val_mae: 0.2517 - learning_rate: 6.1035e-08\n",
      "Epoch 165/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3113 - mae: 0.2331 - val_loss: 0.4764 - val_mae: 0.2518 - learning_rate: 6.1035e-08\n",
      "Epoch 166/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3158 - mae: 0.2349 - val_loss: 0.4758 - val_mae: 0.2516 - learning_rate: 6.1035e-08\n",
      "Epoch 167/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.3076 - mae: 0.2315 - val_loss: 0.4756 - val_mae: 0.2519 - learning_rate: 6.1035e-08\n",
      "Epoch 168/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3005 - mae: 0.2305 - val_loss: 0.4753 - val_mae: 0.2521 - learning_rate: 6.1035e-08\n",
      "Epoch 169/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3108 - mae: 0.2323 - val_loss: 0.4754 - val_mae: 0.2519 - learning_rate: 6.1035e-08\n",
      "Epoch 170/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2899 - mae: 0.2291 - val_loss: 0.4750 - val_mae: 0.2517 - learning_rate: 6.1035e-08\n",
      "Epoch 171/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2992 - mae: 0.2279 - val_loss: 0.4755 - val_mae: 0.2520 - learning_rate: 6.1035e-08\n",
      "Epoch 172/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3053 - mae: 0.2302 - val_loss: 0.4756 - val_mae: 0.2516 - learning_rate: 6.1035e-08\n",
      "Epoch 173/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3028 - mae: 0.2304 - val_loss: 0.4753 - val_mae: 0.2514 - learning_rate: 6.1035e-08\n",
      "Epoch 174/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2949 - mae: 0.2324 - val_loss: 0.4746 - val_mae: 0.2514 - learning_rate: 6.1035e-08\n",
      "Epoch 175/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3079 - mae: 0.2286 - val_loss: 0.4751 - val_mae: 0.2515 - learning_rate: 6.1035e-08\n",
      "Epoch 176/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3012 - mae: 0.2286 - val_loss: 0.4752 - val_mae: 0.2512 - learning_rate: 6.1035e-08\n",
      "Epoch 177/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2942 - mae: 0.2291 - val_loss: 0.4751 - val_mae: 0.2515 - learning_rate: 6.1035e-08\n",
      "Epoch 178/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3174 - mae: 0.2342 - val_loss: 0.4753 - val_mae: 0.2516 - learning_rate: 6.1035e-08\n",
      "Epoch 179/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3037 - mae: 0.2290 - val_loss: 0.4750 - val_mae: 0.2515 - learning_rate: 6.1035e-08\n",
      "Epoch 180/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3033 - mae: 0.2295 - val_loss: 0.4743 - val_mae: 0.2512 - learning_rate: 6.1035e-08\n",
      "Epoch 181/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2834 - mae: 0.2255 - val_loss: 0.4743 - val_mae: 0.2513 - learning_rate: 6.1035e-08\n",
      "Epoch 182/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3074 - mae: 0.2320 - val_loss: 0.4736 - val_mae: 0.2510 - learning_rate: 6.1035e-08\n",
      "Epoch 183/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3009 - mae: 0.2298 - val_loss: 0.4743 - val_mae: 0.2513 - learning_rate: 6.1035e-08\n",
      "Epoch 184/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2896 - mae: 0.2284 - val_loss: 0.4745 - val_mae: 0.2514 - learning_rate: 6.1035e-08\n",
      "Epoch 185/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2890 - mae: 0.2273 - val_loss: 0.4747 - val_mae: 0.2514 - learning_rate: 6.1035e-08\n",
      "Epoch 186/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3093 - mae: 0.2296 - val_loss: 0.4739 - val_mae: 0.2510 - learning_rate: 6.1035e-08\n",
      "Epoch 187/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3105 - mae: 0.2301 - val_loss: 0.4740 - val_mae: 0.2511 - learning_rate: 6.1035e-08\n",
      "Epoch 188/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2916 - mae: 0.2270 - val_loss: 0.4732 - val_mae: 0.2509 - learning_rate: 6.1035e-08\n",
      "Epoch 189/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3047 - mae: 0.2285 - val_loss: 0.4743 - val_mae: 0.2511 - learning_rate: 6.1035e-08\n",
      "Epoch 190/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2996 - mae: 0.2301 - val_loss: 0.4738 - val_mae: 0.2511 - learning_rate: 6.1035e-08\n",
      "Epoch 191/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2864 - mae: 0.2254 - val_loss: 0.4735 - val_mae: 0.2510 - learning_rate: 6.1035e-08\n",
      "Epoch 192/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3113 - mae: 0.2308 - val_loss: 0.4732 - val_mae: 0.2511 - learning_rate: 6.1035e-08\n",
      "Epoch 193/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3028 - mae: 0.2286 - val_loss: 0.4723 - val_mae: 0.2509 - learning_rate: 6.1035e-08\n",
      "Epoch 194/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2820 - mae: 0.2231 - val_loss: 0.4728 - val_mae: 0.2508 - learning_rate: 6.1035e-08\n",
      "Epoch 195/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3006 - mae: 0.2293 - val_loss: 0.4724 - val_mae: 0.2506 - learning_rate: 6.1035e-08\n",
      "Epoch 196/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2951 - mae: 0.2291 - val_loss: 0.4726 - val_mae: 0.2504 - learning_rate: 6.1035e-08\n",
      "Epoch 197/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2973 - mae: 0.2305 - val_loss: 0.4721 - val_mae: 0.2503 - learning_rate: 6.1035e-08\n",
      "Epoch 198/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2890 - mae: 0.2278 - val_loss: 0.4721 - val_mae: 0.2505 - learning_rate: 6.1035e-08\n",
      "Epoch 199/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3107 - mae: 0.2373 - val_loss: 0.4727 - val_mae: 0.2508 - learning_rate: 6.1035e-08\n",
      "Epoch 200/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2980 - mae: 0.2304 - val_loss: 0.4726 - val_mae: 0.2507 - learning_rate: 6.1035e-08\n",
      "Epoch 201/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2890 - mae: 0.2282 - val_loss: 0.4730 - val_mae: 0.2505 - learning_rate: 6.1035e-08\n",
      "Epoch 202/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3054 - mae: 0.2308 - val_loss: 0.4720 - val_mae: 0.2503 - learning_rate: 6.1035e-08\n",
      "Epoch 203/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2883 - mae: 0.2262 - val_loss: 0.4723 - val_mae: 0.2505 - learning_rate: 6.1035e-08\n",
      "Epoch 204/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2861 - mae: 0.2264 - val_loss: 0.4723 - val_mae: 0.2502 - learning_rate: 6.1035e-08\n",
      "Epoch 205/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2983 - mae: 0.2297 - val_loss: 0.4717 - val_mae: 0.2500 - learning_rate: 6.1035e-08\n",
      "Epoch 206/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2942 - mae: 0.2298 - val_loss: 0.4715 - val_mae: 0.2501 - learning_rate: 6.1035e-08\n",
      "Epoch 207/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2958 - mae: 0.2278 - val_loss: 0.4717 - val_mae: 0.2502 - learning_rate: 6.1035e-08\n",
      "Epoch 208/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2838 - mae: 0.2267 - val_loss: 0.4720 - val_mae: 0.2503 - learning_rate: 6.1035e-08\n",
      "Epoch 209/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2921 - mae: 0.2263 - val_loss: 0.4719 - val_mae: 0.2502 - learning_rate: 6.1035e-08\n",
      "Epoch 210/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3043 - mae: 0.2293 - val_loss: 0.4711 - val_mae: 0.2500 - learning_rate: 6.1035e-08\n",
      "Epoch 211/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2973 - mae: 0.2273 - val_loss: 0.4713 - val_mae: 0.2502 - learning_rate: 6.1035e-08\n",
      "Epoch 212/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3115 - mae: 0.2289 - val_loss: 0.4707 - val_mae: 0.2499 - learning_rate: 6.1035e-08\n",
      "Epoch 213/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2876 - mae: 0.2302 - val_loss: 0.4718 - val_mae: 0.2500 - learning_rate: 6.1035e-08\n",
      "Epoch 214/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3090 - mae: 0.2350 - val_loss: 0.4713 - val_mae: 0.2499 - learning_rate: 6.1035e-08\n",
      "Epoch 215/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2973 - mae: 0.2269 - val_loss: 0.4712 - val_mae: 0.2499 - learning_rate: 6.1035e-08\n",
      "Epoch 216/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2921 - mae: 0.2297 - val_loss: 0.4718 - val_mae: 0.2501 - learning_rate: 6.1035e-08\n",
      "Epoch 217/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3023 - mae: 0.2265 - val_loss: 0.4719 - val_mae: 0.2502 - learning_rate: 6.1035e-08\n",
      "Epoch 218/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2937 - mae: 0.2295 - val_loss: 0.4709 - val_mae: 0.2503 - learning_rate: 6.1035e-08\n",
      "Epoch 219/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2914 - mae: 0.2249 - val_loss: 0.4707 - val_mae: 0.2503 - learning_rate: 6.1035e-08\n",
      "Epoch 220/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2973 - mae: 0.2275 - val_loss: 0.4708 - val_mae: 0.2499 - learning_rate: 6.1035e-08\n",
      "Epoch 221/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2956 - mae: 0.2275 - val_loss: 0.4707 - val_mae: 0.2499 - learning_rate: 6.1035e-08\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 222: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n",
      "41/41 - 1s - 15ms/step - loss: 0.2851 - mae: 0.2219 - val_loss: 0.4706 - val_mae: 0.2497 - learning_rate: 6.1035e-08\n",
      "Epoch 223/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2979 - mae: 0.2289 - val_loss: 0.4706 - val_mae: 0.2498 - learning_rate: 3.0518e-08\n",
      "Epoch 224/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2960 - mae: 0.2285 - val_loss: 0.4706 - val_mae: 0.2499 - learning_rate: 3.0518e-08\n",
      "Epoch 225/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.3016 - mae: 0.2272 - val_loss: 0.4714 - val_mae: 0.2500 - learning_rate: 3.0518e-08\n",
      "Epoch 226/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2924 - mae: 0.2233 - val_loss: 0.4713 - val_mae: 0.2499 - learning_rate: 3.0518e-08\n",
      "Epoch 227/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2820 - mae: 0.2255 - val_loss: 0.4712 - val_mae: 0.2498 - learning_rate: 3.0518e-08\n",
      "Epoch 228/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2921 - mae: 0.2267 - val_loss: 0.4716 - val_mae: 0.2501 - learning_rate: 3.0518e-08\n",
      "Epoch 229/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2912 - mae: 0.2263 - val_loss: 0.4718 - val_mae: 0.2500 - learning_rate: 3.0518e-08\n",
      "Epoch 230/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3031 - mae: 0.2316 - val_loss: 0.4715 - val_mae: 0.2499 - learning_rate: 3.0518e-08\n",
      "Epoch 231/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3043 - mae: 0.2297 - val_loss: 0.4711 - val_mae: 0.2497 - learning_rate: 3.0518e-08\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 232: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "41/41 - 1s - 16ms/step - loss: 0.2775 - mae: 0.2238 - val_loss: 0.4711 - val_mae: 0.2497 - learning_rate: 3.0518e-08\n",
      "Epoch 233/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2894 - mae: 0.2266 - val_loss: 0.4714 - val_mae: 0.2500 - learning_rate: 1.5259e-08\n",
      "Epoch 234/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2960 - mae: 0.2254 - val_loss: 0.4718 - val_mae: 0.2499 - learning_rate: 1.5259e-08\n",
      "Epoch 235/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2884 - mae: 0.2252 - val_loss: 0.4712 - val_mae: 0.2499 - learning_rate: 1.5259e-08\n",
      "Epoch 236/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2884 - mae: 0.2269 - val_loss: 0.4714 - val_mae: 0.2501 - learning_rate: 1.5259e-08\n",
      "Epoch 237/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2937 - mae: 0.2321 - val_loss: 0.4708 - val_mae: 0.2499 - learning_rate: 1.5259e-08\n",
      "Epoch 238/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2937 - mae: 0.2294 - val_loss: 0.4705 - val_mae: 0.2498 - learning_rate: 1.5259e-08\n",
      "Epoch 239/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2901 - mae: 0.2267 - val_loss: 0.4707 - val_mae: 0.2497 - learning_rate: 1.5259e-08\n",
      "Epoch 240/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2965 - mae: 0.2250 - val_loss: 0.4713 - val_mae: 0.2499 - learning_rate: 1.5259e-08\n",
      "Epoch 241/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2943 - mae: 0.2286 - val_loss: 0.4709 - val_mae: 0.2500 - learning_rate: 1.5259e-08\n",
      "Epoch 242/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2907 - mae: 0.2296 - val_loss: 0.4708 - val_mae: 0.2499 - learning_rate: 1.5259e-08\n",
      "Epoch 243/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2719 - mae: 0.2211 - val_loss: 0.4706 - val_mae: 0.2497 - learning_rate: 1.5259e-08\n",
      "Epoch 244/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2903 - mae: 0.2274 - val_loss: 0.4701 - val_mae: 0.2495 - learning_rate: 1.5259e-08\n",
      "Epoch 245/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2965 - mae: 0.2269 - val_loss: 0.4703 - val_mae: 0.2497 - learning_rate: 1.5259e-08\n",
      "Epoch 246/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2879 - mae: 0.2276 - val_loss: 0.4707 - val_mae: 0.2496 - learning_rate: 1.5259e-08\n",
      "Epoch 247/300\n",
      "41/41 - 1s - 18ms/step - loss: 0.2940 - mae: 0.2291 - val_loss: 0.4702 - val_mae: 0.2496 - learning_rate: 1.5259e-08\n",
      "Epoch 248/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2893 - mae: 0.2243 - val_loss: 0.4700 - val_mae: 0.2495 - learning_rate: 1.5259e-08\n",
      "Epoch 249/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2849 - mae: 0.2233 - val_loss: 0.4704 - val_mae: 0.2496 - learning_rate: 1.5259e-08\n",
      "Epoch 250/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2858 - mae: 0.2268 - val_loss: 0.4702 - val_mae: 0.2498 - learning_rate: 1.5259e-08\n",
      "Epoch 251/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2897 - mae: 0.2283 - val_loss: 0.4704 - val_mae: 0.2498 - learning_rate: 1.5259e-08\n",
      "Epoch 252/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3005 - mae: 0.2270 - val_loss: 0.4700 - val_mae: 0.2494 - learning_rate: 1.5259e-08\n",
      "Epoch 253/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2827 - mae: 0.2239 - val_loss: 0.4699 - val_mae: 0.2496 - learning_rate: 1.5259e-08\n",
      "Epoch 254/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2875 - mae: 0.2238 - val_loss: 0.4700 - val_mae: 0.2494 - learning_rate: 1.5259e-08\n",
      "Epoch 255/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2743 - mae: 0.2235 - val_loss: 0.4700 - val_mae: 0.2494 - learning_rate: 1.5259e-08\n",
      "Epoch 256/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2978 - mae: 0.2303 - val_loss: 0.4702 - val_mae: 0.2496 - learning_rate: 1.5259e-08\n",
      "Epoch 257/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2884 - mae: 0.2250 - val_loss: 0.4706 - val_mae: 0.2495 - learning_rate: 1.5259e-08\n",
      "Epoch 258/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2896 - mae: 0.2263 - val_loss: 0.4704 - val_mae: 0.2492 - learning_rate: 1.5259e-08\n",
      "Epoch 259/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2886 - mae: 0.2241 - val_loss: 0.4701 - val_mae: 0.2492 - learning_rate: 1.5259e-08\n",
      "Epoch 260/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2968 - mae: 0.2259 - val_loss: 0.4700 - val_mae: 0.2495 - learning_rate: 1.5259e-08\n",
      "Epoch 261/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2993 - mae: 0.2290 - val_loss: 0.4707 - val_mae: 0.2496 - learning_rate: 1.5259e-08\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 262: ReduceLROnPlateau reducing learning rate to 7.629394893626795e-09.\n",
      "41/41 - 1s - 16ms/step - loss: 0.2905 - mae: 0.2261 - val_loss: 0.4702 - val_mae: 0.2494 - learning_rate: 1.5259e-08\n",
      "Epoch 263/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2921 - mae: 0.2244 - val_loss: 0.4701 - val_mae: 0.2495 - learning_rate: 7.6294e-09\n",
      "Epoch 264/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2897 - mae: 0.2257 - val_loss: 0.4708 - val_mae: 0.2500 - learning_rate: 7.6294e-09\n",
      "Epoch 265/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2793 - mae: 0.2225 - val_loss: 0.4699 - val_mae: 0.2496 - learning_rate: 7.6294e-09\n",
      "Epoch 266/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3031 - mae: 0.2283 - val_loss: 0.4697 - val_mae: 0.2495 - learning_rate: 7.6294e-09\n",
      "Epoch 267/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2811 - mae: 0.2251 - val_loss: 0.4691 - val_mae: 0.2495 - learning_rate: 7.6294e-09\n",
      "Epoch 268/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2848 - mae: 0.2251 - val_loss: 0.4696 - val_mae: 0.2494 - learning_rate: 7.6294e-09\n",
      "Epoch 269/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2948 - mae: 0.2295 - val_loss: 0.4697 - val_mae: 0.2495 - learning_rate: 7.6294e-09\n",
      "Epoch 270/300\n",
      "41/41 - 1s - 19ms/step - loss: 0.2935 - mae: 0.2276 - val_loss: 0.4702 - val_mae: 0.2497 - learning_rate: 7.6294e-09\n",
      "Epoch 271/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2819 - mae: 0.2239 - val_loss: 0.4704 - val_mae: 0.2498 - learning_rate: 7.6294e-09\n",
      "Epoch 272/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2892 - mae: 0.2284 - val_loss: 0.4701 - val_mae: 0.2498 - learning_rate: 7.6294e-09\n",
      "Epoch 273/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2846 - mae: 0.2252 - val_loss: 0.4701 - val_mae: 0.2494 - learning_rate: 7.6294e-09\n",
      "Epoch 274/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2951 - mae: 0.2277 - val_loss: 0.4705 - val_mae: 0.2493 - learning_rate: 7.6294e-09\n",
      "Epoch 275/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2890 - mae: 0.2289 - val_loss: 0.4704 - val_mae: 0.2494 - learning_rate: 7.6294e-09\n",
      "Epoch 276/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2838 - mae: 0.2269 - val_loss: 0.4708 - val_mae: 0.2493 - learning_rate: 7.6294e-09\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 277: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "41/41 - 1s - 16ms/step - loss: 0.2876 - mae: 0.2228 - val_loss: 0.4704 - val_mae: 0.2496 - learning_rate: 7.6294e-09\n",
      "Epoch 278/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2813 - mae: 0.2242 - val_loss: 0.4704 - val_mae: 0.2495 - learning_rate: 3.8147e-09\n",
      "Epoch 279/300\n",
      "41/41 - 1s - 18ms/step - loss: 0.2936 - mae: 0.2251 - val_loss: 0.4704 - val_mae: 0.2494 - learning_rate: 3.8147e-09\n",
      "Epoch 280/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2879 - mae: 0.2261 - val_loss: 0.4710 - val_mae: 0.2498 - learning_rate: 3.8147e-09\n",
      "Epoch 281/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2941 - mae: 0.2252 - val_loss: 0.4707 - val_mae: 0.2498 - learning_rate: 3.8147e-09\n",
      "Epoch 282/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2850 - mae: 0.2256 - val_loss: 0.4706 - val_mae: 0.2498 - learning_rate: 3.8147e-09\n",
      "Epoch 283/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.3053 - mae: 0.2265 - val_loss: 0.4701 - val_mae: 0.2495 - learning_rate: 3.8147e-09\n",
      "Epoch 284/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2905 - mae: 0.2231 - val_loss: 0.4699 - val_mae: 0.2498 - learning_rate: 3.8147e-09\n",
      "Epoch 285/300\n",
      "41/41 - 1s - 17ms/step - loss: 0.2910 - mae: 0.2278 - val_loss: 0.4702 - val_mae: 0.2498 - learning_rate: 3.8147e-09\n",
      "Epoch 286/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2848 - mae: 0.2241 - val_loss: 0.4697 - val_mae: 0.2493 - learning_rate: 3.8147e-09\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 287: ReduceLROnPlateau reducing learning rate to 1.907348723406699e-09.\n",
      "41/41 - 1s - 16ms/step - loss: 0.2763 - mae: 0.2235 - val_loss: 0.4700 - val_mae: 0.2492 - learning_rate: 3.8147e-09\n",
      "Epoch 288/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2855 - mae: 0.2279 - val_loss: 0.4705 - val_mae: 0.2491 - learning_rate: 1.9073e-09\n",
      "Epoch 289/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2854 - mae: 0.2258 - val_loss: 0.4709 - val_mae: 0.2492 - learning_rate: 1.9073e-09\n",
      "Epoch 290/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2943 - mae: 0.2264 - val_loss: 0.4708 - val_mae: 0.2492 - learning_rate: 1.9073e-09\n",
      "Epoch 291/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2946 - mae: 0.2259 - val_loss: 0.4702 - val_mae: 0.2494 - learning_rate: 1.9073e-09\n",
      "Epoch 292/300\n",
      "41/41 - 1s - 15ms/step - loss: 0.2805 - mae: 0.2230 - val_loss: 0.4702 - val_mae: 0.2494 - learning_rate: 1.9073e-09\n",
      "Epoch 293/300\n",
      "41/41 - 1s - 18ms/step - loss: 0.2821 - mae: 0.2227 - val_loss: 0.4698 - val_mae: 0.2493 - learning_rate: 1.9073e-09\n",
      "Epoch 294/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2905 - mae: 0.2264 - val_loss: 0.4696 - val_mae: 0.2495 - learning_rate: 1.9073e-09\n",
      "Epoch 295/300\n",
      "41/41 - 1s - 19ms/step - loss: 0.3036 - mae: 0.2280 - val_loss: 0.4702 - val_mae: 0.2491 - learning_rate: 1.9073e-09\n",
      "Epoch 296/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2946 - mae: 0.2265 - val_loss: 0.4701 - val_mae: 0.2492 - learning_rate: 1.9073e-09\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 297: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "41/41 - 1s - 16ms/step - loss: 0.2832 - mae: 0.2275 - val_loss: 0.4702 - val_mae: 0.2491 - learning_rate: 1.9073e-09\n",
      "Epoch 298/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2898 - mae: 0.2272 - val_loss: 0.4697 - val_mae: 0.2491 - learning_rate: 9.5367e-10\n",
      "Epoch 299/300\n",
      "41/41 - 1s - 18ms/step - loss: 0.2886 - mae: 0.2257 - val_loss: 0.4696 - val_mae: 0.2491 - learning_rate: 9.5367e-10\n",
      "Epoch 300/300\n",
      "41/41 - 1s - 16ms/step - loss: 0.2983 - mae: 0.2245 - val_loss: 0.4694 - val_mae: 0.2490 - learning_rate: 9.5367e-10\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Fold Validation MAE: 0.2495\n",
      "\n",
      "Training on 1737 samples, validating on 433 samples.\n",
      "Epoch 1/300\n",
      "55/55 - 1s - 17ms/step - loss: 0.3730 - mae: 0.2760 - val_loss: 0.4340 - val_mae: 0.2357 - learning_rate: 9.5367e-10\n",
      "Epoch 2/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3927 - mae: 0.2789 - val_loss: 0.4246 - val_mae: 0.2353 - learning_rate: 9.5367e-10\n",
      "Epoch 3/300\n",
      "55/55 - 1s - 16ms/step - loss: 0.3813 - mae: 0.2800 - val_loss: 0.4204 - val_mae: 0.2355 - learning_rate: 9.5367e-10\n",
      "Epoch 4/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3815 - mae: 0.2759 - val_loss: 0.4172 - val_mae: 0.2355 - learning_rate: 9.5367e-10\n",
      "Epoch 5/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3838 - mae: 0.2791 - val_loss: 0.4159 - val_mae: 0.2352 - learning_rate: 9.5367e-10\n",
      "Epoch 6/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3831 - mae: 0.2754 - val_loss: 0.4148 - val_mae: 0.2353 - learning_rate: 9.5367e-10\n",
      "Epoch 7/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3918 - mae: 0.2780 - val_loss: 0.4139 - val_mae: 0.2357 - learning_rate: 9.5367e-10\n",
      "Epoch 8/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3678 - mae: 0.2737 - val_loss: 0.4135 - val_mae: 0.2355 - learning_rate: 9.5367e-10\n",
      "Epoch 9/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3742 - mae: 0.2747 - val_loss: 0.4127 - val_mae: 0.2353 - learning_rate: 9.5367e-10\n",
      "Epoch 10/300\n",
      "55/55 - 1s - 16ms/step - loss: 0.3792 - mae: 0.2770 - val_loss: 0.4131 - val_mae: 0.2352 - learning_rate: 9.5367e-10\n",
      "Epoch 11/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3693 - mae: 0.2733 - val_loss: 0.4132 - val_mae: 0.2356 - learning_rate: 9.5367e-10\n",
      "Epoch 12/300\n",
      "55/55 - 1s - 16ms/step - loss: 0.3662 - mae: 0.2747 - val_loss: 0.4142 - val_mae: 0.2357 - learning_rate: 9.5367e-10\n",
      "Epoch 13/300\n",
      "55/55 - 1s - 16ms/step - loss: 0.3758 - mae: 0.2745 - val_loss: 0.4139 - val_mae: 0.2354 - learning_rate: 9.5367e-10\n",
      "Epoch 14/300\n",
      "55/55 - 1s - 16ms/step - loss: 0.3819 - mae: 0.2733 - val_loss: 0.4128 - val_mae: 0.2350 - learning_rate: 9.5367e-10\n",
      "Epoch 15/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3750 - mae: 0.2758 - val_loss: 0.4133 - val_mae: 0.2356 - learning_rate: 9.5367e-10\n",
      "Epoch 16/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3734 - mae: 0.2753 - val_loss: 0.4134 - val_mae: 0.2359 - learning_rate: 9.5367e-10\n",
      "Epoch 17/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3774 - mae: 0.2771 - val_loss: 0.4124 - val_mae: 0.2356 - learning_rate: 9.5367e-10\n",
      "Epoch 18/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3746 - mae: 0.2747 - val_loss: 0.4130 - val_mae: 0.2353 - learning_rate: 9.5367e-10\n",
      "Epoch 19/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3631 - mae: 0.2724 - val_loss: 0.4136 - val_mae: 0.2357 - learning_rate: 9.5367e-10\n",
      "Epoch 20/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3863 - mae: 0.2781 - val_loss: 0.4137 - val_mae: 0.2355 - learning_rate: 9.5367e-10\n",
      "Epoch 21/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3854 - mae: 0.2764 - val_loss: 0.4134 - val_mae: 0.2356 - learning_rate: 9.5367e-10\n",
      "Epoch 22/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3871 - mae: 0.2767 - val_loss: 0.4137 - val_mae: 0.2354 - learning_rate: 9.5367e-10\n",
      "Epoch 23/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3758 - mae: 0.2758 - val_loss: 0.4137 - val_mae: 0.2356 - learning_rate: 9.5367e-10\n",
      "Epoch 24/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3874 - mae: 0.2769 - val_loss: 0.4137 - val_mae: 0.2356 - learning_rate: 9.5367e-10\n",
      "Epoch 25/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3751 - mae: 0.2756 - val_loss: 0.4137 - val_mae: 0.2354 - learning_rate: 9.5367e-10\n",
      "Epoch 26/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3762 - mae: 0.2744 - val_loss: 0.4133 - val_mae: 0.2357 - learning_rate: 9.5367e-10\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 4.768371808516747e-10.\n",
      "55/55 - 1s - 16ms/step - loss: 0.3788 - mae: 0.2750 - val_loss: 0.4133 - val_mae: 0.2354 - learning_rate: 9.5367e-10\n",
      "Epoch 28/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3759 - mae: 0.2764 - val_loss: 0.4127 - val_mae: 0.2351 - learning_rate: 4.7684e-10\n",
      "Epoch 29/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3810 - mae: 0.2790 - val_loss: 0.4133 - val_mae: 0.2353 - learning_rate: 4.7684e-10\n",
      "Epoch 30/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3749 - mae: 0.2756 - val_loss: 0.4135 - val_mae: 0.2356 - learning_rate: 4.7684e-10\n",
      "Epoch 31/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3687 - mae: 0.2733 - val_loss: 0.4129 - val_mae: 0.2350 - learning_rate: 4.7684e-10\n",
      "Epoch 32/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3792 - mae: 0.2758 - val_loss: 0.4131 - val_mae: 0.2353 - learning_rate: 4.7684e-10\n",
      "Epoch 33/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3744 - mae: 0.2748 - val_loss: 0.4123 - val_mae: 0.2354 - learning_rate: 4.7684e-10\n",
      "Epoch 34/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3914 - mae: 0.2780 - val_loss: 0.4113 - val_mae: 0.2347 - learning_rate: 4.7684e-10\n",
      "Epoch 35/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3848 - mae: 0.2791 - val_loss: 0.4127 - val_mae: 0.2352 - learning_rate: 4.7684e-10\n",
      "Epoch 36/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3866 - mae: 0.2783 - val_loss: 0.4132 - val_mae: 0.2358 - learning_rate: 4.7684e-10\n",
      "Epoch 37/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3764 - mae: 0.2763 - val_loss: 0.4134 - val_mae: 0.2357 - learning_rate: 4.7684e-10\n",
      "Epoch 38/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3825 - mae: 0.2758 - val_loss: 0.4129 - val_mae: 0.2354 - learning_rate: 4.7684e-10\n",
      "Epoch 39/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3680 - mae: 0.2730 - val_loss: 0.4131 - val_mae: 0.2353 - learning_rate: 4.7684e-10\n",
      "Epoch 40/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3778 - mae: 0.2774 - val_loss: 0.4130 - val_mae: 0.2355 - learning_rate: 4.7684e-10\n",
      "Epoch 41/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3807 - mae: 0.2755 - val_loss: 0.4140 - val_mae: 0.2356 - learning_rate: 4.7684e-10\n",
      "Epoch 42/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3859 - mae: 0.2750 - val_loss: 0.4141 - val_mae: 0.2354 - learning_rate: 4.7684e-10\n",
      "Epoch 43/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3786 - mae: 0.2763 - val_loss: 0.4128 - val_mae: 0.2352 - learning_rate: 4.7684e-10\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "55/55 - 1s - 15ms/step - loss: 0.3836 - mae: 0.2761 - val_loss: 0.4129 - val_mae: 0.2352 - learning_rate: 4.7684e-10\n",
      "Epoch 45/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3856 - mae: 0.2781 - val_loss: 0.4131 - val_mae: 0.2352 - learning_rate: 2.3842e-10\n",
      "Epoch 46/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3756 - mae: 0.2748 - val_loss: 0.4124 - val_mae: 0.2351 - learning_rate: 2.3842e-10\n",
      "Epoch 47/300\n",
      "55/55 - 1s - 16ms/step - loss: 0.3701 - mae: 0.2699 - val_loss: 0.4136 - val_mae: 0.2358 - learning_rate: 2.3842e-10\n",
      "Epoch 48/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3733 - mae: 0.2733 - val_loss: 0.4136 - val_mae: 0.2356 - learning_rate: 2.3842e-10\n",
      "Epoch 49/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3895 - mae: 0.2760 - val_loss: 0.4137 - val_mae: 0.2356 - learning_rate: 2.3842e-10\n",
      "Epoch 50/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3873 - mae: 0.2793 - val_loss: 0.4139 - val_mae: 0.2356 - learning_rate: 2.3842e-10\n",
      "Epoch 51/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3797 - mae: 0.2747 - val_loss: 0.4132 - val_mae: 0.2354 - learning_rate: 2.3842e-10\n",
      "Epoch 52/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3715 - mae: 0.2733 - val_loss: 0.4131 - val_mae: 0.2355 - learning_rate: 2.3842e-10\n",
      "Epoch 53/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3707 - mae: 0.2718 - val_loss: 0.4136 - val_mae: 0.2357 - learning_rate: 2.3842e-10\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.1920929521291868e-10.\n",
      "55/55 - 1s - 15ms/step - loss: 0.3741 - mae: 0.2756 - val_loss: 0.4134 - val_mae: 0.2355 - learning_rate: 2.3842e-10\n",
      "Epoch 55/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3890 - mae: 0.2768 - val_loss: 0.4145 - val_mae: 0.2360 - learning_rate: 1.1921e-10\n",
      "Epoch 56/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3902 - mae: 0.2800 - val_loss: 0.4134 - val_mae: 0.2358 - learning_rate: 1.1921e-10\n",
      "Epoch 57/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3636 - mae: 0.2733 - val_loss: 0.4129 - val_mae: 0.2355 - learning_rate: 1.1921e-10\n",
      "Epoch 58/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3745 - mae: 0.2729 - val_loss: 0.4127 - val_mae: 0.2354 - learning_rate: 1.1921e-10\n",
      "Epoch 59/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3894 - mae: 0.2752 - val_loss: 0.4138 - val_mae: 0.2354 - learning_rate: 1.1921e-10\n",
      "Epoch 60/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3807 - mae: 0.2786 - val_loss: 0.4138 - val_mae: 0.2352 - learning_rate: 1.1921e-10\n",
      "Epoch 61/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3798 - mae: 0.2762 - val_loss: 0.4133 - val_mae: 0.2352 - learning_rate: 1.1921e-10\n",
      "Epoch 62/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3869 - mae: 0.2759 - val_loss: 0.4134 - val_mae: 0.2355 - learning_rate: 1.1921e-10\n",
      "Epoch 63/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3873 - mae: 0.2771 - val_loss: 0.4142 - val_mae: 0.2356 - learning_rate: 1.1921e-10\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "55/55 - 1s - 15ms/step - loss: 0.3792 - mae: 0.2732 - val_loss: 0.4143 - val_mae: 0.2356 - learning_rate: 1.1921e-10\n",
      "Epoch 65/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3820 - mae: 0.2776 - val_loss: 0.4135 - val_mae: 0.2355 - learning_rate: 5.9605e-11\n",
      "Epoch 66/300\n",
      "55/55 - 1s - 14ms/step - loss: 0.3717 - mae: 0.2742 - val_loss: 0.4135 - val_mae: 0.2355 - learning_rate: 5.9605e-11\n",
      "Epoch 67/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3905 - mae: 0.2759 - val_loss: 0.4137 - val_mae: 0.2356 - learning_rate: 5.9605e-11\n",
      "Epoch 68/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3831 - mae: 0.2751 - val_loss: 0.4134 - val_mae: 0.2352 - learning_rate: 5.9605e-11\n",
      "Epoch 69/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3750 - mae: 0.2744 - val_loss: 0.4136 - val_mae: 0.2356 - learning_rate: 5.9605e-11\n",
      "Epoch 70/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3890 - mae: 0.2801 - val_loss: 0.4134 - val_mae: 0.2354 - learning_rate: 5.9605e-11\n",
      "Epoch 71/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3707 - mae: 0.2719 - val_loss: 0.4135 - val_mae: 0.2355 - learning_rate: 5.9605e-11\n",
      "Epoch 72/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3796 - mae: 0.2786 - val_loss: 0.4131 - val_mae: 0.2355 - learning_rate: 5.9605e-11\n",
      "Epoch 73/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3837 - mae: 0.2760 - val_loss: 0.4135 - val_mae: 0.2355 - learning_rate: 5.9605e-11\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 2.980232380322967e-11.\n",
      "55/55 - 1s - 15ms/step - loss: 0.3894 - mae: 0.2792 - val_loss: 0.4135 - val_mae: 0.2355 - learning_rate: 5.9605e-11\n",
      "Epoch 75/300\n",
      "55/55 - 1s - 14ms/step - loss: 0.3732 - mae: 0.2770 - val_loss: 0.4132 - val_mae: 0.2353 - learning_rate: 2.9802e-11\n",
      "Epoch 76/300\n",
      "55/55 - 1s - 16ms/step - loss: 0.3783 - mae: 0.2760 - val_loss: 0.4135 - val_mae: 0.2354 - learning_rate: 2.9802e-11\n",
      "Epoch 77/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3798 - mae: 0.2769 - val_loss: 0.4136 - val_mae: 0.2355 - learning_rate: 2.9802e-11\n",
      "Epoch 78/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3956 - mae: 0.2780 - val_loss: 0.4132 - val_mae: 0.2351 - learning_rate: 2.9802e-11\n",
      "Epoch 79/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3826 - mae: 0.2778 - val_loss: 0.4139 - val_mae: 0.2350 - learning_rate: 2.9802e-11\n",
      "Epoch 80/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3723 - mae: 0.2749 - val_loss: 0.4132 - val_mae: 0.2351 - learning_rate: 2.9802e-11\n",
      "Epoch 81/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3895 - mae: 0.2759 - val_loss: 0.4139 - val_mae: 0.2355 - learning_rate: 2.9802e-11\n",
      "Epoch 82/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3925 - mae: 0.2820 - val_loss: 0.4132 - val_mae: 0.2350 - learning_rate: 2.9802e-11\n",
      "Epoch 83/300\n",
      "55/55 - 1s - 15ms/step - loss: 0.3628 - mae: 0.2744 - val_loss: 0.4127 - val_mae: 0.2354 - learning_rate: 2.9802e-11\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "55/55 - 1s - 15ms/step - loss: 0.3800 - mae: 0.2741 - val_loss: 0.4133 - val_mae: 0.2356 - learning_rate: 2.9802e-11\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Fold Validation MAE: 0.2347\n",
      "\n",
      "Training on 2170 samples, validating on 433 samples.\n",
      "Epoch 1/300\n",
      "68/68 - 1s - 16ms/step - loss: 0.4176 - mae: 0.2894 - val_loss: 0.4064 - val_mae: 0.2378 - learning_rate: 1.4901e-11\n",
      "Epoch 2/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4065 - mae: 0.2897 - val_loss: 0.4013 - val_mae: 0.2387 - learning_rate: 1.4901e-11\n",
      "Epoch 3/300\n",
      "68/68 - 1s - 16ms/step - loss: 0.3938 - mae: 0.2872 - val_loss: 0.3989 - val_mae: 0.2395 - learning_rate: 1.4901e-11\n",
      "Epoch 4/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4120 - mae: 0.2867 - val_loss: 0.3964 - val_mae: 0.2394 - learning_rate: 1.4901e-11\n",
      "Epoch 5/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4086 - mae: 0.2904 - val_loss: 0.3957 - val_mae: 0.2396 - learning_rate: 1.4901e-11\n",
      "Epoch 6/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3956 - mae: 0.2888 - val_loss: 0.3961 - val_mae: 0.2398 - learning_rate: 1.4901e-11\n",
      "Epoch 7/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3930 - mae: 0.2877 - val_loss: 0.3974 - val_mae: 0.2400 - learning_rate: 1.4901e-11\n",
      "Epoch 8/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3886 - mae: 0.2835 - val_loss: 0.3980 - val_mae: 0.2400 - learning_rate: 1.4901e-11\n",
      "Epoch 9/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3997 - mae: 0.2891 - val_loss: 0.3968 - val_mae: 0.2398 - learning_rate: 1.4901e-11\n",
      "Epoch 10/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3960 - mae: 0.2881 - val_loss: 0.3959 - val_mae: 0.2400 - learning_rate: 1.4901e-11\n",
      "Epoch 11/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4101 - mae: 0.2883 - val_loss: 0.3960 - val_mae: 0.2402 - learning_rate: 1.4901e-11\n",
      "Epoch 12/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4063 - mae: 0.2900 - val_loss: 0.3957 - val_mae: 0.2396 - learning_rate: 1.4901e-11\n",
      "Epoch 13/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4167 - mae: 0.2918 - val_loss: 0.3955 - val_mae: 0.2395 - learning_rate: 1.4901e-11\n",
      "Epoch 14/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3937 - mae: 0.2885 - val_loss: 0.3960 - val_mae: 0.2399 - learning_rate: 1.4901e-11\n",
      "Epoch 15/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3977 - mae: 0.2838 - val_loss: 0.3954 - val_mae: 0.2399 - learning_rate: 1.4901e-11\n",
      "Epoch 16/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3957 - mae: 0.2870 - val_loss: 0.3956 - val_mae: 0.2399 - learning_rate: 1.4901e-11\n",
      "Epoch 17/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4006 - mae: 0.2867 - val_loss: 0.3957 - val_mae: 0.2398 - learning_rate: 1.4901e-11\n",
      "Epoch 18/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4114 - mae: 0.2911 - val_loss: 0.3947 - val_mae: 0.2393 - learning_rate: 1.4901e-11\n",
      "Epoch 19/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4067 - mae: 0.2898 - val_loss: 0.3944 - val_mae: 0.2389 - learning_rate: 1.4901e-11\n",
      "Epoch 20/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3976 - mae: 0.2857 - val_loss: 0.3950 - val_mae: 0.2395 - learning_rate: 1.4901e-11\n",
      "Epoch 21/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4059 - mae: 0.2873 - val_loss: 0.3954 - val_mae: 0.2394 - learning_rate: 1.4901e-11\n",
      "Epoch 22/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3947 - mae: 0.2864 - val_loss: 0.3955 - val_mae: 0.2395 - learning_rate: 1.4901e-11\n",
      "Epoch 23/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4156 - mae: 0.2932 - val_loss: 0.3953 - val_mae: 0.2396 - learning_rate: 1.4901e-11\n",
      "Epoch 24/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4064 - mae: 0.2886 - val_loss: 0.3954 - val_mae: 0.2398 - learning_rate: 1.4901e-11\n",
      "Epoch 25/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4100 - mae: 0.2903 - val_loss: 0.3950 - val_mae: 0.2399 - learning_rate: 1.4901e-11\n",
      "Epoch 26/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3987 - mae: 0.2877 - val_loss: 0.3955 - val_mae: 0.2399 - learning_rate: 1.4901e-11\n",
      "Epoch 27/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4109 - mae: 0.2885 - val_loss: 0.3951 - val_mae: 0.2397 - learning_rate: 1.4901e-11\n",
      "Epoch 28/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4045 - mae: 0.2894 - val_loss: 0.3944 - val_mae: 0.2396 - learning_rate: 1.4901e-11\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 7.450580950807417e-12.\n",
      "68/68 - 1s - 15ms/step - loss: 0.4017 - mae: 0.2880 - val_loss: 0.3951 - val_mae: 0.2398 - learning_rate: 1.4901e-11\n",
      "Epoch 30/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3966 - mae: 0.2860 - val_loss: 0.3954 - val_mae: 0.2401 - learning_rate: 7.4506e-12\n",
      "Epoch 31/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4110 - mae: 0.2879 - val_loss: 0.3961 - val_mae: 0.2403 - learning_rate: 7.4506e-12\n",
      "Epoch 32/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4016 - mae: 0.2895 - val_loss: 0.3961 - val_mae: 0.2403 - learning_rate: 7.4506e-12\n",
      "Epoch 33/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4027 - mae: 0.2886 - val_loss: 0.3961 - val_mae: 0.2398 - learning_rate: 7.4506e-12\n",
      "Epoch 34/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4062 - mae: 0.2908 - val_loss: 0.3957 - val_mae: 0.2395 - learning_rate: 7.4506e-12\n",
      "Epoch 35/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4120 - mae: 0.2893 - val_loss: 0.3960 - val_mae: 0.2396 - learning_rate: 7.4506e-12\n",
      "Epoch 36/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3985 - mae: 0.2896 - val_loss: 0.3964 - val_mae: 0.2398 - learning_rate: 7.4506e-12\n",
      "Epoch 37/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4031 - mae: 0.2904 - val_loss: 0.3967 - val_mae: 0.2402 - learning_rate: 7.4506e-12\n",
      "Epoch 38/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3984 - mae: 0.2872 - val_loss: 0.3959 - val_mae: 0.2394 - learning_rate: 7.4506e-12\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "68/68 - 1s - 14ms/step - loss: 0.4044 - mae: 0.2891 - val_loss: 0.3969 - val_mae: 0.2395 - learning_rate: 7.4506e-12\n",
      "Epoch 40/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3963 - mae: 0.2875 - val_loss: 0.3971 - val_mae: 0.2399 - learning_rate: 3.7253e-12\n",
      "Epoch 41/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3959 - mae: 0.2844 - val_loss: 0.3962 - val_mae: 0.2397 - learning_rate: 3.7253e-12\n",
      "Epoch 42/300\n",
      "68/68 - 1s - 16ms/step - loss: 0.3900 - mae: 0.2854 - val_loss: 0.3952 - val_mae: 0.2395 - learning_rate: 3.7253e-12\n",
      "Epoch 43/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3925 - mae: 0.2839 - val_loss: 0.3952 - val_mae: 0.2396 - learning_rate: 3.7253e-12\n",
      "Epoch 44/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4065 - mae: 0.2911 - val_loss: 0.3950 - val_mae: 0.2396 - learning_rate: 3.7253e-12\n",
      "Epoch 45/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3953 - mae: 0.2858 - val_loss: 0.3946 - val_mae: 0.2397 - learning_rate: 3.7253e-12\n",
      "Epoch 46/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4007 - mae: 0.2888 - val_loss: 0.3956 - val_mae: 0.2397 - learning_rate: 3.7253e-12\n",
      "Epoch 47/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4038 - mae: 0.2867 - val_loss: 0.3961 - val_mae: 0.2399 - learning_rate: 3.7253e-12\n",
      "Epoch 48/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3977 - mae: 0.2884 - val_loss: 0.3964 - val_mae: 0.2399 - learning_rate: 3.7253e-12\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.8626452377018543e-12.\n",
      "68/68 - 1s - 16ms/step - loss: 0.3934 - mae: 0.2869 - val_loss: 0.3958 - val_mae: 0.2399 - learning_rate: 3.7253e-12\n",
      "Epoch 50/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3992 - mae: 0.2892 - val_loss: 0.3970 - val_mae: 0.2399 - learning_rate: 1.8626e-12\n",
      "Epoch 51/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4059 - mae: 0.2906 - val_loss: 0.3974 - val_mae: 0.2400 - learning_rate: 1.8626e-12\n",
      "Epoch 52/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3948 - mae: 0.2871 - val_loss: 0.3977 - val_mae: 0.2400 - learning_rate: 1.8626e-12\n",
      "Epoch 53/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4057 - mae: 0.2883 - val_loss: 0.3968 - val_mae: 0.2399 - learning_rate: 1.8626e-12\n",
      "Epoch 54/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4071 - mae: 0.2893 - val_loss: 0.3961 - val_mae: 0.2399 - learning_rate: 1.8626e-12\n",
      "Epoch 55/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3985 - mae: 0.2870 - val_loss: 0.3951 - val_mae: 0.2398 - learning_rate: 1.8626e-12\n",
      "Epoch 56/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3993 - mae: 0.2879 - val_loss: 0.3953 - val_mae: 0.2400 - learning_rate: 1.8626e-12\n",
      "Epoch 57/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3971 - mae: 0.2866 - val_loss: 0.3961 - val_mae: 0.2400 - learning_rate: 1.8626e-12\n",
      "Epoch 58/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4096 - mae: 0.2903 - val_loss: 0.3960 - val_mae: 0.2400 - learning_rate: 1.8626e-12\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "68/68 - 1s - 15ms/step - loss: 0.4050 - mae: 0.2906 - val_loss: 0.3958 - val_mae: 0.2402 - learning_rate: 1.8626e-12\n",
      "Epoch 60/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3999 - mae: 0.2867 - val_loss: 0.3956 - val_mae: 0.2400 - learning_rate: 9.3132e-13\n",
      "Epoch 61/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4065 - mae: 0.2888 - val_loss: 0.3955 - val_mae: 0.2403 - learning_rate: 9.3132e-13\n",
      "Epoch 62/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3946 - mae: 0.2864 - val_loss: 0.3958 - val_mae: 0.2399 - learning_rate: 9.3132e-13\n",
      "Epoch 63/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4155 - mae: 0.2913 - val_loss: 0.3961 - val_mae: 0.2398 - learning_rate: 9.3132e-13\n",
      "Epoch 64/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4136 - mae: 0.2903 - val_loss: 0.3951 - val_mae: 0.2397 - learning_rate: 9.3132e-13\n",
      "Epoch 65/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4078 - mae: 0.2897 - val_loss: 0.3957 - val_mae: 0.2400 - learning_rate: 9.3132e-13\n",
      "Epoch 66/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.3785 - mae: 0.2824 - val_loss: 0.3958 - val_mae: 0.2401 - learning_rate: 9.3132e-13\n",
      "Epoch 67/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4003 - mae: 0.2874 - val_loss: 0.3959 - val_mae: 0.2399 - learning_rate: 9.3132e-13\n",
      "Epoch 68/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4003 - mae: 0.2887 - val_loss: 0.3958 - val_mae: 0.2397 - learning_rate: 9.3132e-13\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 4.656613094254636e-13.\n",
      "68/68 - 1s - 15ms/step - loss: 0.3935 - mae: 0.2874 - val_loss: 0.3955 - val_mae: 0.2397 - learning_rate: 9.3132e-13\n",
      "Epoch 70/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4006 - mae: 0.2858 - val_loss: 0.3957 - val_mae: 0.2398 - learning_rate: 4.6566e-13\n",
      "Epoch 71/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4085 - mae: 0.2893 - val_loss: 0.3959 - val_mae: 0.2395 - learning_rate: 4.6566e-13\n",
      "Epoch 72/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4034 - mae: 0.2912 - val_loss: 0.3966 - val_mae: 0.2399 - learning_rate: 4.6566e-13\n",
      "Epoch 73/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4006 - mae: 0.2873 - val_loss: 0.3969 - val_mae: 0.2400 - learning_rate: 4.6566e-13\n",
      "Epoch 74/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3916 - mae: 0.2868 - val_loss: 0.3965 - val_mae: 0.2398 - learning_rate: 4.6566e-13\n",
      "Epoch 75/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.3945 - mae: 0.2836 - val_loss: 0.3974 - val_mae: 0.2401 - learning_rate: 4.6566e-13\n",
      "Epoch 76/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4015 - mae: 0.2903 - val_loss: 0.3973 - val_mae: 0.2401 - learning_rate: 4.6566e-13\n",
      "Epoch 77/300\n",
      "68/68 - 1s - 14ms/step - loss: 0.4019 - mae: 0.2878 - val_loss: 0.3970 - val_mae: 0.2401 - learning_rate: 4.6566e-13\n",
      "Epoch 78/300\n",
      "68/68 - 1s - 15ms/step - loss: 0.4026 - mae: 0.2863 - val_loss: 0.3963 - val_mae: 0.2399 - learning_rate: 4.6566e-13\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Fold Validation MAE: 0.2396\n",
      "\n",
      "Average CV MAE: 0.2567\n",
      "\n",
      "Training on the full train dataset and testing on unseen data...\n",
      "Epoch 1/300\n",
      "82/82 - 1s - 16ms/step - loss: 0.4221 - mae: 0.2978 - val_loss: 0.6057 - val_mae: 0.2840 - learning_rate: 4.6566e-13\n",
      "Epoch 2/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4296 - mae: 0.2984 - val_loss: 0.6042 - val_mae: 0.2844 - learning_rate: 4.6566e-13\n",
      "Epoch 3/300\n",
      "82/82 - 1s - 16ms/step - loss: 0.4352 - mae: 0.2973 - val_loss: 0.6038 - val_mae: 0.2844 - learning_rate: 4.6566e-13\n",
      "Epoch 4/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4156 - mae: 0.2962 - val_loss: 0.6026 - val_mae: 0.2840 - learning_rate: 4.6566e-13\n",
      "Epoch 5/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4268 - mae: 0.2962 - val_loss: 0.6021 - val_mae: 0.2839 - learning_rate: 4.6566e-13\n",
      "Epoch 6/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4250 - mae: 0.2961 - val_loss: 0.6013 - val_mae: 0.2843 - learning_rate: 4.6566e-13\n",
      "Epoch 7/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4222 - mae: 0.2973 - val_loss: 0.6007 - val_mae: 0.2842 - learning_rate: 4.6566e-13\n",
      "Epoch 8/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4261 - mae: 0.2984 - val_loss: 0.6020 - val_mae: 0.2842 - learning_rate: 4.6566e-13\n",
      "Epoch 9/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4180 - mae: 0.2937 - val_loss: 0.6017 - val_mae: 0.2838 - learning_rate: 4.6566e-13\n",
      "Epoch 10/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4284 - mae: 0.2980 - val_loss: 0.6024 - val_mae: 0.2838 - learning_rate: 4.6566e-13\n",
      "Epoch 11/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4251 - mae: 0.2980 - val_loss: 0.6030 - val_mae: 0.2842 - learning_rate: 4.6566e-13\n",
      "Epoch 12/300\n",
      "82/82 - 1s - 16ms/step - loss: 0.4186 - mae: 0.2952 - val_loss: 0.6023 - val_mae: 0.2839 - learning_rate: 4.6566e-13\n",
      "Epoch 13/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4273 - mae: 0.2970 - val_loss: 0.6023 - val_mae: 0.2841 - learning_rate: 4.6566e-13\n",
      "Epoch 14/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4181 - mae: 0.2950 - val_loss: 0.6026 - val_mae: 0.2843 - learning_rate: 4.6566e-13\n",
      "Epoch 15/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4392 - mae: 0.3007 - val_loss: 0.6025 - val_mae: 0.2844 - learning_rate: 4.6566e-13\n",
      "Epoch 16/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4260 - mae: 0.2982 - val_loss: 0.6032 - val_mae: 0.2844 - learning_rate: 4.6566e-13\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "82/82 - 1s - 15ms/step - loss: 0.4287 - mae: 0.2978 - val_loss: 0.6035 - val_mae: 0.2845 - learning_rate: 4.6566e-13\n",
      "Epoch 18/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4372 - mae: 0.3005 - val_loss: 0.6025 - val_mae: 0.2842 - learning_rate: 2.3283e-13\n",
      "Epoch 19/300\n",
      "82/82 - 1s - 16ms/step - loss: 0.4281 - mae: 0.2982 - val_loss: 0.6024 - val_mae: 0.2840 - learning_rate: 2.3283e-13\n",
      "Epoch 20/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4166 - mae: 0.2948 - val_loss: 0.6020 - val_mae: 0.2838 - learning_rate: 2.3283e-13\n",
      "Epoch 21/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4203 - mae: 0.2962 - val_loss: 0.6015 - val_mae: 0.2842 - learning_rate: 2.3283e-13\n",
      "Epoch 22/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4165 - mae: 0.2948 - val_loss: 0.6013 - val_mae: 0.2843 - learning_rate: 2.3283e-13\n",
      "Epoch 23/300\n",
      "82/82 - 1s - 16ms/step - loss: 0.4218 - mae: 0.2967 - val_loss: 0.6022 - val_mae: 0.2844 - learning_rate: 2.3283e-13\n",
      "Epoch 24/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4406 - mae: 0.3011 - val_loss: 0.6027 - val_mae: 0.2842 - learning_rate: 2.3283e-13\n",
      "Epoch 25/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4221 - mae: 0.2957 - val_loss: 0.6036 - val_mae: 0.2842 - learning_rate: 2.3283e-13\n",
      "Epoch 26/300\n",
      "82/82 - 1s - 14ms/step - loss: 0.4329 - mae: 0.2984 - val_loss: 0.6041 - val_mae: 0.2843 - learning_rate: 2.3283e-13\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.164153273563659e-13.\n",
      "82/82 - 1s - 15ms/step - loss: 0.4184 - mae: 0.2964 - val_loss: 0.6036 - val_mae: 0.2845 - learning_rate: 2.3283e-13\n",
      "Epoch 28/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4240 - mae: 0.2964 - val_loss: 0.6031 - val_mae: 0.2841 - learning_rate: 1.1642e-13\n",
      "Epoch 29/300\n",
      "82/82 - 1s - 17ms/step - loss: 0.4198 - mae: 0.2957 - val_loss: 0.6031 - val_mae: 0.2842 - learning_rate: 1.1642e-13\n",
      "Epoch 30/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4236 - mae: 0.2971 - val_loss: 0.6027 - val_mae: 0.2846 - learning_rate: 1.1642e-13\n",
      "Epoch 31/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4343 - mae: 0.2991 - val_loss: 0.6031 - val_mae: 0.2846 - learning_rate: 1.1642e-13\n",
      "Epoch 32/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4366 - mae: 0.2985 - val_loss: 0.6019 - val_mae: 0.2844 - learning_rate: 1.1642e-13\n",
      "Epoch 33/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4233 - mae: 0.2985 - val_loss: 0.6028 - val_mae: 0.2845 - learning_rate: 1.1642e-13\n",
      "Epoch 34/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4260 - mae: 0.2967 - val_loss: 0.6026 - val_mae: 0.2842 - learning_rate: 1.1642e-13\n",
      "Epoch 35/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4191 - mae: 0.2945 - val_loss: 0.6019 - val_mae: 0.2844 - learning_rate: 1.1642e-13\n",
      "Epoch 36/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4276 - mae: 0.2990 - val_loss: 0.6034 - val_mae: 0.2845 - learning_rate: 1.1642e-13\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "82/82 - 1s - 15ms/step - loss: 0.4257 - mae: 0.2975 - val_loss: 0.6018 - val_mae: 0.2841 - learning_rate: 1.1642e-13\n",
      "Epoch 38/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4438 - mae: 0.3007 - val_loss: 0.6029 - val_mae: 0.2844 - learning_rate: 5.8208e-14\n",
      "Epoch 39/300\n",
      "82/82 - 1s - 14ms/step - loss: 0.4154 - mae: 0.2950 - val_loss: 0.6020 - val_mae: 0.2840 - learning_rate: 5.8208e-14\n",
      "Epoch 40/300\n",
      "82/82 - 1s - 14ms/step - loss: 0.4359 - mae: 0.3006 - val_loss: 0.6029 - val_mae: 0.2841 - learning_rate: 5.8208e-14\n",
      "Epoch 41/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4198 - mae: 0.2960 - val_loss: 0.6028 - val_mae: 0.2841 - learning_rate: 5.8208e-14\n",
      "Epoch 42/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4250 - mae: 0.2975 - val_loss: 0.6036 - val_mae: 0.2843 - learning_rate: 5.8208e-14\n",
      "Epoch 43/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4106 - mae: 0.2950 - val_loss: 0.6026 - val_mae: 0.2843 - learning_rate: 5.8208e-14\n",
      "Epoch 44/300\n",
      "82/82 - 1s - 14ms/step - loss: 0.4240 - mae: 0.2978 - val_loss: 0.6024 - val_mae: 0.2844 - learning_rate: 5.8208e-14\n",
      "Epoch 45/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4290 - mae: 0.2990 - val_loss: 0.6027 - val_mae: 0.2846 - learning_rate: 5.8208e-14\n",
      "Epoch 46/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4365 - mae: 0.3000 - val_loss: 0.6022 - val_mae: 0.2843 - learning_rate: 5.8208e-14\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 2.9103831839091474e-14.\n",
      "82/82 - 1s - 15ms/step - loss: 0.4197 - mae: 0.2951 - val_loss: 0.6028 - val_mae: 0.2846 - learning_rate: 5.8208e-14\n",
      "Epoch 48/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4200 - mae: 0.2966 - val_loss: 0.6024 - val_mae: 0.2844 - learning_rate: 2.9104e-14\n",
      "Epoch 49/300\n",
      "82/82 - 1s - 16ms/step - loss: 0.4199 - mae: 0.2943 - val_loss: 0.6024 - val_mae: 0.2842 - learning_rate: 2.9104e-14\n",
      "Epoch 50/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4177 - mae: 0.2955 - val_loss: 0.6023 - val_mae: 0.2843 - learning_rate: 2.9104e-14\n",
      "Epoch 51/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4395 - mae: 0.3011 - val_loss: 0.6024 - val_mae: 0.2846 - learning_rate: 2.9104e-14\n",
      "Epoch 52/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4202 - mae: 0.2956 - val_loss: 0.6027 - val_mae: 0.2844 - learning_rate: 2.9104e-14\n",
      "Epoch 53/300\n",
      "82/82 - 1s - 14ms/step - loss: 0.4320 - mae: 0.2970 - val_loss: 0.6023 - val_mae: 0.2844 - learning_rate: 2.9104e-14\n",
      "Epoch 54/300\n",
      "82/82 - 1s - 14ms/step - loss: 0.4296 - mae: 0.2963 - val_loss: 0.6027 - val_mae: 0.2845 - learning_rate: 2.9104e-14\n",
      "Epoch 55/300\n",
      "82/82 - 1s - 14ms/step - loss: 0.4224 - mae: 0.2973 - val_loss: 0.6022 - val_mae: 0.2841 - learning_rate: 2.9104e-14\n",
      "Epoch 56/300\n",
      "82/82 - 1s - 15ms/step - loss: 0.4221 - mae: 0.2948 - val_loss: 0.6032 - val_mae: 0.2845 - learning_rate: 2.9104e-14\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "82/82 - 1s - 15ms/step - loss: 0.4284 - mae: 0.2971 - val_loss: 0.6025 - val_mae: 0.2842 - learning_rate: 2.9104e-14\n",
      "Final Test Loss: 0.6007357835769653 MAE: 0.2841860353946686\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 4) Train-Test Split (chronological on Year)\n",
    "# ------------------------------------------------------------------\n",
    "split_year = 2016\n",
    "train_df = df_agg_scaled[df_agg_scaled[\"Year\"] < split_year].copy()\n",
    "test_df  = df_agg_scaled[df_agg_scaled[\"Year\"] >= split_year].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"Year\"] + target_cols)\n",
    "y_train = train_df[target_cols]\n",
    "\n",
    "X_test  = test_df.drop(columns=[\"Year\"] + target_cols)\n",
    "y_test  = test_df[target_cols]\n",
    "\n",
    "model_input_columns = X_train.columns.tolist()\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test  shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Build & Train a Deeper FFNN with Callbacks & Cross-Validation\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Set environment variables for multi-threading\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"20\"\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(20)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(20)\n",
    "\n",
    "# Ensure reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(input_dim,)),\n",
    "\n",
    "        Dense(512, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(256, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(128, activation='swish', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(64, activation='swish'),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Dense(64, activation='swish'),\n",
    "        Dense(3, activation='linear')  # Predicting Gold, Silver, Bronze\n",
    "    ])\n",
    "    optimizer = AdamW(learning_rate=0.0005, weight_decay=1e-5)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "nn_model = build_nn_model(X_train.shape[1])\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Time Series Cross-Validation\n",
    "# ------------------------------------------------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_results = []\n",
    "\n",
    "for train_index, val_index in tscv.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    print(f\"\\nTraining on {len(train_index)} samples, validating on {len(val_index)} samples.\")\n",
    "\n",
    "    history = nn_model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        epochs=300,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    val_predictions = nn_model.predict(X_val_fold)\n",
    "    val_mae = mean_absolute_error(y_val_fold, val_predictions)\n",
    "    cv_results.append(val_mae)\n",
    "    print(f\"Fold Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage CV MAE: {np.mean(cv_results):.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Final Training & Testing\n",
    "# ------------------------------------------------------------------\n",
    "if not X_test.empty:\n",
    "    print(\"\\nTraining on the full train dataset and testing on unseen data...\")\n",
    "    history = nn_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=300,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=2\n",
    "    )\n",
    "    final_loss, final_mae = nn_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Final Test Loss:\", final_loss, \"MAE:\", final_mae)\n",
    "else:\n",
    "    print(\"No test set to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e6a4fdb-ca86-4185-8b79-46503070f79f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Medal history for NOC=CHN (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1992</td>\n",
       "      <td>14.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>1996</td>\n",
       "      <td>13.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2004</td>\n",
       "      <td>42.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2008</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2016</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>2020</td>\n",
       "      <td>59.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>2024</td>\n",
       "      <td>71.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1299  1988         4.0          16.0          30.0\n",
       "1459  1992        14.0          41.0          15.0\n",
       "1635  1996        13.0          66.0          15.0\n",
       "1831  2000        31.0          19.0          15.0\n",
       "2032  2004        42.0          27.0          13.0\n",
       "2232  2008        72.0          49.0          49.0\n",
       "2436  2012        50.0          38.0          29.0\n",
       "2641  2016        44.0          30.0          35.0\n",
       "2848  2020        59.0          49.0          33.0\n",
       "3054  2024        71.0          57.0          40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2020:\n",
      "  Gold=34.8, Silver=24.2, Bronze=31.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(34.839764, 24.19242, 31.703182)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 6) Prediction Utility for a Single (NOC,Year)\n",
    "# ------------------------------------------------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def predict_extrapolated_noc_year(\n",
    "    noc_code, \n",
    "    year_value,\n",
    "    df_agg,              # Unscaled aggregator DataFrame with columns: Year, NOC_{xxx}, and numeric features\n",
    "    scaler_features,     # The fitted feature scaler (StandardScaler/MinMaxScaler)\n",
    "    scaler_targets,      # The fitted target scaler\n",
    "    nn_model,            # Your trained model\n",
    "    feature_cols,        # List of columns used for model input\n",
    "    target_cols=[\"Medal_Gold\",\"Medal_Silver\",\"Medal_Bronze\"],\n",
    "    max_history_medals=10,\n",
    "    max_close_points=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a new \"extrapolated\" row for (Year=year_value, NOC=noc_code) using polynomial\n",
    "    regression (degree 2 to 4) on up to 'max_close_points' existing data from the same NOC,\n",
    "    for each numeric feature. That predicted \"base\" is used instead of zeros.\n",
    "\n",
    "    Steps:\n",
    "      1) Print up to 'max_history_medals' past data points for that NOC (debugging).\n",
    "      2) Gather up to 'max_close_points' aggregator rows from 'df_agg' with the smallest\n",
    "         year-distance from 'year_value'.\n",
    "      3) For each feature in 'feature_cols' (except 'NOC_*','Year','Medal_*'):\n",
    "         - Fit polynomials of degree in [2,3,4] to (year -> feature value) on those points.\n",
    "         - Pick the best degree by minimal MSE. Evaluate at 'year_value'.\n",
    "      4) Fill the new row with those predicted feature values, set 'NOC_{noc_code}' to 1,\n",
    "         other 'NOC_*' to 0, and 'Year'=year_value.\n",
    "      5) Scale & feed to nn_model. Invert the target scaling for final predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Print medal history for debugging\n",
    "    #    Filter all rows for this NOC, sorted by year, tail() for up to 10 data points\n",
    "    noc_col = f\"NOC_{noc_code}\"\n",
    "    if noc_col not in df_agg.columns:\n",
    "        print(f\"[Extrapolation] Column {noc_col} not in df_agg. No data for NOC={noc_code}.\")\n",
    "        return None\n",
    "    \n",
    "    df_noc_all = df_agg[df_agg[noc_col] == 1].copy()\n",
    "    df_noc_all_sorted = df_noc_all.sort_values(\"Year\")\n",
    "    if not df_noc_all_sorted.empty:\n",
    "        print(f\"[Debug] Medal history for NOC={noc_code} (up to last {max_history_medals} rows):\")\n",
    "        display(df_noc_all_sorted[[\"Year\"]+target_cols].tail(max_history_medals))\n",
    "    else:\n",
    "        print(f\"[Debug] No existing data for NOC={noc_code} in df_agg.\")\n",
    "\n",
    "    # 2) Gather up to 'max_close_points' aggregator rows from that NOC,\n",
    "    #    sorted by absolute difference from 'year_value'\n",
    "    df_noc_all_sorted[\"year_diff\"] = (df_noc_all_sorted[\"Year\"] - year_value).abs()\n",
    "    df_noc_closest = df_noc_all_sorted.nsmallest(max_close_points, \"year_diff\").copy()\n",
    "    # If this is empty or we have no data, we'll fallback to zeros for all columns\n",
    "    # but let's keep going\n",
    "\n",
    "    # 3) Construct an empty row\n",
    "    empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n",
    "    empty_row[\"Year\"] = float(year_value)\n",
    "\n",
    "    # If we have the NOC col, set it to 1, and set other NOC_... to 0\n",
    "    for c in df_agg.columns:\n",
    "        if c.startswith(\"NOC_\"):\n",
    "            empty_row[c] = 1.0 if c == noc_col else 0.0\n",
    "\n",
    "    # For each feature in feature_cols, we take a moving average if it isn't \"NOC_\", \"Year\", or \"Medal_\"\n",
    "    def moving_average_baseline(x_vals, y_vals, year_value, window=3):\n",
    "        \"\"\"\n",
    "        Computes a moving average using numpy arrays instead of DataFrame operations.\n",
    "        \"\"\"\n",
    "        valid_indices = x_vals < year_value  # Filter past years only\n",
    "        if np.sum(valid_indices) < window:\n",
    "            window = np.sum(valid_indices)  # Adjust window size\n",
    "    \n",
    "        if window < 1:\n",
    "            return np.mean(y_vals)  # Return mean if insufficient data\n",
    "    \n",
    "        sorted_indices = np.argsort(x_vals[valid_indices])\n",
    "        moving_avg = np.mean(y_vals[valid_indices][sorted_indices][-window:])\n",
    "        return moving_avg\n",
    "\n",
    "\n",
    "\n",
    "    # We'll gather the (Year -> feature) pairs from df_noc_closest and do polynomial on them\n",
    "    for col in feature_cols:\n",
    "        # Skip if it's Year or NOC_ or Medal_ columns\n",
    "        if col.startswith(\"NOC_\") or col.startswith(\"Medal_\") or col == \"Year\":\n",
    "            continue\n",
    "        # Extract existing data from df_noc_closest\n",
    "        if col not in df_noc_closest.columns:\n",
    "            # fallback to zero if col doesn't exist\n",
    "            empty_row[col] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # valid rows for this col (some might be float columns)\n",
    "        valid_rows = df_noc_closest[~df_noc_closest[col].isna()].copy()\n",
    "        if valid_rows.empty:\n",
    "            empty_row[col] = 0.0\n",
    "            continue\n",
    "        \n",
    "        # x = year, y = feature value\n",
    "        x_vals = valid_rows[\"Year\"].to_numpy().astype(float)\n",
    "        y_vals = valid_rows[col].to_numpy().astype(float)\n",
    "\n",
    "        pred_val = moving_average_baseline(x_vals, y_vals, year_value)\n",
    "        empty_row[col] = pred_val\n",
    "\n",
    "    # 4) Now we have a \"predicted\" row for the new year\n",
    "    #    Separate out features\n",
    "    X_unscaled = empty_row[feature_cols].copy()\n",
    "    # 5) Scale\n",
    "    X_scaled = scaler_features.transform(X_unscaled)\n",
    "\n",
    "    # 6) Predict in scaled space\n",
    "    preds_scaled = nn_model.predict(X_scaled)\n",
    "\n",
    "    # 7) Invert scaling -> real medal counts\n",
    "    preds_real = scaler_targets.inverse_transform(preds_scaled)\n",
    "    gold, silver, bronze = preds_real[0]\n",
    "\n",
    "    print(f\"\\n[Extrapolated] Predicted medals for NOC={noc_code}, Year={year_value}:\")\n",
    "    print(f\"  Gold={gold:.1f}, Silver={silver:.1f}, Bronze={bronze:.1f}\")\n",
    "    return (gold, silver, bronze)\n",
    "\n",
    "\n",
    "predict_extrapolated_noc_year(\n",
    "    noc_code=\"CHN\",\n",
    "    year_value=2020,\n",
    "    df_agg=df_agg,\n",
    "    scaler_features=scaler_features,\n",
    "    scaler_targets=scaler_targets,\n",
    "    nn_model=nn_model,\n",
    "    feature_cols=feature_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a0c6d8e-ba0b-4250-b434-afeea0e43e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting for NOC=CHN, Year=2012\n",
      "[Debug] Medal history for NOC=CHN (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1992</td>\n",
       "      <td>14.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>1996</td>\n",
       "      <td>13.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2004</td>\n",
       "      <td>42.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2008</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2016</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>2020</td>\n",
       "      <td>59.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>2024</td>\n",
       "      <td>71.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1299  1988         4.0          16.0          30.0\n",
       "1459  1992        14.0          41.0          15.0\n",
       "1635  1996        13.0          66.0          15.0\n",
       "1831  2000        31.0          19.0          15.0\n",
       "2032  2004        42.0          27.0          13.0\n",
       "2232  2008        72.0          49.0          49.0\n",
       "2436  2012        50.0          38.0          29.0\n",
       "2641  2016        44.0          30.0          35.0\n",
       "2848  2020        59.0          49.0          33.0\n",
       "3054  2024        71.0          57.0          40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2012:\n",
      "  Gold=29.3, Silver=21.0, Bronze=29.6\n",
      "Predicted medals: Gold=29.3, Silver=21.0, Bronze=29.6\n",
      "Actual medals: Gold=50.0, Silver=38.0, Bronze=29.0\n",
      "\n",
      "Predicting for NOC=USA, Year=2012\n",
      "[Debug] Medal history for NOC=USA (up to last 10 rows):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>1988</td>\n",
       "      <td>87.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>1992</td>\n",
       "      <td>87.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>1996</td>\n",
       "      <td>157.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2000</td>\n",
       "      <td>128.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2004</td>\n",
       "      <td>115.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2008</td>\n",
       "      <td>121.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>2012</td>\n",
       "      <td>139.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>2016</td>\n",
       "      <td>137.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>2020</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>2024</td>\n",
       "      <td>131.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1417  1988        87.0          66.0          54.0\n",
       "1588  1992        87.0          50.0          85.0\n",
       "1784  1996       157.0          46.0          52.0\n",
       "1984  2000       128.0          61.0          51.0\n",
       "2185  2004       115.0          75.0          69.0\n",
       "2389  2008       121.0         110.0          78.0\n",
       "2594  2012       139.0          55.0          44.0\n",
       "2801  2016       137.0          52.0          67.0\n",
       "3007  2020       113.0         110.0          75.0\n",
       "3213  2024       131.0          96.0          94.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=USA, Year=2012:\n",
      "  Gold=80.0, Silver=37.4, Bronze=41.6\n",
      "Predicted medals: Gold=80.0, Silver=37.4, Bronze=41.6\n",
      "Actual medals: Gold=139.0, Silver=55.0, Bronze=44.0\n",
      "\n",
      "Predicting for NOC=GBR, Year=2012\n",
      "[Debug] Medal history for NOC=GBR (up to last 10 rows):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>1988</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>1992</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1996</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>2000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>2004</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>2008</td>\n",
       "      <td>31.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2012</td>\n",
       "      <td>46.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>2016</td>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>2020</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1320  1988        22.0          17.0          15.0\n",
       "1484  1992         8.0           3.0          39.0\n",
       "1663  1996         2.0          15.0           9.0\n",
       "1861  2000        22.0          20.0          10.0\n",
       "2061  2004        17.0          23.0          15.0\n",
       "2262  2008        31.0          25.0          25.0\n",
       "2466  2012        46.0          28.0          48.0\n",
       "2671  2016        64.0          55.0          26.0\n",
       "2879  2020        41.0          43.0          57.0\n",
       "3085  2024        39.0          41.0          76.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=GBR, Year=2012:\n",
      "  Gold=9.6, Silver=16.3, Bronze=12.6\n",
      "Predicted medals: Gold=9.6, Silver=16.3, Bronze=12.6\n",
      "Actual medals: Gold=46.0, Silver=28.0, Bronze=48.0\n",
      "\n",
      "Predicting for NOC=RUS, Year=2012\n",
      "[Debug] Medal history for NOC=RUS (up to last 10 rows):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>1996</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>2000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>2004</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2008</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>2016</td>\n",
       "      <td>50.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "39    1900         0.0           0.0           0.0\n",
       "97    1908         1.0           2.0           0.0\n",
       "123   1912         0.0           5.0           2.0\n",
       "196   1924         0.0           0.0           0.0\n",
       "1748  1996        36.0          45.0          34.0\n",
       "1948  2000        66.0          67.0          54.0\n",
       "2148  2004        48.0          46.0          95.0\n",
       "2351  2008        43.0          46.0          53.0\n",
       "2556  2012        50.0          39.0          49.0\n",
       "2762  2016        50.0          28.0          35.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=RUS, Year=2012:\n",
      "  Gold=42.8, Silver=26.6, Bronze=37.0\n",
      "Predicted medals: Gold=42.8, Silver=26.6, Bronze=37.0\n",
      "Actual medals: Gold=50.0, Silver=39.0, Bronze=49.0\n",
      "\n",
      "Predicting for NOC=GER, Year=2012\n",
      "[Debug] Medal history for NOC=GER (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1964</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>1992</td>\n",
       "      <td>81.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>1996</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2004</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>2008</td>\n",
       "      <td>42.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>2012</td>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2016</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>2020</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>2024</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "660   1964        22.0          44.0          50.0\n",
       "1486  1992        81.0          57.0          60.0\n",
       "1667  1996        42.0          35.0          45.0\n",
       "1865  2000        31.0          23.0          62.0\n",
       "2065  2004        41.0          43.0          63.0\n",
       "2266  2008        42.0          16.0          41.0\n",
       "2470  2012        43.0          27.0          22.0\n",
       "2675  2016        47.0          43.0          67.0\n",
       "2883  2020        21.0          26.0          34.0\n",
       "3089  2024        21.0          49.0          38.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=GER, Year=2012:\n",
      "  Gold=31.9, Silver=26.1, Bronze=29.6\n",
      "Predicted medals: Gold=31.9, Silver=26.1, Bronze=29.6\n",
      "Actual medals: Gold=43.0, Silver=27.0, Bronze=22.0\n",
      "\n",
      "Predicting for NOC=CHN, Year=2016\n",
      "[Debug] Medal history for NOC=CHN (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1992</td>\n",
       "      <td>14.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>1996</td>\n",
       "      <td>13.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2004</td>\n",
       "      <td>42.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2008</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2016</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>2020</td>\n",
       "      <td>59.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>2024</td>\n",
       "      <td>71.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1299  1988         4.0          16.0          30.0\n",
       "1459  1992        14.0          41.0          15.0\n",
       "1635  1996        13.0          66.0          15.0\n",
       "1831  2000        31.0          19.0          15.0\n",
       "2032  2004        42.0          27.0          13.0\n",
       "2232  2008        72.0          49.0          49.0\n",
       "2436  2012        50.0          38.0          29.0\n",
       "2641  2016        44.0          30.0          35.0\n",
       "2848  2020        59.0          49.0          33.0\n",
       "3054  2024        71.0          57.0          40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2016:\n",
      "  Gold=35.4, Silver=24.2, Bronze=31.8\n",
      "Predicted medals: Gold=35.4, Silver=24.2, Bronze=31.8\n",
      "Actual medals: Gold=44.0, Silver=30.0, Bronze=35.0\n",
      "\n",
      "Predicting for NOC=USA, Year=2016\n",
      "[Debug] Medal history for NOC=USA (up to last 10 rows):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>1988</td>\n",
       "      <td>87.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>1992</td>\n",
       "      <td>87.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>1996</td>\n",
       "      <td>157.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2000</td>\n",
       "      <td>128.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2004</td>\n",
       "      <td>115.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2008</td>\n",
       "      <td>121.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>2012</td>\n",
       "      <td>139.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>2016</td>\n",
       "      <td>137.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>2020</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>2024</td>\n",
       "      <td>131.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1417  1988        87.0          66.0          54.0\n",
       "1588  1992        87.0          50.0          85.0\n",
       "1784  1996       157.0          46.0          52.0\n",
       "1984  2000       128.0          61.0          51.0\n",
       "2185  2004       115.0          75.0          69.0\n",
       "2389  2008       121.0         110.0          78.0\n",
       "2594  2012       139.0          55.0          44.0\n",
       "2801  2016       137.0          52.0          67.0\n",
       "3007  2020       113.0         110.0          75.0\n",
       "3213  2024       131.0          96.0          94.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=USA, Year=2016:\n",
      "  Gold=78.7, Silver=36.6, Bronze=40.9\n",
      "Predicted medals: Gold=78.7, Silver=36.6, Bronze=40.9\n",
      "Actual medals: Gold=137.0, Silver=52.0, Bronze=67.0\n",
      "\n",
      "Predicting for NOC=GBR, Year=2016\n",
      "[Debug] Medal history for NOC=GBR (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>1988</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>1992</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1996</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>2000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>2004</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>2008</td>\n",
       "      <td>31.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2012</td>\n",
       "      <td>46.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>2016</td>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>2020</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1320  1988        22.0          17.0          15.0\n",
       "1484  1992         8.0           3.0          39.0\n",
       "1663  1996         2.0          15.0           9.0\n",
       "1861  2000        22.0          20.0          10.0\n",
       "2061  2004        17.0          23.0          15.0\n",
       "2262  2008        31.0          25.0          25.0\n",
       "2466  2012        46.0          28.0          48.0\n",
       "2671  2016        64.0          55.0          26.0\n",
       "2879  2020        41.0          43.0          57.0\n",
       "3085  2024        39.0          41.0          76.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=GBR, Year=2016:\n",
      "  Gold=11.5, Silver=18.2, Bronze=14.3\n",
      "Predicted medals: Gold=11.5, Silver=18.2, Bronze=14.3\n",
      "Actual medals: Gold=64.0, Silver=55.0, Bronze=26.0\n",
      "\n",
      "Predicting for NOC=RUS, Year=2016\n",
      "[Debug] Medal history for NOC=RUS (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>1996</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>2000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>2004</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2008</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>2016</td>\n",
       "      <td>50.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "39    1900         0.0           0.0           0.0\n",
       "97    1908         1.0           2.0           0.0\n",
       "123   1912         0.0           5.0           2.0\n",
       "196   1924         0.0           0.0           0.0\n",
       "1748  1996        36.0          45.0          34.0\n",
       "1948  2000        66.0          67.0          54.0\n",
       "2148  2004        48.0          46.0          95.0\n",
       "2351  2008        43.0          46.0          53.0\n",
       "2556  2012        50.0          39.0          49.0\n",
       "2762  2016        50.0          28.0          35.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=RUS, Year=2016:\n",
      "  Gold=39.8, Silver=23.6, Bronze=34.8\n",
      "Predicted medals: Gold=39.8, Silver=23.6, Bronze=34.8\n",
      "Actual medals: Gold=50.0, Silver=28.0, Bronze=35.0\n",
      "\n",
      "Predicting for NOC=GER, Year=2016\n",
      "[Debug] Medal history for NOC=GER (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1964</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>1992</td>\n",
       "      <td>81.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>1996</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2004</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>2008</td>\n",
       "      <td>42.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>2012</td>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2016</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>2020</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>2024</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "660   1964        22.0          44.0          50.0\n",
       "1486  1992        81.0          57.0          60.0\n",
       "1667  1996        42.0          35.0          45.0\n",
       "1865  2000        31.0          23.0          62.0\n",
       "2065  2004        41.0          43.0          63.0\n",
       "2266  2008        42.0          16.0          41.0\n",
       "2470  2012        43.0          27.0          22.0\n",
       "2675  2016        47.0          43.0          67.0\n",
       "2883  2020        21.0          26.0          34.0\n",
       "3089  2024        21.0          49.0          38.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=GER, Year=2016:\n",
      "  Gold=29.9, Silver=25.4, Bronze=27.7\n",
      "Predicted medals: Gold=29.9, Silver=25.4, Bronze=27.7\n",
      "Actual medals: Gold=47.0, Silver=43.0, Bronze=67.0\n",
      "\n",
      "Predicting for NOC=CHN, Year=2020\n",
      "[Debug] Medal history for NOC=CHN (up to last 10 rows):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1992</td>\n",
       "      <td>14.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>1996</td>\n",
       "      <td>13.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2004</td>\n",
       "      <td>42.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2008</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2016</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>2020</td>\n",
       "      <td>59.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>2024</td>\n",
       "      <td>71.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1299  1988         4.0          16.0          30.0\n",
       "1459  1992        14.0          41.0          15.0\n",
       "1635  1996        13.0          66.0          15.0\n",
       "1831  2000        31.0          19.0          15.0\n",
       "2032  2004        42.0          27.0          13.0\n",
       "2232  2008        72.0          49.0          49.0\n",
       "2436  2012        50.0          38.0          29.0\n",
       "2641  2016        44.0          30.0          35.0\n",
       "2848  2020        59.0          49.0          33.0\n",
       "3054  2024        71.0          57.0          40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2020:\n",
      "  Gold=34.8, Silver=24.2, Bronze=31.7\n",
      "Predicted medals: Gold=34.8, Silver=24.2, Bronze=31.7\n",
      "Actual medals: Gold=59.0, Silver=49.0, Bronze=33.0\n",
      "\n",
      "Predicting for NOC=USA, Year=2020\n",
      "[Debug] Medal history for NOC=USA (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>1988</td>\n",
       "      <td>87.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>1992</td>\n",
       "      <td>87.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>1996</td>\n",
       "      <td>157.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2000</td>\n",
       "      <td>128.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2004</td>\n",
       "      <td>115.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2008</td>\n",
       "      <td>121.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>2012</td>\n",
       "      <td>139.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>2016</td>\n",
       "      <td>137.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>2020</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>2024</td>\n",
       "      <td>131.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1417  1988        87.0          66.0          54.0\n",
       "1588  1992        87.0          50.0          85.0\n",
       "1784  1996       157.0          46.0          52.0\n",
       "1984  2000       128.0          61.0          51.0\n",
       "2185  2004       115.0          75.0          69.0\n",
       "2389  2008       121.0         110.0          78.0\n",
       "2594  2012       139.0          55.0          44.0\n",
       "2801  2016       137.0          52.0          67.0\n",
       "3007  2020       113.0         110.0          75.0\n",
       "3213  2024       131.0          96.0          94.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=USA, Year=2020:\n",
      "  Gold=80.3, Silver=36.7, Bronze=41.2\n",
      "Predicted medals: Gold=80.3, Silver=36.7, Bronze=41.2\n",
      "Actual medals: Gold=113.0, Silver=110.0, Bronze=75.0\n",
      "\n",
      "Predicting for NOC=GBR, Year=2020\n",
      "[Debug] Medal history for NOC=GBR (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>1988</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>1992</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1996</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>2000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>2004</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>2008</td>\n",
       "      <td>31.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2012</td>\n",
       "      <td>46.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>2016</td>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>2020</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1320  1988        22.0          17.0          15.0\n",
       "1484  1992         8.0           3.0          39.0\n",
       "1663  1996         2.0          15.0           9.0\n",
       "1861  2000        22.0          20.0          10.0\n",
       "2061  2004        17.0          23.0          15.0\n",
       "2262  2008        31.0          25.0          25.0\n",
       "2466  2012        46.0          28.0          48.0\n",
       "2671  2016        64.0          55.0          26.0\n",
       "2879  2020        41.0          43.0          57.0\n",
       "3085  2024        39.0          41.0          76.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=GBR, Year=2020:\n",
      "  Gold=15.0, Silver=21.9, Bronze=17.6\n",
      "Predicted medals: Gold=15.0, Silver=21.9, Bronze=17.6\n",
      "Actual medals: Gold=41.0, Silver=43.0, Bronze=57.0\n",
      "\n",
      "Predicting for NOC=RUS, Year=2020\n",
      "[Debug] Medal history for NOC=RUS (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>1996</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>2000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>2004</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2008</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>2016</td>\n",
       "      <td>50.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "39    1900         0.0           0.0           0.0\n",
       "97    1908         1.0           2.0           0.0\n",
       "123   1912         0.0           5.0           2.0\n",
       "196   1924         0.0           0.0           0.0\n",
       "1748  1996        36.0          45.0          34.0\n",
       "1948  2000        66.0          67.0          54.0\n",
       "2148  2004        48.0          46.0          95.0\n",
       "2351  2008        43.0          46.0          53.0\n",
       "2556  2012        50.0          39.0          49.0\n",
       "2762  2016        50.0          28.0          35.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=RUS, Year=2020:\n",
      "  Gold=31.4, Silver=18.7, Bronze=30.5\n",
      "Predicted medals: Gold=31.4, Silver=18.7, Bronze=30.5\n",
      "No actual data for NOC=RUS, Year=2020\n",
      "\n",
      "Predicting for NOC=GER, Year=2020\n",
      "[Debug] Medal history for NOC=GER (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1964</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>1992</td>\n",
       "      <td>81.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>1996</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2004</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>2008</td>\n",
       "      <td>42.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>2012</td>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2016</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>2020</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>2024</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "660   1964        22.0          44.0          50.0\n",
       "1486  1992        81.0          57.0          60.0\n",
       "1667  1996        42.0          35.0          45.0\n",
       "1865  2000        31.0          23.0          62.0\n",
       "2065  2004        41.0          43.0          63.0\n",
       "2266  2008        42.0          16.0          41.0\n",
       "2470  2012        43.0          27.0          22.0\n",
       "2675  2016        47.0          43.0          67.0\n",
       "2883  2020        21.0          26.0          34.0\n",
       "3089  2024        21.0          49.0          38.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=GER, Year=2020:\n",
      "  Gold=31.0, Silver=25.2, Bronze=29.4\n",
      "Predicted medals: Gold=31.0, Silver=25.2, Bronze=29.4\n",
      "Actual medals: Gold=21.0, Silver=26.0, Bronze=34.0\n",
      "\n",
      "Predicting for NOC=CHN, Year=2024\n",
      "[Debug] Medal history for NOC=CHN (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1988</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1992</td>\n",
       "      <td>14.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>1996</td>\n",
       "      <td>13.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>2004</td>\n",
       "      <td>42.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2008</td>\n",
       "      <td>72.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2016</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>2020</td>\n",
       "      <td>59.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>2024</td>\n",
       "      <td>71.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1299  1988         4.0          16.0          30.0\n",
       "1459  1992        14.0          41.0          15.0\n",
       "1635  1996        13.0          66.0          15.0\n",
       "1831  2000        31.0          19.0          15.0\n",
       "2032  2004        42.0          27.0          13.0\n",
       "2232  2008        72.0          49.0          49.0\n",
       "2436  2012        50.0          38.0          29.0\n",
       "2641  2016        44.0          30.0          35.0\n",
       "2848  2020        59.0          49.0          33.0\n",
       "3054  2024        71.0          57.0          40.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=CHN, Year=2024:\n",
      "  Gold=27.3, Silver=13.1, Bronze=25.9\n",
      "Predicted medals: Gold=27.3, Silver=13.1, Bronze=25.9\n",
      "Actual medals: Gold=71.0, Silver=57.0, Bronze=40.0\n",
      "\n",
      "Predicting for NOC=USA, Year=2024\n",
      "[Debug] Medal history for NOC=USA (up to last 10 rows):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>1988</td>\n",
       "      <td>87.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>1992</td>\n",
       "      <td>87.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>1996</td>\n",
       "      <td>157.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>2000</td>\n",
       "      <td>128.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>2004</td>\n",
       "      <td>115.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2008</td>\n",
       "      <td>121.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>2012</td>\n",
       "      <td>139.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2801</th>\n",
       "      <td>2016</td>\n",
       "      <td>137.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>2020</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>2024</td>\n",
       "      <td>131.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1417  1988        87.0          66.0          54.0\n",
       "1588  1992        87.0          50.0          85.0\n",
       "1784  1996       157.0          46.0          52.0\n",
       "1984  2000       128.0          61.0          51.0\n",
       "2185  2004       115.0          75.0          69.0\n",
       "2389  2008       121.0         110.0          78.0\n",
       "2594  2012       139.0          55.0          44.0\n",
       "2801  2016       137.0          52.0          67.0\n",
       "3007  2020       113.0         110.0          75.0\n",
       "3213  2024       131.0          96.0          94.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=USA, Year=2024:\n",
      "  Gold=77.9, Silver=28.5, Bronze=40.9\n",
      "Predicted medals: Gold=77.9, Silver=28.5, Bronze=40.9\n",
      "Actual medals: Gold=131.0, Silver=96.0, Bronze=94.0\n",
      "\n",
      "Predicting for NOC=GBR, Year=2024\n",
      "[Debug] Medal history for NOC=GBR (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>1988</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>1992</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1996</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>2000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>2004</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>2008</td>\n",
       "      <td>31.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2012</td>\n",
       "      <td>46.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>2016</td>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>2020</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>2024</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "1320  1988        22.0          17.0          15.0\n",
       "1484  1992         8.0           3.0          39.0\n",
       "1663  1996         2.0          15.0           9.0\n",
       "1861  2000        22.0          20.0          10.0\n",
       "2061  2004        17.0          23.0          15.0\n",
       "2262  2008        31.0          25.0          25.0\n",
       "2466  2012        46.0          28.0          48.0\n",
       "2671  2016        64.0          55.0          26.0\n",
       "2879  2020        41.0          43.0          57.0\n",
       "3085  2024        39.0          41.0          76.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "[Extrapolated] Predicted medals for NOC=GBR, Year=2024:\n",
      "  Gold=13.1, Silver=17.7, Bronze=19.8\n",
      "Predicted medals: Gold=13.1, Silver=17.7, Bronze=19.8\n",
      "Actual medals: Gold=39.0, Silver=41.0, Bronze=76.0\n",
      "\n",
      "Predicting for NOC=RUS, Year=2024\n",
      "[Debug] Medal history for NOC=RUS (up to last 10 rows):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>1996</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>2000</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>2004</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2008</td>\n",
       "      <td>43.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>2012</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>2016</td>\n",
       "      <td>50.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "39    1900         0.0           0.0           0.0\n",
       "97    1908         1.0           2.0           0.0\n",
       "123   1912         0.0           5.0           2.0\n",
       "196   1924         0.0           0.0           0.0\n",
       "1748  1996        36.0          45.0          34.0\n",
       "1948  2000        66.0          67.0          54.0\n",
       "2148  2004        48.0          46.0          95.0\n",
       "2351  2008        43.0          46.0          53.0\n",
       "2556  2012        50.0          39.0          49.0\n",
       "2762  2016        50.0          28.0          35.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=RUS, Year=2024:\n",
      "  Gold=31.4, Silver=18.7, Bronze=30.5\n",
      "Predicted medals: Gold=31.4, Silver=18.7, Bronze=30.5\n",
      "No actual data for NOC=RUS, Year=2024\n",
      "\n",
      "Predicting for NOC=GER, Year=2024\n",
      "[Debug] Medal history for NOC=GER (up to last 10 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Medal_Gold</th>\n",
       "      <th>Medal_Silver</th>\n",
       "      <th>Medal_Bronze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>1964</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>1992</td>\n",
       "      <td>81.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>1996</td>\n",
       "      <td>42.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>2000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>2004</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>2008</td>\n",
       "      <td>42.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>2012</td>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>2016</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>2020</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>2024</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year  Medal_Gold  Medal_Silver  Medal_Bronze\n",
       "660   1964        22.0          44.0          50.0\n",
       "1486  1992        81.0          57.0          60.0\n",
       "1667  1996        42.0          35.0          45.0\n",
       "1865  2000        31.0          23.0          62.0\n",
       "2065  2004        41.0          43.0          63.0\n",
       "2266  2008        42.0          16.0          41.0\n",
       "2470  2012        43.0          27.0          22.0\n",
       "2675  2016        47.0          43.0          67.0\n",
       "2883  2020        21.0          26.0          34.0\n",
       "3089  2024        21.0          49.0          38.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Zhu\\AppData\\Local\\Temp\\ipykernel_2692\\3886155821.py:60: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  empty_row = pd.DataFrame(columns=df_agg.columns, index=[0]).fillna(0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Extrapolated] Predicted medals for NOC=GER, Year=2024:\n",
      "  Gold=23.4, Silver=13.6, Bronze=26.6\n",
      "Predicted medals: Gold=23.4, Silver=13.6, Bronze=26.6\n",
      "Actual medals: Gold=21.0, Silver=49.0, Bronze=38.0\n",
      "\n",
      "Prediction vs Actual Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOC</th>\n",
       "      <th>Year</th>\n",
       "      <th>Gold_Predicted</th>\n",
       "      <th>Silver_Predicted</th>\n",
       "      <th>Bronze_Predicted</th>\n",
       "      <th>Gold_Actual</th>\n",
       "      <th>Silver_Actual</th>\n",
       "      <th>Bronze_Actual</th>\n",
       "      <th>Gold_Error_Pct</th>\n",
       "      <th>Silver_Error_Pct</th>\n",
       "      <th>Bronze_Error_Pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHN</td>\n",
       "      <td>2012</td>\n",
       "      <td>29.253687</td>\n",
       "      <td>21.019611</td>\n",
       "      <td>29.622604</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>41.492626</td>\n",
       "      <td>44.685233</td>\n",
       "      <td>2.146912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA</td>\n",
       "      <td>2012</td>\n",
       "      <td>79.952843</td>\n",
       "      <td>37.352993</td>\n",
       "      <td>41.643990</td>\n",
       "      <td>139.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>42.479969</td>\n",
       "      <td>32.085467</td>\n",
       "      <td>5.354569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBR</td>\n",
       "      <td>2012</td>\n",
       "      <td>9.645769</td>\n",
       "      <td>16.347416</td>\n",
       "      <td>12.619681</td>\n",
       "      <td>46.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>79.030937</td>\n",
       "      <td>41.616372</td>\n",
       "      <td>73.708997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RUS</td>\n",
       "      <td>2012</td>\n",
       "      <td>42.775246</td>\n",
       "      <td>26.644657</td>\n",
       "      <td>36.951443</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>14.449509</td>\n",
       "      <td>31.680366</td>\n",
       "      <td>24.588892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GER</td>\n",
       "      <td>2012</td>\n",
       "      <td>31.868126</td>\n",
       "      <td>26.149736</td>\n",
       "      <td>29.576557</td>\n",
       "      <td>43.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.888079</td>\n",
       "      <td>3.149124</td>\n",
       "      <td>34.438896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CHN</td>\n",
       "      <td>2016</td>\n",
       "      <td>35.415104</td>\n",
       "      <td>24.190081</td>\n",
       "      <td>31.788765</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>19.511127</td>\n",
       "      <td>19.366398</td>\n",
       "      <td>9.174957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>USA</td>\n",
       "      <td>2016</td>\n",
       "      <td>78.721161</td>\n",
       "      <td>36.585896</td>\n",
       "      <td>40.886162</td>\n",
       "      <td>137.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>42.539299</td>\n",
       "      <td>29.642509</td>\n",
       "      <td>38.975878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GBR</td>\n",
       "      <td>2016</td>\n",
       "      <td>11.450920</td>\n",
       "      <td>18.190201</td>\n",
       "      <td>14.337427</td>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>82.107937</td>\n",
       "      <td>66.926908</td>\n",
       "      <td>44.856049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RUS</td>\n",
       "      <td>2016</td>\n",
       "      <td>39.790421</td>\n",
       "      <td>23.629026</td>\n",
       "      <td>34.829044</td>\n",
       "      <td>50.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.419159</td>\n",
       "      <td>15.610620</td>\n",
       "      <td>0.488445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GER</td>\n",
       "      <td>2016</td>\n",
       "      <td>29.881506</td>\n",
       "      <td>25.417639</td>\n",
       "      <td>27.661968</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>36.422328</td>\n",
       "      <td>40.889212</td>\n",
       "      <td>58.713480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CHN</td>\n",
       "      <td>2020</td>\n",
       "      <td>34.839764</td>\n",
       "      <td>24.192419</td>\n",
       "      <td>31.703182</td>\n",
       "      <td>59.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>40.949553</td>\n",
       "      <td>50.627716</td>\n",
       "      <td>3.929751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>USA</td>\n",
       "      <td>2020</td>\n",
       "      <td>80.296158</td>\n",
       "      <td>36.724766</td>\n",
       "      <td>41.214718</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>28.941453</td>\n",
       "      <td>66.613849</td>\n",
       "      <td>45.047043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GBR</td>\n",
       "      <td>2020</td>\n",
       "      <td>14.989407</td>\n",
       "      <td>21.869223</td>\n",
       "      <td>17.633331</td>\n",
       "      <td>41.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>63.440472</td>\n",
       "      <td>49.141343</td>\n",
       "      <td>69.064331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RUS</td>\n",
       "      <td>2020</td>\n",
       "      <td>31.396978</td>\n",
       "      <td>18.737026</td>\n",
       "      <td>30.497177</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GER</td>\n",
       "      <td>2020</td>\n",
       "      <td>30.990297</td>\n",
       "      <td>25.198483</td>\n",
       "      <td>29.427483</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>47.572844</td>\n",
       "      <td>3.082760</td>\n",
       "      <td>13.448581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CHN</td>\n",
       "      <td>2024</td>\n",
       "      <td>27.282793</td>\n",
       "      <td>13.106811</td>\n",
       "      <td>25.861164</td>\n",
       "      <td>71.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>61.573531</td>\n",
       "      <td>77.005595</td>\n",
       "      <td>35.347090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>USA</td>\n",
       "      <td>2024</td>\n",
       "      <td>77.879410</td>\n",
       "      <td>28.468098</td>\n",
       "      <td>40.889854</td>\n",
       "      <td>131.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>40.550069</td>\n",
       "      <td>70.345732</td>\n",
       "      <td>56.500155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GBR</td>\n",
       "      <td>2024</td>\n",
       "      <td>13.087743</td>\n",
       "      <td>17.661221</td>\n",
       "      <td>19.832893</td>\n",
       "      <td>39.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>66.441685</td>\n",
       "      <td>56.923852</td>\n",
       "      <td>73.904088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RUS</td>\n",
       "      <td>2024</td>\n",
       "      <td>31.396978</td>\n",
       "      <td>18.737026</td>\n",
       "      <td>30.497177</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GER</td>\n",
       "      <td>2024</td>\n",
       "      <td>23.412365</td>\n",
       "      <td>13.644920</td>\n",
       "      <td>26.552418</td>\n",
       "      <td>21.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>11.487452</td>\n",
       "      <td>72.153224</td>\n",
       "      <td>30.125216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NOC  Year  Gold_Predicted  Silver_Predicted  Bronze_Predicted  \\\n",
       "0   CHN  2012       29.253687         21.019611         29.622604   \n",
       "1   USA  2012       79.952843         37.352993         41.643990   \n",
       "2   GBR  2012        9.645769         16.347416         12.619681   \n",
       "3   RUS  2012       42.775246         26.644657         36.951443   \n",
       "4   GER  2012       31.868126         26.149736         29.576557   \n",
       "5   CHN  2016       35.415104         24.190081         31.788765   \n",
       "6   USA  2016       78.721161         36.585896         40.886162   \n",
       "7   GBR  2016       11.450920         18.190201         14.337427   \n",
       "8   RUS  2016       39.790421         23.629026         34.829044   \n",
       "9   GER  2016       29.881506         25.417639         27.661968   \n",
       "10  CHN  2020       34.839764         24.192419         31.703182   \n",
       "11  USA  2020       80.296158         36.724766         41.214718   \n",
       "12  GBR  2020       14.989407         21.869223         17.633331   \n",
       "13  RUS  2020       31.396978         18.737026         30.497177   \n",
       "14  GER  2020       30.990297         25.198483         29.427483   \n",
       "15  CHN  2024       27.282793         13.106811         25.861164   \n",
       "16  USA  2024       77.879410         28.468098         40.889854   \n",
       "17  GBR  2024       13.087743         17.661221         19.832893   \n",
       "18  RUS  2024       31.396978         18.737026         30.497177   \n",
       "19  GER  2024       23.412365         13.644920         26.552418   \n",
       "\n",
       "    Gold_Actual  Silver_Actual  Bronze_Actual  Gold_Error_Pct  \\\n",
       "0          50.0           38.0           29.0       41.492626   \n",
       "1         139.0           55.0           44.0       42.479969   \n",
       "2          46.0           28.0           48.0       79.030937   \n",
       "3          50.0           39.0           49.0       14.449509   \n",
       "4          43.0           27.0           22.0       25.888079   \n",
       "5          44.0           30.0           35.0       19.511127   \n",
       "6         137.0           52.0           67.0       42.539299   \n",
       "7          64.0           55.0           26.0       82.107937   \n",
       "8          50.0           28.0           35.0       20.419159   \n",
       "9          47.0           43.0           67.0       36.422328   \n",
       "10         59.0           49.0           33.0       40.949553   \n",
       "11        113.0          110.0           75.0       28.941453   \n",
       "12         41.0           43.0           57.0       63.440472   \n",
       "13         -1.0           -1.0           -1.0       -1.000000   \n",
       "14         21.0           26.0           34.0       47.572844   \n",
       "15         71.0           57.0           40.0       61.573531   \n",
       "16        131.0           96.0           94.0       40.550069   \n",
       "17         39.0           41.0           76.0       66.441685   \n",
       "18         -1.0           -1.0           -1.0       -1.000000   \n",
       "19         21.0           49.0           38.0       11.487452   \n",
       "\n",
       "    Silver_Error_Pct  Bronze_Error_Pct  \n",
       "0          44.685233          2.146912  \n",
       "1          32.085467          5.354569  \n",
       "2          41.616372         73.708997  \n",
       "3          31.680366         24.588892  \n",
       "4           3.149124         34.438896  \n",
       "5          19.366398          9.174957  \n",
       "6          29.642509         38.975878  \n",
       "7          66.926908         44.856049  \n",
       "8          15.610620          0.488445  \n",
       "9          40.889212         58.713480  \n",
       "10         50.627716          3.929751  \n",
       "11         66.613849         45.047043  \n",
       "12         49.141343         69.064331  \n",
       "13         -1.000000         -1.000000  \n",
       "14          3.082760         13.448581  \n",
       "15         77.005595         35.347090  \n",
       "16         70.345732         56.500155  \n",
       "17         56.923852         73.904088  \n",
       "18         -1.000000         -1.000000  \n",
       "19         72.153224         30.125216  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_and_compare(noc_list, df_agg, scaler_features, scaler_targets, nn_model, feature_cols, check_years=[2012, 2016, 2020, 2024], show_history=False):\n",
    "    \"\"\"\n",
    "    Predicts medal counts using the trained model and compares with actual data.\n",
    "\n",
    "    Parameters:\n",
    "        noc_list (list): List of NOCs to predict and compare.\n",
    "        df_agg (pd.DataFrame): Aggregated dataframe with historical data.\n",
    "        scaler_features (StandardScaler): Scaler for feature normalization.\n",
    "        scaler_targets (StandardScaler): Scaler for target values.\n",
    "        nn_model (Sequential): Trained neural network model.\n",
    "        feature_cols (list): Columns used for model input.\n",
    "        check_years (list): Olympic years to check.\n",
    "        show_history (bool): Whether to display the country's medal count history.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of actual vs. predicted medal counts.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for year in check_years:\n",
    "        for noc in noc_list:\n",
    "            print(f\"\\nPredicting for NOC={noc}, Year={year}\")\n",
    "\n",
    "            # Get predictions for NOC and year\n",
    "            try:\n",
    "                pred_result = predict_extrapolated_noc_year(\n",
    "                    noc_code=noc,\n",
    "                    year_value=year,\n",
    "                    df_agg=df_agg,\n",
    "                    scaler_features=scaler_features,\n",
    "                    scaler_targets=scaler_targets,\n",
    "                    nn_model=nn_model,\n",
    "                    feature_cols=feature_cols\n",
    "                )\n",
    "\n",
    "                if pred_result is not None:\n",
    "                    gold_pred, silver_pred, bronze_pred = pred_result\n",
    "                    print(f\"Predicted medals: Gold={gold_pred:.1f}, Silver={silver_pred:.1f}, Bronze={bronze_pred:.1f}\")\n",
    "                else:\n",
    "                    gold_pred, silver_pred, bronze_pred = None, None, None\n",
    "                    print(f\"Prediction unavailable for NOC={noc}, Year={year}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Prediction failed for NOC={noc}, Year={year}. Error: {str(e)}\")\n",
    "                gold_pred, silver_pred, bronze_pred = None, None, None\n",
    "\n",
    "            # Get actual data\n",
    "            noc_col = f\"NOC_{noc}\"  # Adjusting for one-hot encoded columns\n",
    "            if noc_col not in df_agg.columns:\n",
    "                print(f\"[Check] No data for NOC={noc}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            df_noc_all = df_agg[df_agg[noc_col] == 1].copy()\n",
    "            df_noc_filtered = df_noc_all[df_noc_all[\"Year\"] == year]\n",
    "\n",
    "            if show_history and not df_noc_all.empty:\n",
    "                print(f\"[History] Medal history for NOC={noc} up to year {year}:\")\n",
    "                display(df_noc_all[[\"Year\", \"Medal_Gold\", \"Medal_Silver\", \"Medal_Bronze\"]].tail(10))\n",
    "\n",
    "            if not df_noc_filtered.empty:\n",
    "                gold_actual = df_noc_filtered[\"Medal_Gold\"].values[0]\n",
    "                silver_actual = df_noc_filtered[\"Medal_Silver\"].values[0]\n",
    "                bronze_actual = df_noc_filtered[\"Medal_Bronze\"].values[0]\n",
    "                print(f\"Actual medals: Gold={gold_actual}, Silver={silver_actual}, Bronze={bronze_actual}\")\n",
    "            else:\n",
    "                gold_actual, silver_actual, bronze_actual = None, None, None\n",
    "                print(f\"No actual data for NOC={noc}, Year={year}\")\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                \"NOC\": noc,\n",
    "                \"Year\": year,\n",
    "                \"Gold_Predicted\": gold_pred,\n",
    "                \"Silver_Predicted\": silver_pred,\n",
    "                \"Bronze_Predicted\": bronze_pred,\n",
    "                \"Gold_Actual\": gold_actual,\n",
    "                \"Silver_Actual\": silver_actual,\n",
    "                \"Bronze_Actual\": bronze_actual\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame for easier visualization\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Calculate absolute differences and percentage errors for performance evaluation\n",
    "    results_df[\"Gold_Error_Pct\"] = (abs(results_df[\"Gold_Predicted\"] - results_df[\"Gold_Actual\"]) / results_df[\"Gold_Actual\"]) * 100\n",
    "    results_df[\"Silver_Error_Pct\"] = (abs(results_df[\"Silver_Predicted\"] - results_df[\"Silver_Actual\"]) / results_df[\"Silver_Actual\"]) * 100\n",
    "    results_df[\"Bronze_Error_Pct\"] = (abs(results_df[\"Bronze_Predicted\"] - results_df[\"Bronze_Actual\"]) / results_df[\"Bronze_Actual\"]) * 100\n",
    "\n",
    "    # Replace NaN values with -1 for missing data (countries like Russia that didn't participate)\n",
    "    results_df.fillna(-1, inplace=True)\n",
    "\n",
    "    # Display final comparison results\n",
    "    print(\"\\nPrediction vs Actual Summary:\")\n",
    "    display(results_df)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def visualize_selected_nocs(results_df, selected_nocs, selected_years, color='Red'):\n",
    "    \"\"\"\n",
    "    Generate heatmaps for selected NOCs and years using a single-color theme with grayscale.\n",
    "\n",
    "    Parameters:\n",
    "        results_df (pd.DataFrame): DataFrame containing prediction vs. actual medal count results.\n",
    "        selected_nocs (list): List of NOCs to visualize.\n",
    "        selected_years (list): List of Olympic years to visualize.\n",
    "        color (str): Color theme for the heatmap (e.g., 'Blues', 'Greens', 'Reds').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the dataframe for selected NOCs and years\n",
    "    filtered_df = results_df[(results_df[\"NOC\"].isin(selected_nocs)) & \n",
    "                             (results_df[\"Year\"].isin(selected_years))]\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"No data available for selected NOCs and years. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    # Generate heatmaps for Gold, Silver, and Bronze prediction errors\n",
    "    for metric in [\"Gold_Error_Pct\", \"Silver_Error_Pct\", \"Bronze_Error_Pct\"]:\n",
    "        heatmap_data = filtered_df.pivot(index=\"NOC\", columns=\"Year\", values=metric)\n",
    "\n",
    "        # Check if heatmap data is not empty\n",
    "        if not heatmap_data.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(heatmap_data, annot=True, cmap=color, fmt=\".1f\", linewidths=0.5)\n",
    "            plt.title(f\"{metric.replace('_', ' ')} Heatmap for Selected NOCs\")\n",
    "            plt.xlabel(\"Year\")\n",
    "            plt.ylabel(\"NOC\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Skipping heatmap for {metric} due to lack of valid data.\")\n",
    "\n",
    "# Example usage\n",
    "noc_list = [\"CHN\", \"USA\", \"GBR\", \"RUS\", \"GER\"]\n",
    "comparison_results = predict_and_compare(\n",
    "    noc_list=noc_list,\n",
    "    df_agg=df_agg,\n",
    "    scaler_features=scaler_features,\n",
    "    scaler_targets=scaler_targets,\n",
    "    nn_model=nn_model,\n",
    "    feature_cols=feature_cols,\n",
    "    show_history=False\n",
    ")\n",
    "selected_nocs = noc_list\n",
    "selected_years = [2012, 2016, 2020, 2024]\n",
    "\n",
    "visualize_selected_nocs(comparison_results, selected_nocs, selected_years, color=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259382b-c11b-4ed2-8c8e-d01fd85decfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
